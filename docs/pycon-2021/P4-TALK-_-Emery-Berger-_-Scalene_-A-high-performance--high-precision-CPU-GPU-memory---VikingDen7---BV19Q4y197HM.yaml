- en: P4：TALK _ Emery Berger _ Scalene_ A high-performance, high-precision CPU+GPU+memory
    - VikingDen7 - BV19Q4y197HM
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: P4：演讲 _ Emery Berger _ Scalene_ 高性能、高精度的CPU+GPU+内存 - VikingDen7 - BV19Q4y197HM
- en: Hi， I'm Emery Berger， a professor of computer science at the University of Massachusetts。
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 嗨，我是Emery Berger，马萨诸塞大学的计算机科学教授。
- en: '![](img/a9915b6f966cee2d9257e21938089fd0_1.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a9915b6f966cee2d9257e21938089fd0_1.png)'
- en: Today， I'll be talking about a new profiler for Python called scaling。 As you
    know。 when you have a performance problem or you just want your code to run， faster。
    you reach for a profiler to help you identify problems and hopefully fix them。
    Listen here is a selection of some of the most popular profilers， including the
    ones。
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 今天，我将讨论一个新的Python分析器，叫做scalene。正如你所知，当你遇到性能问题或只是想让代码运行得更快时，你会使用分析器来帮助你识别问题并希望修复它们。这里有一些最受欢迎的分析器的选择，包括这些。
- en: that come with Python by default。 In fact， there's such a proliferation of profilers
    that one of the profilers is actually called。 Yappy， which stands for yet another
    Python profiler。 So today， I'll be talking about Scalian。 which is another profiler，
    but one that's， quite different from the others。 So it's not really yet another
    profiler， as I hope you'll see。
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 这些测试是Python默认附带的。实际上，分析器的数量如此之多，以至于其中一个分析器实际上叫做Yappy，意为“又一个Python分析器”。所以今天，我将讨论Scalene，这是另一个分析器，但它与其他分析器大相径庭。因此，它并不算是又一个分析器，希望你能理解。
- en: So one thing that you want out of a profiler is not too much runtime overhead。
    If you're profiling a program， it's because it's already too slow， so you don't
    want。 to get that much slower。 So let's see how all of these profilers stack up
    in terms of performance。 To measure this， we're going to run them on a benchmark
    from the Pi Performance suite。
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 所以你希望分析器的一个特点是不要有太多的运行时开销。如果你在分析一个程序，那是因为它已经太慢了，所以你不想让它变得更慢。那么让我们看看这些分析器在性能方面的表现。为了测量这个，我们将使用来自Pi
    Performance套件的基准测试。
- en: On the Y-axis of the graph， we have normalized execution time， which means we
    took the time。 running the profiler and divided it by how long it took to run
    the program without profiling。 That means that the best case would be 1。0x， meaning
    no overhand。 So some of the profilers do quite well with minimal overhead， 1。0
    being as low as you， can get。
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 图表的Y轴表示标准化执行时间，这意味着我们将运行分析器所需的时间除以没有分析的程序运行时间。这意味着最佳情况是1.0x，表示没有开销。因此，一些分析器在开销最小的情况下表现得很好，1.0是你能达到的最低值。
- en: and these three profilers go up to no more than 1。5x， which is pretty good，
    which。 is why they're marked in green。 Unfortunately， some of the other profilers
    are considerably slower。 These are shown in yellow for caution。 This includes
    the built-in C profile。 And here。 the slowdowns range from 2x to almost 7x。 This
    isn't great， obviously。
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 这三个分析器的开销最高不超过1.5倍，这相当不错，所以它们被标记为绿色。不幸的是，其他一些分析器则慢得多。这些用黄色标记以示警告。这包括内置的C分析器。在这里，减速范围从2倍到近7倍。显然，这并不好。
- en: You certainly wouldn't want to run in production with profiling on， and it's
    inconvenient that。 things are slowed down this much， but maybe it's not a deal
    breaker。 On the other hand。 some other profilers impose pretty drastic overheads，
    up to almost 40x， as you can see。 To make this concrete and really feel the pain，
    imagine profiling a program that takes。
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 你肯定不想在生产环境中运行带有分析的程序，而且事情变得这么慢也是不方便的，但也许这并不是致命的缺陷。另一方面，一些其他分析器的开销相当高，几乎达到40倍，如你所见。为了使这一点具体化并真正感受痛苦，想象一下分析一个需要。
- en: just one minute to run， but now it takes 40 minutes。 For most people。 this would
    be an unacceptably high overhead， which is why， in the graph。 they're colored
    red for stop。 But it gets even worse。 so there is a profiler for memory called
    memory profiler， and memory。
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 运行只需一分钟，但现在需要40分钟。对大多数人来说，这将是不可接受的高开销，这就是为什么在图表中，它们被标记为红色以示停止。但情况更糟，所以有一个内存分析器叫做memory
    profiler。
- en: profiler imposes overheads of almost 300x， which is pretty extreme， really an
    extraordinary。 amount of slowdown。 Now， obviously， we should put kind of a big
    asterisk here by memory profiler。 because after， all， all these other profilers
    that I've shown before only profile CPU time。 Well。 memory profiler profiles memory，
    so maybe it makes sense for it to be more costly， and。
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 一些分析器的开销高达几乎300倍，这真是极端，实际上是非常异常的减速。显然，我们应该在内存分析器旁边加个大星号，因为毕竟，我之前展示的所有其他分析器仅分析CPU时间。嗯，内存分析器分析内存，所以它可能更耗费资源。
- en: really a lot more costly。 But as you'll see， this is not necessarily the case。
    It is possible to profile memory vastly more efficiently。 So to make the rest
    of this discussion easier， we're going to move from a graph to a table。 and then
    we'll rotate it， so it's a little bit easier to read。
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，这要昂贵得多。但正如你将看到的，这并不一定是正确的。实际上，可以以更高效的方式分析内存。为了让后面的讨论更简单，我们将从图表转向表格，然后我们会旋转它，以便更容易阅读。
- en: And so now you may be asking yourself where Scalene fits in on this graph。 And
    the answer is Scalene is pretty efficient。 For this benchmark。 it's just 20% slower
    than the original program。 There are options that let Scalene impose even less
    overhead。 but this talk is mostly， about the full Scalene with its default options。
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你可能在问，Scalene在这个图表中适合什么位置。答案是，Scalene非常高效。对于这个基准测试，它的速度比原始程序慢20%。有一些选项可以让Scalene的开销更小，但这次演讲主要是关于使用默认选项的完整Scalene。
- en: and the full Scalene profiles CPU， a Profiler， memory， and a Profiler GPU。 and
    we'll talk about that more about that in detail soon。 Now。 profilers typically
    fall into one of two categories。 They profile either by functions or by lines。
    So what this means is if you have a function-level profile， yes， it still shows
    line numbers。
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的Scalene能够分析CPU、内存和GPU，我们将很快详细讨论这些内容。现在，分析器通常分为两类。它们要么按函数分析，要么按行分析。这意味着，如果你有一个函数级分析，是的，它仍然会显示行号。
- en: but only the line numbers that correspond to the start of the function。 Otherwise。
    the rest of the information is aggregated over the entire function， which。 is
    fine if you have lots of little functions that each don't do very much。 And then
    the profiler says， hey， this function is taking a lot of time。
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 但仅显示与函数开始对应的行号。否则，其他信息将会在整个函数上汇总，这对于有很多小函数且每个函数做的事情不多的情况是可以的。然后分析器会说，嘿，这个函数耗时很长。
- en: Maybe you should make it run faster。 However， it's really not so great when
    you have long functions。 Really， once you get over a certain length， a function-level
    profiler is not super helpful。 because it says， hey， this function is slow， but
    you might want more granular assistance。 which is where line-level profilers come
    in。 So a line-level profiler， of course。
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 也许你应该让它运行得更快。然而，当你有长函数时，这并不是很好。实际上，一旦函数超过某个长度，函数级分析器就不会特别有用，因为它会说，嘿，这个函数很慢，但你可能需要更细致的帮助。这就是行级分析器的用武之地。所以行级分析器，当然。
- en: reports information for every line of code， which， is great when you're profiling
    large functions。 like I mentioned earlier， or code that interacts， with libraries
    like NumPy。 where each line is potentially doing really a lot of work， or。 in
    cases where function calls are just obscured because of Python overloading。 On
    the other hand。
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 为每一行代码报告信息，这在分析大型函数时非常好。正如我之前提到的，或与像NumPy这样的库交互的代码，每一行可能都在执行很多工作，或者在函数调用由于Python重载而被模糊的情况下。另一方面。
- en: when you're profiling a large program， this might be far too fine， of granularity，
    right？
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 当你分析一个大型程序时，这可能会过于精细，对吧？
- en: Profiling every single line in a large program could really miss the forest
    for the trees。 So you might be asking yourself， why not both？ And indeed， that's
    the approach that Scameline takes。 Scameline simultaneously performs function-level
    and line-level profiling。 So that's great。 There's another characteristic of some
    profilers， which is you have to change your code to make。
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在大型程序中分析每一行可能真的会忽略整体。所以你可能在问，为什么不两者兼顾？事实上，这正是Scalene采用的方法。Scalene同时进行函数级和行级分析。所以这很好。有些分析器的另一个特点是，你必须更改你的代码才能使其。
- en: the profiler be able to actually profile it。 Now it's not an extraordinarily
    huge change。 You just put at profile decorators on functions that you want to
    profile。 But this is kind of putting the card before the horse。 It assumes that
    you already know where the problems are， which is not always the case。
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 分析器能够实际进行分析。现在，这并不是一个非常巨大的变化。你只需在想要分析的函数上添加分析装饰器。但这有点像本末倒置。它假设你已经知道问题所在，而这并不总是如此。
- en: Also it's a pain to go in and change the program before running the profiler。
    So Scameline。 like many of the other profilers， works on unmodified code， reporting
    performance。 information for the whole program。 But it also supports the at-profile
    decorators。 So when you use these in Scameline， it tells Scameline to focus just
    on specific functions。
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，在运行分析器之前更改程序是非常麻烦的。因此，Scameline和许多其他分析器一样，能够在未修改的代码上工作，报告整个程序的性能信息。但它也支持at-profile装饰器。因此，当你在Scameline中使用这些时，它会告诉Scameline只关注特定函数。
- en: which works best once you already know where the problem spots are， which you
    can find。 out without modifying any code。 Now a number of profilers have surprisingly
    limited support for Python threads。 So you can see that there's a number of profilers，
    nearly half of them， that actually don't work。 when you're running multi-threaded
    code。 And what I mean by that is either they literally don't work。
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你知道问题点在哪里，这种方法效果最佳，而你可以在不修改任何代码的情况下找到这些问题。现在，许多分析器对Python线程的支持令人惊讶地有限。因此，你可以看到，近一半的分析器在运行多线程代码时实际上无法工作。我所说的就是它们要么根本无法工作。
- en: that is they just fail to operate， entirely， or they misreport the performance。
    So if you have code that's running in a thread， its runtime might be completely
    ignored。 To our surprise， it turns out that no existing profiler correctly profiles
    code that uses。 the multi-processing library。 So Scameline turns out to be the
    only option if you want to profile such an application。
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 也就是说，它们完全无法操作，或者错误报告性能。如果你有在线程中运行的代码，其运行时间可能会被完全忽略。令我们惊讶的是，现有的分析器没有一个能够正确分析使用多处理库的代码。因此，如果你想分析这样的应用程序，Scameline是唯一的选择。
- en: But these features are not the biggest advantages of Scameline overpass profilers。
    No pass profiler can do any of the things listed here on this slide， except for
    one， which。 I'll mention in a second， just to read the information across the
    top。 Scameline can separate out Python time versus C or native time。
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 但这些功能并不是Scameline相对于其他分析器的最大优势。除了一个，其他分析器都无法执行此幻灯片上列出的任何操作，我会在稍后提到。只需查看顶部的信息即可。Scameline可以将Python时间与C或本地时间分开。
- en: It can identify how much time is spent in system。 System calls meaning IO。 It
    profiles memory。 It profiles a GPU if you have one。 It illustrates memory trends
    and reports copy volume。 which I'll explain in a minute， and， it automatically
    detects memory leaks。 Now like I said。 there is one profiler that does one of
    these things。 Naturally。
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 它可以识别系统中花费的时间。系统调用意味着IO。它对内存进行分析。如果你有GPU，它也会进行分析。它展示内存趋势并报告复制量，这一点我会在稍后解释，并且它会自动检测内存泄漏。正如我所说，只有一个分析器可以做到这些事情，自然如此。
- en: memory profiler does memory profiling。 Remember， for this benchmark。 memory
    profiler was almost 300 times slower， while Scameline。 ran the same benchmark
    and produced a memory profile with only 20% overhead。 All right。 so in the rest
    of this talk， I'm going to talk about how Scameline is able to。
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 内存分析器进行内存分析。请记住，对于此基准测试，内存分析器的速度几乎慢了300倍，而Scameline在同样的基准测试中运行，并产生了仅有20%开销的内存分析。好吧，接下来我将讨论Scameline如何做到这一点。
- en: profile all of these other things， which gives you vastly more information about
    your。 Python programs and makes Scameline way better at helping you identify and
    fix performance。 problems。 So Scameline is straightforward to use。 You've just
    replaced Python 3 with Scameline。 There are lots of options that let you tailor
    how Scameline profiles your code。
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 分析所有其他内容，这会为你的Python程序提供更多信息，使Scameline在帮助你识别和修复性能问题方面表现得更好。因此，使用Scameline非常简单。你只需将Python
    3替换为Scameline。还有很多选项可以让你定制Scameline分析代码的方式。
- en: '#help provides a list， as is normal。 I will walk through a few really useful
    options that you would want to know when you''re using。 Scameline。 So one particularly
    useful option is reduced profile， which only produces profiles from。 lines of
    code that run for at least 1% of overall execution time， or which allocates some。
    reasonable amount of memory。 So I''m not going to show it。'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '#help提供了一个列表，这是正常的。我将介绍一些使用Scameline时你想要了解的非常有用的选项。因此，一个特别有用的选项是减少分析，它只产生执行时间占总体执行时间至少1%的代码行的分析，或分配一些合理数量的内存。所以我不会展示它。'
- en: but we're going to be using this option for all the next examples。 By default。
    Scameline prints its output to the console， but you can also tell Scameline， to
    output to a file。 And if you want， you can tell it to use -html， which will lead
    it to produce a webpage containing。 its results。 And here is an example profile。
    We'll dig into this in more detail in a minute。
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 但我们将在接下来的所有示例中使用这个选项。默认情况下，Scameline 会将输出打印到控制台，但你也可以告诉 Scameline 将输出写入文件。如果你愿意，你可以告诉它使用
    -html，这样它将生成一个包含其结果的网页。以下是一个示例配置文件。我们稍后会详细讨论这一点。
- en: but the top part of the profile is the line， level information with lines emitted
    when they have a little contribution to runtime or。 memory consumption。 And at
    the bottom， you get the function level profile。 which Scameline reports for every，
    single profile all the aggregated information。 It also provides a breakdown in
    descending order of the lines responsible for the most。
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 但配置文件的顶部部分是行级信息，当行对运行时或内存消耗的贡献较小时，会输出这些信息。在底部，你会得到函数级配置文件，Scameline 会报告每个单独的配置文件所有的汇总信息。它还提供了按责任行的降序分类。
- en: memory consumption。 So you can look for things that might be consuming an ordented
    amount of memory at a glance。 Finally， briefly， Scameline also allows you to disable
    memory profiling and some other。 aspects of profiling that I'll talk about later
    by saying CPU only。 This is a bit of a misnomer because if you have a GPU， Scameline
    will still do GPU profiling。
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 内存消耗。因此，你可以快速查看可能消耗大量内存的内容。最后，简而言之，Scameline 还允许你通过声明 CPU only 来禁用内存分析和其他一些分析方面，我稍后会谈到。这有点误导，因为如果你有
    GPU，Scameline 仍会进行 GPU 分析。
- en: but both of these are extremely efficient and effectively impose zero overhead。
    So when you go ahead and you run Scameline with CPU only， you can see that there's
    far。 fewer columns of information that are reported and then you just focus on
    CPU time or potentially。 GPU time。 Okay， so let's walk through a simple example
    of how Scameline's holistic approach to profiling。
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这两者都是极其高效的，几乎不会带来任何开销。因此，当你运行 Scameline 并选择 CPU only 时，你会看到报告的信息列数大大减少，然后你可以专注于
    CPU 时间或可能的 GPU 时间。好的，让我们简单看看 Scameline 的整体分析方法。
- en: can root out performance problems。 So here is some code that uses NumPy。 And
    we'll focus in on this。 so don't worry about it right now。 But here's what a C
    profile profile looks like。 So this is a function level profile and it may be
    hard to see， but what it tells us is。 that main consumes all the runtime， which
    is not super helpful。 By contrast。
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 可以排查性能问题。这里有一些使用 NumPy 的代码。我们将重点关注这一点，所以现在不必担心。但这是一个 C 配置文件的样子。这是一个函数级的配置文件，可能不太容易看出，但它告诉我们
    main 消耗了所有的运行时间，这并不是特别有帮助。相比之下。
- en: here's a profile from line profiler。 And this of course tells us how much time
    is being spent on each of the lines of main。 but it's not particularly actionable。
    It doesn't really give us much of a clue as to what's going on or why there would
    be a。 slowdown or a particular line of code。 So here is Scameline's profile。 I'm
    omitting the function level profile and memory consumption summary for now。
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这是来自行分析器的一个配置文件。当然，它告诉我们 main 的每一行花费了多少时间，但这并不是特别可操作。它并没有真正给我们提供有关发生了什么或为什么某一行代码会出现
    slowdown 的线索。因此，这是 Scameline 的配置文件。我现在省略了函数级配置文件和内存消耗的总结。
- en: So rather than just saying how much time was spent in each line of code， Scameline
    breaks。 down execution time into time spent running Python code， time spent running
    native code。 which means C libraries like NumPy and system time， which includes
    time spent doing IL。 Usually as a Python programmer， if you're trying to make
    a program much more efficient， your。
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，Scameline 并不仅仅是说每行代码花费了多少时间，而是将执行时间分解为运行 Python 代码所花费的时间、运行本地代码所花费的时间，即像
    NumPy 这样的 C 库，以及系统时间，包括执行 IL 的时间。通常作为 Python 程序员，如果你想让程序更高效，你的。
- en: goal is to move execution time out of the Python interpreter and into native
    libraries。 Right now。 it's clear that the code is actually doing just that。 That
    is most of the time is being spent running native code。 So if that was all the
    information you had， you might reasonably conclude that there's。
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 目标是将执行时间从 Python 解释器移出到本地库。目前，很明显代码确实在做这一点。大部分时间都花在运行本地代码上。因此，如果这就是你所拥有的所有信息，你可能会合理地得出结论。
- en: really no optimization opportunity here， it's all running in native。 This is
    as good as you could hope for。 But Scameline provides more information。 So in
    addition to CPU time， as has been mentioned， it produces memory profiles like
    memory profiler。 But it breaks down how much of the memory consumption was from
    Python and how much was， native。
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 这里实际上没有优化机会，所有操作都在本机中运行。这是你所能期待的最佳情况。但Scameline提供了更多信息。因此，除了CPU时间外，正如之前提到的，它还生成内存配置文件，如内存分析器。但它详细说明了内存消耗中有多少来自Python，有多少来自本机。
- en: And where it's blank， it means that all of the memory consumed was in native
    code。 And so here you can see there's a lot of memory consumption happening in
    native code， in line。 six in particular。 In addition to profiling overall memory
    consumption。 Scameline profiles memory usage trends over， time。 So it illustrates
    these things called sparklines。
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在空白处，意味着所有消耗的内存都在本机代码中。因此，在这里你可以看到，在第六行中，本机代码消耗了大量内存。除了对整体内存消耗进行分析外，Scameline还分析了随时间变化的内存使用趋势。因此，它展示了所谓的迷你图。
- en: And a sparkline you can think of as an inline graph。 So the x-axis is time and
    the y-axis is memory consumption。 Each line has its own memory trend sparkline。
    Finally。 Scameline reports a novel metric that we call copy volume。 And this is
    meant to capture several performance problems。 So copying can be quite expensive
    and it's often indicative of a problem like accidentally。 converting between native
    and Python data structures。 And as I mentioned before。 Scameline also reports
    GPU time， but we didn't run this on。
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以将迷你图视为内联图。x轴是时间，y轴是内存消耗。每条线都有自己的内存趋势迷你图。最后，Scameline报告了一个新指标，我们称之为复制量。这是为了捕捉几个性能问题。因此，复制可能非常昂贵，并且通常表明一个问题，比如意外地在本机和Python数据结构之间转换。正如我之前提到的，Scameline还报告了GPU时间，但我们没有在这个上运行。
- en: a GPU so that would all be -- there would be no reports。 And in this case， regardless。
    NumPy doesn't use the GPU。 So it's immaterial。 All right。 So let's dig in。 Let's
    look at this line six， which seems to be where all the action is happening。 This
    one line is responsible for 34% of execution time。 It's all a native code， which，
    like I said。
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 对于GPU来说，这样就不会有任何报告。在这种情况下，无论如何，NumPy并不使用GPU。所以这没有意义。好的，让我们深入研究。让我们看看第六行，似乎所有的操作都发生在这里。这一行负责34%的执行时间。全都是本机代码，正如我所说。
- en: does not necessarily make it a good candidate， for optimization。 But the native
    code is also allocating a lot of memory。 Again， that maybe seems reasonable。 It's
    really a lot。 It's 87% of all memory activity。 But the memory trend is interesting。
    It exhibits this sawtooth pattern。 It goes up and it goes down。
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这并不一定使其成为优化的良好候选者。但本机代码也分配了大量内存。再次强调，这可能看起来合理。确实很多，达到了所有内存活动的87%。但内存趋势很有趣，呈现出锯齿形模式，上升后又下降。
- en: And this indicates that it allocated a big chunk of memory， allocated what looks
    at a。 glance to be the same amount of memory， which it is， and then freed it。
    So this is a big clue。 And this is confirmed by the copy volume number。 The copy
    volume indicates that there's a ton of copying happening。 So what's going on？
    So if we look at this code， np。
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 这表明它分配了一大块内存，分配的内存量在第一眼看来是相同的，实际上也是如此，然后释放了它。因此，这是一个重要的线索。这一点通过复制量的数字得到了确认。复制量表明有大量的复制发生。那么发生了什么呢？如果我们看看这段代码，np。
- en: array is a NumPy library call that converts its argument into， a NumPy array。
    which really means a native C or Fortran array。 But in this case， the argument。
    which is the return value of np。random。uniform， is already， a NumPy array。 And
    unfortunately。 by default， np。array makes a copy of its input。 And in this case，
    that copy is entirely unnecessary。
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: array是一个NumPy库调用，它将其参数转换为NumPy数组，这实际上意味着本机C或Fortran数组。但在这种情况下，参数，即np.random.uniform的返回值，已经是一个NumPy数组。不幸的是，默认情况下，np.array会复制其输入。而在这种情况下，这个复制完全没有必要。
- en: So we can easily speed up this program quite drastically。 So what we're going
    to do is we're going to change this one line of code slightly。 On the bottom。
    we have the optimized version。 And if you look， you can see that the only difference
    between these two versions is that。 we got rid of the redundant np。array call。
    We can immediately see that this has the desired result。
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以轻松地大幅加速这个程序。我们要做的是稍微更改这一行代码。在底部，我们有优化版本。如果你看看，你会发现这两个版本之间唯一的区别是，我们去掉了多余的np.array调用。我们可以立即看到，这达到了预期效果。
- en: CPU time has dropped because we're no longer copying as much。 But more importantly。
    the sawtooth pattern in the memory trend is now gone and the copy， volume has
    gone to zero。 So our modification worked。 We can， of course， verify by testing
    that we get the same results。 There's one change that scaling effectively directly
    led us to dropped peak memory usage， from 1。
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: CPU时间下降是因为我们不再复制那么多。但更重要的是，内存趋势中的锯齿形模式现在消失了，复制量降为零。因此，我们的修改是有效的。当然，我们可以通过测试来验证得到相同的结果。有一个改变，Scaling有效地直接使我们降低了峰值内存使用量，从1。
- en: 6 gigabytes to about 900 megs。 And not only did it reduce maximum memory consumption，
    almost by half。 but it also cut， 15% off total execution time。 So not bad for
    a day's work。 So in the rest of the talk， I'm just going to talk about some technical
    challenges on。 how scaling achieves its precision and unprecedented level of detail
    while maintaining low overhead。
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 从6GB减少到大约900MB。不仅最大内存消耗几乎减少了一半，而且还减少了15%的总执行时间。所以对于一天的工作来说，效果不错。在接下来的讲座中，我将讨论一些技术挑战，如何在保持低开销的同时，Scaling实现其精度和前所未有的细节水平。
- en: And one of the most pernicious challenges is how Python handles signals。 Scaling
    relies on signals to track many aspects of performance。 Whenever a timer goes
    off。 Scaling looks to see what line of code is currently executing。 and it also
    measures the load on the GPU。 And over time。
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 而最令人头疼的挑战之一是Python如何处理信号。Scaling依赖信号跟踪性能的多个方面。每当定时器响起时，Scaling会查看当前正在执行的代码行，并且还会测量GPU的负载。随着时间的推移。
- en: this very accurately predicts which lines of code are consuming the most， time。
    Now。 the way that delivery works in Python is that if you're running Python code，
    the delivery。 of signals happens promptly。 So if you say I want a timer interrupt
    to go off every 0。01 seconds。 it pretty much will， happen every 0。01 seconds when
    you're running Python bytecodes most of the time。
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这非常准确地预测了哪些代码行消耗了最多的时间。现在，Python中的交付方式是，如果你在运行Python代码，信号的交付会迅速发生。因此，如果你说我想让定时器每`0.01秒`中断一次，它几乎会在你运行Python字节码时每`0.01秒`发生一次。
- en: Because Python delivers signals right after it executes one bytecode in its
    interpreter。 However。 if you are running native code， so if one of these bytecodes
    actually is a call。 to a native function， like a NumPy call， for example， the
    whole execution time will have。 no signals delivered until Python regains control。
    So in effect。
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 因为Python在其解释器中执行一个字节码后立即交付信号。然而，如果你正在运行本地代码，例如，如果这些字节码之一实际上是对本地函数的调用，比如NumPy调用，那么在Python重新获得控制之前，整个执行时间将不会交付信号。因此，实际上。
- en: it's as if from the perspective of a sampling profiler that no time elapsed，
    whatsoever。 If Scaling just relied on timer signals alone， it would misreport
    where programs are spending。 their time。 This would really degrade the quality
    of information from the perspective of a Scaling user。 To combat this， Scaling
    uses an algorithm that infers how much time was spent in native， code。
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 从采样分析器的角度来看，就好像没有任何时间流逝。如果Scaling仅仅依赖于定时器信号，它会错误报告程序花费时间的地方。这将真正降低Scaling用户信息质量。为了解决这个问题，Scaling使用了一种算法来推断在本地代码中花费了多少时间。
- en: It does this by checking the current time and here we're looking at the so-called
    virtual。 clock that is to say the time spent when the process is actually scheduled
    for execution。 And it checks the current time according to that clock。 Then when
    it gets another signal。 it checks again to see what the time was。 And it turns
    out that you can then actually accurately say how much time was spent in Python。
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 它通过检查当前时间来实现这一点，这里我们看到所谓的虚拟时钟，也就是说，进程实际调度执行时所花费的时间。它根据该时钟检查当前时间。当它收到另一个信号时，它再次检查看看时间是多少。结果表明，你可以准确地说出在Python中花费了多少时间。
- en: and how much time was spent in native code。 Intuitively。 this makes sense if
    the program was delayed beyond the expected interval。 So the interval was say
    0。01 seconds and it took 10 seconds。 We kind of know where those 10 seconds minus
    0。01 seconds went。 They went to C code。 And so there's a little formula here that
    we can show leads to an accurate prediction of。
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 以及在本地代码中花费了多少时间。直观上，这是合理的，如果程序的延迟超出了预期的间隔。例如，间隔是`0.01秒`，但花了10秒。我们大致知道那10秒减去`0.01秒`去哪了。它们去了C代码。因此，这里有一个小公式可以准确预测。
- en: the amount of time spent in Python code and its C code。 And I'll show you this
    in just a second。 In addition， we can rely on the same insight to separate out
    system time from total execution， time。 So here， Scaling records the actual wall
    clock time， which includes any time spent on。 the CPU or sleeping because of I/O。
    And this lets Scaling compute how much time was spent in system time for each
    line of。
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Python 代码和其 C 代码中花费的时间。稍后我会给你展示这一点。此外，我们可以依靠相同的洞察力将系统时间与总执行时间分开。因此，Scaling
    记录了实际的墙钟时间，其中包括在 CPU 上或因 I/O 而处于休眠状态的任何时间。这使得 Scaling 能够计算每行代码中花费的系统时间。
- en: code。 So here I have an example that demonstrates Scaling's ability to tease
    apart native code。 execution time and Python time。 This particular program is
    designed to spend 0。7 seconds in a native extension that we wrote， specifically
    for this purpose and 0。3 seconds inside of Python。 And Scaling accurately determines
    that the Python code is consuming roughly 30% of the。
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 代码。因此，这里我有一个示例，展示了 Scaling 拆分本地代码执行时间和 Python 时间的能力。这个特定程序被设计为在我们为此目的编写的本地扩展中花费
    0.7 秒，在 Python 中花费 0.3 秒。Scaling 准确地确定 Python 代码消耗了大约 30% 的时间。
- en: execution time and the C code roughly 70% of the time。 This despite the fact
    that the sampling is not being delivered， right， the timer signals。 are not being
    delivered during the entire time the C code is running。 It's quite accurate。 The
    error here is just 1%。 So we're going to look at two more features of Scaling
    quickly。
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 执行时间和 C 代码大约占 70% 的时间。尽管在 C 代码运行的整个过程中并没有发送计时器信号，采样仍然是相当准确的。这里的误差仅为 1%。因此，我们将快速查看
    Scaling 的另外两个功能。
- en: One is GPU profiling。 Here is an image of Scaling running in a Jupyter Notebook
    on a system that has GPUs。 So Scaling does not yet do full profiling on Jupyter
    Notebooks， but we're working on， it。 It does CPU profiling and GPU profiling，
    but not the memory profiling。 So to profile a program with Scaling， as usual you
    pip install it， and then in a Jupyter Notebook。
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 一个是 GPU 分析。这是 Scaling 在拥有 GPU 的系统中运行在 Jupyter Notebook 上的图像。因此，Scaling 目前尚未在
    Jupyter Notebook 上进行全面分析，但我们正在努力。它执行 CPU 分析和 GPU 分析，但不进行内存分析。要使用 Scaling 对程序进行分析，像往常一样进行
    pip 安装，然后在 Jupyter Notebook 中。
- en: you load it with %loadxt， and then you finally run it with %scrun。 And here's
    what a profile looks like。 If Scaling detects that there is an NVIDIA GPU on the
    system。 it will automatically do， GPU profiling。 And you can see the results here。
    This is on a program that uses PyTorch， which does take advantage of available
    GPUs。
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 你使用 %loadxt 加载它，然后最后使用 %scrun 运行它。这就是分析的样子。如果 Scaling 检测到系统上有 NVIDIA GPU，它将自动进行
    GPU 分析。你可以在这里看到结果。这是一个使用 PyTorch 的程序，确实利用了可用的 GPU。
- en: if you tell it to。 And you can see in addition to profiling Python， native，
    and system time。 Scaling is now showing， in the final column right before the
    source code。 GPU time also is a percent。 And Scaling implements GPU profiling
    using the timer-driven sampling just as with the CPU。 time。 It measures the load
    on the GPU and attributes it to the currently running line of code。
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，除了对 Python、本地和系统时间进行分析外，Scaling 现在在源代码前的最后一列中显示 GPU 时间也占一定百分比。Scaling
    使用与 CPU 时间相同的基于计时器的采样实现 GPU 分析。它测量 GPU 的负载，并将其归因于当前正在运行的代码行。
- en: And over time， again， this delivers a precise and accurate reporting of the
    time spent。 So now let's talk finally about how Scaling performs automatic memory
    leaks detection。 And at a high level， imagine that for every line， every time
    a line of Python code allocates。 some memory， we say that it got heads on a coin
    flip。 And every time that that memory is freed。
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 随着时间的推移，这再次提供了对花费时间的精确和准确的报告。那么现在最后让我们谈谈 Scaling 如何执行自动内存泄漏检测。从高层次来看，想象一下，每当一行
    Python 代码分配一些内存时，我们就说它在掷硬币时得到了正面。每当该内存被释放时，我们称之为反面。
- en: we call it tails。 So if that line of code is not leaking， every single time
    it allocates memory。 eventually， there's going to be a matching free when it reclaims
    that memory。 And so over time。 you'll get a bunch of heads and a bunch of tails。
    And if they balance out， then we can conclude。 quite reasonably， that there is
    no memory leak。 If however。
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果那行代码没有泄漏，每次分配内存时，最终在回收该内存时会有一个匹配的释放。因此，随着时间的推移，你将得到一堆正面和一堆反面。如果它们能够平衡，那么我们可以合理地得出结论：没有内存泄漏。然而，如果你告诉它。
- en: these allocations don't get paired up with any freeze， and we just get allocations。
    allocations allocations， you can conclude， quite safely， that you do have a memory
    leak on。 your hands。 So tracking this for every single allocation performed by
    Python or a native library。 and， then associating it with every line of code，
    and then checking every free to see which things。
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这些分配没有与任何冻结配对，我们只获得了分配。分配分配，你可以安全地得出结论，你确实手上有一个内存泄漏。因此，跟踪Python或本机库执行的每个分配，然后将其与每行代码关联，再检查每个释放以查看哪些东西。
- en: were allocated would be very， very costly。 So instead， as usual， Scaling does
    this via sampling。 So Scaling randomly chooses allocations to track。 It records
    the address of the allocated object。 And then， on every free， it does a quick
    comparison to see if that same object was just， freed。 If。 over and over again，
    allocated objects are not freed， then we can conclude that the。
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 被分配将是非常非常昂贵的。因此，Scaling像往常一样通过采样来完成这个过程。Scaling随机选择要跟踪的分配。它记录分配对象的地址。然后，在每个释放时，它进行快速比较，以查看是否同一个对象刚刚被释放。如果反复出现，分配的对象没有被释放，那么我们可以得出结论。
- en: line is leaking memory。 So conceptually， Scaling assumes that all lines of code
    start with an equal number of。 allocations and freeze， right？ And one heads and
    one tails， meaning no leaks。 It then gradually builds up information as the program
    is running， sampling allocations。 and recording whether they were freed。 And over
    time。
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 行正在泄漏内存。因此，从概念上讲，Scaling假设所有代码行开始时都有相等数量的分配和冻结，对吧？一头一尾，意味着没有泄漏。它在程序运行时逐渐构建信息，采样分配并记录它们是否被释放。随着时间的推移。
- en: we will see a pattern that will start to emerge。 And after we have enough data。
    we can conclude when the allocations are roughly balanced， there's， no leak。 and
    when they're quite unbalanced， it indicates a leak。 And when Scaling concludes
    with high probability that there is a leak， it reports the probability。
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将看到开始出现的模式。在我们有足够的数据后，我们可以得出结论，当分配大致平衡时，没有泄漏，而当它们非常不平衡时，则表示存在泄漏。当Scaling以高概率得出存在泄漏的结论时，它报告该概率。
- en: in the volume in megabytes per second， which indicates how serious the leak
    is， which lets。 you as the programmer focus your effort on the leaks that really
    matter。 So in conclusion。 I hope I've convinced you that Scaling is not yet another
    profiler。 It's a very different profiler。 It's precise， providing unprecedented
    levels of detail for CPU time while simultaneously。
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 每秒兆字节的体积，表示泄漏的严重性，让你作为程序员专注于真正重要的泄漏。因此，总之，我希望我能让你相信，Scaling并不是另一个性能分析器，而是一个非常不同的性能分析器。它非常精确，为CPU时间提供前所未有的详细信息，同时。
- en: profiling memory， GPU utilization， copy volume， and identifying leaks。 It's
    reasonably fast。 and it's accurate， correctly attributing time for code that uses
    threads or， multiprocessing。 And finally， it's easy to install。 It works on Linux，
    Mac， and Jupyter Notebooks。 We look forward to feedback and success stories， of
    course， on using Scaling to identify and。
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 对内存、GPU利用率、复制量进行分析，并识别泄漏。这相对较快，并且准确地将时间归因于使用线程或多处理的代码。最后，它易于安装。它在Linux、Mac和Jupyter
    Notebooks上运行。我们期待反馈和成功案例，当然，使用Scaling来识别和。
- en: fix performance problems in your code。 And thank you for your attention。 [BLANK_AUDIO]。
    [BLANK_AUDIO]， [BLANK_AUDIO]， [BLANK_AUDIO]， [BLANK_AUDIO]， [BLANK_AUDIO]， [BLANK_AUDIO]。
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 修复代码中的性能问题。感谢你的关注。[BLANK_AUDIO]。[BLANK_AUDIO]，[BLANK_AUDIO]，[BLANK_AUDIO]，[BLANK_AUDIO]，[BLANK_AUDIO]，[BLANK_AUDIO]。
- en: '![](img/a9915b6f966cee2d9257e21938089fd0_3.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a9915b6f966cee2d9257e21938089fd0_3.png)'
