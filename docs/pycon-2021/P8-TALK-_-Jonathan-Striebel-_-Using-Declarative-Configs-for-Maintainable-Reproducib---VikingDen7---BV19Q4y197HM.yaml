- en: P8：TALK _ Jonathan Striebel _ Using Declarative Configs for Maintainable Reproducib
    - VikingDen7 - BV19Q4y197HM
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[MUSIC]。'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/653290599afbe20edf764ce6a6535ff6_1.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/653290599afbe20edf764ce6a6535ff6_2.png)'
  prefs: []
  type: TYPE_IMG
- en: Hello everyone。 I'm very happy to welcome you to my talk about declarative conflicts
    for。 a maintainable reproducible code。 My name is Jonathan and you can write me
    an email or follow me on Twitter。 I'm working for Scalable Minds， a software company
    in Potsdam， Germany。 We're working on image analysis tools and services。 Before
    diving into the topic in detail。
  prefs: []
  type: TYPE_NORMAL
- en: I'd like to give you an overview of the domain I'm working in and the problems
    we are facing。 At Scalable Minds， we've built webmasters， an open source online
    tool that allows you to view and。 annotate 3D volumetric image data。 What you
    see here is a 3D scan of brain tissue from a microscope。 The different cells are
    segmented as shown by the color and。
  prefs: []
  type: TYPE_NORMAL
- en: one cell is rendered in 3D at the bottom center。 If you want to try it for yourself。
    you could go to webnossos。org or use this QR code。 We are also automating the
    annotation of such cells， which is what I'm working on most of the time。 For that，
    we are running experiments。 It starts with multiple terabytes of grayscale image
    data。
  prefs: []
  type: TYPE_NORMAL
- en: On that， we train machine learning systems to detect where the cell boundaries
    are。 Therefore。 we manage the training data and evaluation data， which was acquired
    manually via webnossos before。 The training usually takes up to weeks。 Afterwards。
    we are running segmentation and agglomeration algorithms。
  prefs: []
  type: TYPE_NORMAL
- en: The result is then a dense 3D reconstruction of all neurons in the data。 which
    is then used for biological analysis by our clients。 Since we were working with
    massive data and computation heavy algorithms。 we run this pipeline paralyzed
    in high-performance computing clusters。
  prefs: []
  type: TYPE_NORMAL
- en: This is just an overview of a single experiment。 Since the domain is very much
    researched driven。 we constantly iterate on our pipeline。 We might run an experiment
    with dataset 1 and develop a feature A to improve the results。 Additionally， we
    start with dataset 2。 On the first dataset。 we add feature B and figure out that
    we should replace feature A with feature C after some time。
  prefs: []
  type: TYPE_NORMAL
- en: Also， we apply feature B and a new feature D on the second dataset。 and afterwards
    also use D on dataset 1。 This happens with many more in-between steps on a time
    scale of months and years。 and might give you an idea of the problems we run into。
    We need to keep an overview of the features we use for each dataset in experiment。
  prefs: []
  type: TYPE_NORMAL
- en: and be able to really run an experiment maybe a year later， but with the up-to-date
    pipeline。 so that we can simply switch on the feature that we just developed yesterday。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/653290599afbe20edf764ce6a6535ff6_4.png)'
  prefs: []
  type: TYPE_IMG
- en: Those challenges motivate this talk。 We need to keep our application hierarchy
    maintainable。 and at the same time reproducible。 Reproducible not only in the
    sense of pinning the code and the packages of a specific experiment。 but being
    able to rerun that old experiment with the current codebase。 Therefore。 we have
    four main points that helped us to accomplish this。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/653290599afbe20edf764ce6a6535ff6_6.png)'
  prefs: []
  type: TYPE_IMG
- en: First， we separate our feature flags and parameters， the configuration， from
    the rest of the code。 This splits the structure of the experiments and the evolving
    pipeline。 and gives us the notion of the experiment hierarchy I've just shown
    before。 Second。 we want to verify the configuration of an experiment against the
    available features。
  prefs: []
  type: TYPE_NORMAL
- en: but also the other way around， verifying that the code only uses available configuration
    parameters。 Finally， since the configurations might change over time。 we want
    to have automated migrations or evolutions in place。 First。 I'd like to give you
    an overview about the options to implement this。
  prefs: []
  type: TYPE_NORMAL
- en: before then diving into the actual code。 For this talk。 I've prepared a toy
    example that is much more simple than our usual experiments。 but the mechanisms
    are basically the same。 In the example， we load a dataset in this case。 the linear
    root dataset about sport exercises。 You can see different sport exercises and
    the numbers of repetitions that a person could do。
  prefs: []
  type: TYPE_NORMAL
- en: in the different rows。 We perform outlier detection on it。 and then plot the
    data where an outlier here is marked in orange。 For the plot。 we have to select
    two of the available columns for the different axes。 In this case， we have chins。
    sit-ups， and chumps available in the dataset， and plot the chumps and the chins。
    Here。
  prefs: []
  type: TYPE_NORMAL
- en: you can see that one person can do much more chumps， than the others。 and therefore
    is marked as an outlier here in the top。 The code for this experiment would look
    roughly like this。 We load the data from some path。 find the outliers， where we
    might have to specify a threshold in this case 10。
  prefs: []
  type: TYPE_NORMAL
- en: and plot the data using two specific axes。 Now， when we extract the configuration。
    we have to extract， in this case， the path， the threshold， and the two axes for
    the plot。 You could。 of course， simply write a function with a four parameters，
    but instead。 I recommend to keep this completely separate of your code， using
    a declarative configuration。
  prefs: []
  type: TYPE_NORMAL
- en: Declarative means that only data can be specified here， in contrast to imperative
    programming。 such as with a Python programming language， which can use if clauses
    or for loops。 The benefit of separating this and using a declarative config。 that
    this forces you to keep a simple configuration。
  prefs: []
  type: TYPE_NORMAL
- en: and it cannot contain any logic that instead must be specified in your application。
    So。 we need a declarative input format for the configuration。 Then this needs
    to be represented in our code base。 And to turn this input format into the representation。
  prefs: []
  type: TYPE_NORMAL
- en: we need some sort of de-serialization in between。 Let's have a look what our
    options for those are。 For the input format， a typical choice is to use arguments
    for a command line interface。 Therefore。 all configuration must be supplied on
    the command line。 which doesn't work too well if you have a lot of parameters。
    It's great for a few parameters though。
  prefs: []
  type: TYPE_NORMAL
- en: The usual choice is to use the Python build in ArcPERS to load the arguments。
    But here。 nothing prevents you from accessing a round key of your configuration。
    which would result then in a runtime error。 Therefore， I prefer to use the external
    package-typer。 which allows you to specify the expected parameters and data types
    as function arguments。
  prefs: []
  type: TYPE_NORMAL
- en: which then can also be type-checked， more about that in a second。 Another alternative
    is to use environment variables for the input， which can be accessed via OS。enviren
    in Python。 What I prefer for large configs is to extract them into a separate
    file。 In general， we use YAML for that， but there's many other choices like JSON，
    Tumble or any。
  prefs: []
  type: TYPE_NORMAL
- en: To load a YAML file， you need a third-party library， such as PyYAML。 which gives
    you a Pythonic representation of that file。 This representation consists of the
    basic Python types， such as dictionaries， lists， integers， etc。 The problem here
    is again that we could access the wrong key， which would result in a runtime error。
  prefs: []
  type: TYPE_NORMAL
- en: Since our experiments run for days， this needs to be cached earlier， for example
    in tests。 an additional possibility that we use is， to turn the configuration
    into a class。 The class then defines the different parameters and their types
    as their attributes。 To get the init method and other helper methods， we use the
    utter library。
  prefs: []
  type: TYPE_NORMAL
- en: which adds them automatically。 Now we access the parameters of the config object
    as usual attributes。 This allows us to use a tag checker before running the code。
    which catches the wrong usage beforehand。 Since loading a YAML file into Python
    gives us the basic Python structure we have on the left。 we need a tool to convert
    this into the object of the class on the right。 For this。
  prefs: []
  type: TYPE_NORMAL
- en: we can directly use the see address library， which provides converters for many
    data types and can also be adapted with custom converters。 Before diving into
    the code， let's have a look at the landscape of possibilities you have if you。
    want to implement a similar system。 I've shown three different options how you
    can supply your。 config data to your application。 Then this needs to be represented
    in your code。
  prefs: []
  type: TYPE_NORMAL
- en: for which you have many options。 If you want to add type information to a plain
    dictionary。 there is typing。type。dict。 We rather using objects of custom classes
    as shown before。 To automate the special methods of those classes， you can use
    named tuples or data classes。 which are part of Python。 If you have more complex
    scenarios， you might want to use the third party。
  prefs: []
  type: TYPE_NORMAL
- en: party library， such as pedantic， which is great， or in our case， attors。 I'm
    not providing a detailed comparison here， this would be just too much for this
    talk。 To create an object of such a class， you also need a converter。 Typeload
    is a popular library。 that side is specifically for data classes， pedantic comes
    with built-in converters for。
  prefs: []
  type: TYPE_NORMAL
- en: its classes， and then there's see address， which I prefer， because you can write
    custom。 structures for your own classes。 Finally， I'd recommend to use a type
    checker to validate。 the usage of the config objects in your code， such as myPy
    or PyType。 So let's see how this would look like in code。 Here I've prepared the
    example I've shown you before。
  prefs: []
  type: TYPE_NORMAL
- en: in a TruePider notebook。 In this case， we're loading another dataset， the Iris
    dataset。 which is about flowers。 We also perform outlier detection， this time
    with a slightly different。 threshold， and then plot the data where we choose two
    axes。 In this case， the data consists of。 four columns with different attributes
    about sizes of the flowers。 Also， we have specified here。
  prefs: []
  type: TYPE_NORMAL
- en: already which of the rows is an outlier or not。 Then we see the result for the
    two different axes。 in the plot below。 Also， I have another example， the one I've
    just shown before。 with the linear root dataset and another outlier factor here
    of 10。 We use， of course。 two different axes to plot。 Again， the chins and the
    chumps and this results in just the plot you've。
  prefs: []
  type: TYPE_NORMAL
- en: seen before。 When we want to convert this code into a code where we supply a
    split configuration。 we can start by just writing this config file。 In this case，
    for the first example， we would。 specify the dataset Iris in this case。 We specify
    50 for the outlier factor and the two different。 axes for the plot。 We can do
    the same for the other example where we have another config。
  prefs: []
  type: TYPE_NORMAL
- en: this time with the linear root dataset， another parameter and two other axes
    for the plot。 If we want to use this in our two-pider notebook here， we have to
    specify a class。 as I have shown before。 In this case， it's the config schema
    class where we have a dataset。 We look into the dataset type in a second and the
    outlier number， which is an integer。
  prefs: []
  type: TYPE_NORMAL
- en: and the two axis parameters that are strings。 So the dataset parameter here
    is an enum。 In this。 case， we can just have two options for this enum， linear
    root and Iris。 So to load now this configuration， into an object of this config
    schema class。 we have to first load the file when we have opened the file， here。
    We load it with yaml load。
  prefs: []
  type: TYPE_NORMAL
- en: which gives us a dictionary with just the structure of the file。 Then we can
    use c-adgers。 which I've presented before， to structure this dictionary into an
    object， of this class。 the config schema。 So in the end， the result is the config
    here below。 with just the data we've supplied in our config file。 This now can
    be used to adapt the code that。
  prefs: []
  type: TYPE_NORMAL
- en: we've seen before， we just check which of the two datasets we have。 Then we
    use the parameter。 from the configuration for the outline detection， and also
    the two axis we've supplied。 Now。 instead of having the two experiments in the
    notebook， we can simply use the different。 configuration。 Now， this is now for
    the configuration with the linear dataset。
  prefs: []
  type: TYPE_NORMAL
- en: We load this configuration， and rerun this part and generate the fitting plot
    here。 Now。 if you've made a mistake and you， supplied here instead of plot y，
    plot z。 you can check this ahead of running this using myPY， the static type checker。
    In this case。 using nbqa， which can run this on notebooks， we start myPY， using
    this notebook。
  prefs: []
  type: TYPE_NORMAL
- en: We have to save it before， obviously。 And can now see that we have an error
    because we can't use plot z。 which isn't part of our schema， but instead have
    to use y or x。 So if we fix this again。 we're back to no error。 So what you've
    seen now is that we can separate our config and code using a declarative。 yaml
    config in this case。 We can verify our configuration since we loaded into our
    defined schema class。
  prefs: []
  type: TYPE_NORMAL
- en: using sea others。 And we can verify then the usage of this configuration in
    our code using myPY。 Let's have a look how this would be in a more complex use
    case。 In this case。 we are not only plotting a simple scatterplot of the data，
    but we want to add more。 possibilities for plotting。 So if we have a plot now，
    instead of only using x and y， we also define。
  prefs: []
  type: TYPE_NORMAL
- en: the C attribute， which is optional， since we still can do 2D plots。 Another
    alternative to have instead of the scatterplot is now a heatmap plot。 which just
    uses x and y。 Since we can only supply one of the two plotting possibilities。
    which has to define a union of those， which basically means that an object of
    the union of the plot schema is either a scatterplot schema。
  prefs: []
  type: TYPE_NORMAL
- en: or a heatmap schema。 And you can see which of those it is by looking at the
    kind which either is。 scatter， in this case， it just can be the scatter string
    or heatmap。 Then just as before。 we can use this in our config schema。 So we have
    a nested schema here in our。 config schema just for the plot。 The data is just
    the same as before， we have an enum here。
  prefs: []
  type: TYPE_NORMAL
- en: and we have the integer for the outliers。 But this now is the union of the two
    schemata we defined here。 above。 So now we have to adapt our configuration。 In
    this case， we still take the linear data set。 we use still the same number for
    the outlier detection， we do a scatterplot， and then if we。 just supply chains
    and sit ups， this results in exactly the same as before。 Code is a bit more。
  prefs: []
  type: TYPE_NORMAL
- en: complex。 Now， since we have if clauses for the different plotting possibilities。
    So here。 just have the same scatterplot as before。 Now we can also add the optional
    Z parameter。 And we reload this。 And see in the end， now we have the 3d version
    of the plot。 How does this。 work in detail again？ So because here we are using
    the scatterplot schema。
  prefs: []
  type: TYPE_NORMAL
- en: with this time supplied the， set attribute， and in the if part here below。 we
    find out if z is defined or not。 And since it is， defined， we're doing now a scatterplot。
    So let's say， okay， we want to do the heatmap this time。 So again。 we just use
    the heatmap parameter from here， supplied in the config。
  prefs: []
  type: TYPE_NORMAL
- en: reload the config and run our code。 And now we get an error， because we couldn't
    parse this。 configuration， since C was defined。 Since C is not part of the 2d
    heatmap， we have to remove it。 and can rerun this。 And simply verifying this config
    can also be done easily ahead of time。 So in this time， we just get a heatmap。
    Now， we've adapted the configuration for B manually。
  prefs: []
  type: TYPE_NORMAL
- en: but instead of adapting this manually every time， we just want to have an evolution。
    So we still have， the configuration of a， and we want to run this old configuration
    also with a。 So for this， I've， prepared here an evolution。 Could look simply
    like this。 we load the dictionary of the config， called a draw config in this
    case。
  prefs: []
  type: TYPE_NORMAL
- en: then we look if there's a version defined。 So here， you'd have， to say， okay。
    the new config also has a version key。 This would be two in this case， and the
    default。 version for our old versions for our old version conflicts are simply
    one。 And if we have an old。 config， we just run our evolution， which then transforms
    those two plots into the plotting。
  prefs: []
  type: TYPE_NORMAL
- en: possibilities that we have now。 So now instead of loading the new configuration，
    below here。 we start with the old configuration and run evolution。 And can now
    use just the old experiment with the updated code， where the configuration is。
    migrated automatically。 So what we have are the config files that are transferred
    into the。
  prefs: []
  type: TYPE_NORMAL
- en: dictionaries， which are then transferred into the objects。 So we always have
    an up-to-date schema class， for our newest code。 And now what I've just shown
    before is to use an evolution。 where we have versioned config files and do the
    evolutions on the dictionaries。
  prefs: []
  type: TYPE_NORMAL
- en: So we know it only the simple schema class of the current code。 As an alternative。
    you can also do the evolutions on your structured objects， but then you also need
    to keep all the。 old schema classes that you had defined before。 So I've shown
    you this overview before， you might。 miss some more libraries here which are built
    to track experiments。 Those are sacred， for example。
  prefs: []
  type: TYPE_NORMAL
- en: an L flow or a guild， and they are great to tag your experiments and can be
    combined with。 the configuration mechanisms I've described here， but they don't
    have the same possibilities on their。 own， such as type checking or evolutions。
    They are especially useful to track metrics across your experiments。 So to summarize，
    I've also now shown you two ways to evolve your configs so that you can run and。
  prefs: []
  type: TYPE_NORMAL
- en: migrate old experiments with a backwards compatible application。 Let's have
    a look at our goals again。 First， the maintainability。 The separation of the declarative
    config helps us to keep an overview of。 our experiments and the config and code
    verification helps us that those work as expected in our pipeline。
  prefs: []
  type: TYPE_NORMAL
- en: Also， this verification together with a migration system also ensures that old
    experiments are。 reproducible with our ever evolving pipeline。 So feel free to
    contact me if you have any questions。 and I'm very happy to discuss any of those
    topics in detail。 I hope that this gave you some useful。 input for maintainable
    and reproducible applications。 Thank you very much。 (Silence)， (Silence)。
  prefs: []
  type: TYPE_NORMAL
- en: (Silence)， (Silence)， (Silence)， (Silence)， (Silence)， [ Silence ]。
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/653290599afbe20edf764ce6a6535ff6_8.png)'
  prefs: []
  type: TYPE_IMG
