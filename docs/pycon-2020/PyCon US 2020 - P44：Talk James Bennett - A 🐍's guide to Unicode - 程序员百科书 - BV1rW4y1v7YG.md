# PyCon US 2020 - P44ÔºöTalk James Bennett - A üêç's guide to Unicode - Á®ãÂ∫èÂëòÁôæÁßë‰π¶ - BV1rW4y1v7YG

 HiÔºå I'm James and I'm here today to talk to you about Unicode„ÄÇ



![](img/2a3245dcd9af276985281a539c729f78_1.png)

 Now I know that word can provoke some reactions in peopleÔºå so the very first thing I want„ÄÇ

 to do is remind you of a wise message from a beloved modern philosopher„ÄÇ

 Because Unicode is complex and I'm sure you've heard scary stories about itÔºå but it's not„ÄÇ

 something that you have to be afraid of and by the end of this talk I hope you won't be„ÄÇ

 afraid of it„ÄÇ Because you're going to understand where Unicode came from„ÄÇ

 what it is and how it really worksÔºå how it gets implemented in computer systems and especially programming languages like„ÄÇ

 Python and how you can work with it and recognize where the complexity in it is found so that„ÄÇ

 you can manage that complexity and write more effective and more confident code so that you„ÄÇ

 don't have to be afraid of Unicode anymore„ÄÇ

![](img/2a3245dcd9af276985281a539c729f78_3.png)

 But of course we have to start somewhereÔºå which means starting at the beginning„ÄÇ

 And the history of Unicode really is the history of written textÔºå which is kind of complicated„ÄÇ



![](img/2a3245dcd9af276985281a539c729f78_5.png)

 Because we're still not sure entirely what that history looks like„ÄÇ

 We know writing seems to have been invented independently multiple times throughout history„ÄÇ

 and over the thousands of years since then people have come up with almost an unbelievable„ÄÇ

 number of different ways of writing down their thoughts and words„ÄÇ

 Writing you can think of as a basis for a writing system has probably been used at someÔºå point„ÄÇ

 There are writing systems that are based on individual sounds or syllables or whole wordsÔºå or ideas„ÄÇ

 There are writing systems that are based on abstract representations of an idea„ÄÇ

 There are writing systems that are based on the position of your mouth as you pronounceÔºå a sound„ÄÇ

 Really anything you can imagineÔºå probably someone has come up with„ÄÇ

 And the history of text encompasses all of those thingsÔºå which can be pretty complicated„ÄÇ

 Now fortunately for this talk we really only need to talk about the last couple of centuries„ÄÇ

 When we've tried to come up with systems for representing and transmitting text electronically„ÄÇ

 There are older systems for long distance transmission„ÄÇ

 There are semaphore systems and flag codes and signal fires and many other systems that„ÄÇ

 were really effective„ÄÇ But today we're mostly concerned with electronic or electromagnetic broadcast over a wire or„ÄÇ

 radio waves„ÄÇ And we've probably all seen some examples of early attempts at this„ÄÇ



![](img/2a3245dcd9af276985281a539c729f78_7.png)

 This is one a lot of people know is Morse codeÔºå which was developed for a telegraph system„ÄÇ

 It's a variable width encoding uses binary alphabet of two charactersÔºå a dot and a dash„ÄÇ

 And there was a whole family of different telegraph codes with different principles and based on„ÄÇ

 different ideas„ÄÇ One of the more popular later on was ITA„ÄÇ

 which is a baudot code named after Emil BaudotÔºå who„ÄÇ

 also gives us the baud as a unit of transmission rate„ÄÇ

 And baudot codes are kind of interesting because they introduced this concept of control characters„ÄÇ

 If you look at thisÔºå it's a five bit binary code„ÄÇ Normally these messages would be recorded by being punched as holes in a paper tape„ÄÇ

 whichÔºå sounds like it should only be able to handle 32 characters„ÄÇ That's two to the fifth power„ÄÇ

 But here the capacity is over 60 characters because it uses a control character to switch„ÄÇ

 between two different alphabets of characters„ÄÇ These sorts of clever innovations let people do a lot of cool things with the telegraph system„ÄÇ

 as it evolved and eventually developed into modern computer text encoding systemsÔºå many„ÄÇ

 of which were heavily influenced by these telegraph codes„ÄÇ

 This of course is the 100 pound gorilla in the roomÔºå ASCIIÔºå which owes a lot to ITA2„ÄÇ

 and the baudot family of telegraph codes„ÄÇ But ASCII really took over the world even though it shouldn't have„ÄÇ

 The problem with ASCII of course is it's the American standard codeÔºå which is a problem„ÄÇ

 in a world that contains a lot more countries than America and a lot more languages thanÔºå English„ÄÇ

 which meant that of courseÔºå even though ASCII was built into a lot of systems„ÄÇ



![](img/2a3245dcd9af276985281a539c729f78_9.png)

 and still is today and a lot of things will assume ASCII„ÄÇ

 Lots of different people developed text encodings to represent their languagesÔºå their dialects„ÄÇ

 their regionsÔºå their countries„ÄÇ There are a huge number of them out there„ÄÇ

 This is just a subset that I took from a list that I found online„ÄÇ

 And of course that brought its own set of problems because how do you work together with„ÄÇ

 so many different possible encodingsÔºü How do you avoid the kinds of bugs and translation and encoding problems that can come up when„ÄÇ

 you have this many different options and you may not even know which of them are beingÔºå usedÔºü

 Wouldn't it be great if we just had one universal agreed-on solutionÔºü



![](img/2a3245dcd9af276985281a539c729f78_11.png)

 WellÔºå that's what Unicode is supposed to be„ÄÇ And it's worth pausing for a moment to make sure we understand really what Unicode is„ÄÇ



![](img/2a3245dcd9af276985281a539c729f78_13.png)

 A lot of single page guides will really make a point of saying Unicode is not a character„ÄÇ

 set and Unicode is not an encoding„ÄÇ It's much more productive to think of Unicode as a set of databases and specifications and„ÄÇ

 rules and properties for describing different human writing systems that we know about„ÄÇ And yes„ÄÇ

 some of those include ways to encode it into binary text„ÄÇ Yes„ÄÇ

 some of those include sets of charactersÔºå but Unicode itself is so much more than any„ÄÇ

 of those individual components„ÄÇ And of course that means it's also complex„ÄÇ And it really has to be„ÄÇ

 If you think about that long list of different encodingsÔºå Unicode has to try to do the job„ÄÇ

 of all of them and handle all of the things that they handled„ÄÇ

 If any given individual encoding only needed to handle perhaps one languages or one dialects„ÄÇ

 or one region's particular language and rules for writingÔºå but Unicode has to be able to„ÄÇ

 handle them all„ÄÇ There's a lot of complexity that's just inherent to that task„ÄÇ

 And of course that means it's very different from those older single purpose encodingsÔºå but„ÄÇ

 different doesn't have to be the same thing as scary„ÄÇ

 And I hope that's something you'll take away from this talk„ÄÇ

 Now we need to dive a little bit deeper into the terminology just to be able to talk usefully„ÄÇ



![](img/2a3245dcd9af276985281a539c729f78_15.png)

 about Unicode„ÄÇ So let's stop and do a quick glossary check„ÄÇ

 Because often we fall into very informal terminology„ÄÇ Like we start talking about characters„ÄÇ

 And Unicode does have a concept of characterÔºå but it's much more of an abstract entity than„ÄÇ

 in those older encodings and character sets„ÄÇ The basic atoms of Unicode„ÄÇ

 the things that make it up are called code points„ÄÇ

 And you might try to think of a code point as a characterÔºå but we're going to see examples„ÄÇ

 of why that's risky„ÄÇ And Unicode itself is organized into planes of code points„ÄÇ

 two to the 16th or 65Ôºå536Ôºå code points per plane„ÄÇ Originally there was just one plane„ÄÇ

 Now there are 17 of them„ÄÇ And code points are much like Unicode's concept of a character„ÄÇ

 still sort of an abstractÔºå entity„ÄÇ When we start encoding Unicode into a binary format„ÄÇ

 we need to translate it into code unitsÔºå which are the atoms of a binary encoding„ÄÇ

 And then if we do want to go up a higher levelÔºå if we want something that's analogous to what„ÄÇ

 we would call a characterÔºå Unicode has the term "graphyme„ÄÇ"„ÄÇ

 Sometimes you'll also see it described as a grapheme cluster„ÄÇ

 And there are a couple different variations„ÄÇ There's legacy grapheme clusters and extended grapheme clusters„ÄÇ

 You don't really need to know all the differences between those to be able to work effectively„ÄÇ

 with Unicode in Python„ÄÇ But a grapheme isÔºå in Unicode's terms„ÄÇ

 the smallest or the minimally distinctive unit ofÔºå writing in a particular system„ÄÇ

 A grapheme is the smallest thing such that if you change itÔºå you change the meaning ofÔºå the text„ÄÇ

 and there's not a one-to-one correspondence between these and code pointsÔºå as we're aboutÔºå to see„ÄÇ

 So let's look at some examples of code points„ÄÇ Here's one that's probably familiar to a lot of people„ÄÇ

 It's just a Latin capital letter AÔºå as you can see from the name„ÄÇ Its code point is 0041„ÄÇ

 Code points are always a number„ÄÇ By traditionÔºå they're expressed in hexadecimal„ÄÇ

 And we can see that it has a block and a category and some other information„ÄÇ And in fact„ÄÇ

 this is just a subset of the information Unicode has on this code point„ÄÇ

 Blocks are a way of organizing code points below the level of a plane„ÄÇ

 Blocks are contiguous sets of code points that are all related„ÄÇ So for example„ÄÇ

 the basic Latin block contains the code points for the Latin alphabetÔºå or„ÄÇ

 at least the most common parts of it„ÄÇ It has a lot of overlap with ASCII„ÄÇ In fact„ÄÇ

 the first 128 code points in Unicode match the 128 values in ASCII„ÄÇ We can see its category„ÄÇ

 It's an uppercase letter„ÄÇ We can see it has information about bi-directionality„ÄÇ

 English and most other Western European languagesÔºå are written left to right„ÄÇ

 There's also this combining class propertyÔºå which we'll get to in just a second„ÄÇ Of course„ÄÇ

 there's a lot more that you could look up if you went trolling through all of„ÄÇ

 the information in the Unicode database„ÄÇ So let's look at something a little bit more complicated„ÄÇ

 This is code „ÄÇ0308Ôºå which by itself doesn't really do anything„ÄÇ It wants to go with something else„ÄÇ

 And when it doesÔºå it shows up as an accent markÔºå diuresisÔºå or sometimes you might call„ÄÇ

 it an umlaut or just dot„ÄÇ But here we can seeÔºå for example„ÄÇ

 that combining class value suddenly shows up has a value ofÔºå 230„ÄÇ

 which says when we're rendering this and we see this in a sequence of code points„ÄÇ

 it shows up above whatever came before it„ÄÇ And there are different values for combining class to show positioning and how different„ÄÇ

 things combine together to form a single visible glyph on your screen or when printed„ÄÇ

 But notice that this means if we want that u with an umlaut above itÔºå we're using multiple„ÄÇ

 code points to produce what is one character from the human reader's perspective„ÄÇ

 So we've already broken that concept of one code point is one character because here we„ÄÇ

 have a character that uses two code points„ÄÇ And it actually goes the actual complexity shows up in both directions„ÄÇ

 For exampleÔºå here this character from the Arabic sections of Unicode is one code point„ÄÇ

 but 18 characters„ÄÇ And there are actually several of these in Unicode„ÄÇ

 These are used in Arabic religious type setting where there are certain phrases that tend„ÄÇ

 to occur quite often and there are ligatures for representing them just as a single unit„ÄÇ

 when type setting and when printing and displaying„ÄÇ But again„ÄÇ

 we see you can't assume one code point is one character and in fact this is„ÄÇ

 one code point that isn't necessarily even one word„ÄÇ

 So Unicode yes can be complex and yes you need to know that there's a difference between„ÄÇ

 code points and characters and graphims„ÄÇ But when you sit down and think about it„ÄÇ

 any system that tries to handle all of the complexity„ÄÇ

 of human writing sooner or later is going to run into something like this and is going„ÄÇ

 to present this level of complexity to you in some way„ÄÇ

 And of course it keeps going because this means that often in Unicode there are multiple„ÄÇ

 ways to write the same thing„ÄÇ Going back to that you with the umlop above it„ÄÇ

 there are at least two ways you can writeÔºå this in Unicode„ÄÇ

 Here's the one we saw earlier that uses the combining accent character„ÄÇ

 There's also a pre composed form that does it in one code point„ÄÇ

 And this is there for historical reasons„ÄÇ A lot of earlier encodings that were single purpose had pre composed characters for different„ÄÇ

 combinations of letters and accent marks„ÄÇ And so for compatibility Unicode has to have them as well so that you can do a lossless conversion„ÄÇ

 to and from Unicode back to your original encoding„ÄÇ If you ever want to„ÄÇ

 But this means that in Unicode we can end up with multiple ways to write the same thing„ÄÇ

 And these two sequences of code points where the one pre composed point and the decomposed„ÄÇ

 sequence of two code points are considered equivalent„ÄÇ

 And in fact Unicode calls them canonically equivalent because it should always be safe„ÄÇ

 to swap one of these for the other„ÄÇ You won't change the meaning of your text by doing so„ÄÇ

 But it also has a concept of compatibility equivalence which is where it may not always„ÄÇ

 be safe to swap between two different ways of writing the same thing„ÄÇ

 So here for example we have a code point that represents a composed fraction one half and„ÄÇ

 a decomposed sequence that writes it out as a one and a two with a splash between them„ÄÇ

 There are times when it's correct to swap between these there are also times when it'sÔºå not„ÄÇ

 And this gives rise to the concept of normalization„ÄÇ

 Which is a way that we can take different sequences that may represent the same thing„ÄÇ

 and find out if they do by making them equal after the normalization„ÄÇ

 And because Unicode has both composed and decomposed forms and has two different types„ÄÇ

 of equivalents„ÄÇ There are four different ways to normalize Unicode depending on what the result should„ÄÇ

 look like and what rules you want to apply„ÄÇ So you can either get a composed or a decomposed form after the normalization„ÄÇ

 You can use either canonical or compatibility equivalence rules as you're doing this„ÄÇ

 Now we'll get to this a little bit later on but if you're just feeling overwhelmed and„ÄÇ

 want a general recommendation if you ever need to do Unicode normalization yourself„ÄÇ

 it's probably best to pick form NFKC„ÄÇ That's the one that will make the most trade-offs in favor of what you probably want but we'll„ÄÇ

 see examples of how different normalization forms can be good or bad a little bit laterÔºå on„ÄÇ

 Speaking of multiple ways of writing the same thing though a lot of languages have multiple„ÄÇ

 different forms for different charactersÔºå uppercase and lowercase and in fact Unicode has three„ÄÇ

 different cases lowercaseÔºå uppercase and title case and multiple different case mappings„ÄÇ

 and ways of transforming characters according to case as well as the concept of completely„ÄÇ

 uncased characters„ÄÇ And in fact most code points in Unicode were most characters abstract entities that Unicode„ÄÇ

 handles are uncased because the case mappings won't change them because they're coming from„ÄÇ

 languages or systems of symbols that just don't have a concept of case„ÄÇ

 Now you might be wondering well how then do I do things like case insensitive comparisons„ÄÇ

 especially because Unicode if you dig into it has at least three different concepts of„ÄÇ

 case and ways to find out what case a character is or whether it even is case and the answer„ÄÇ

 is case folding which Python supports and we'll see examples of it in a little bit but I do„ÄÇ

 want to call out that Python's documentation says something not great„ÄÇ

 Python says case folding is like a more aggressive form of lowercase and while it's true that„ÄÇ

 for a lot of Western European languages the result of case folding will look lowercase this„ÄÇ

 is not a guarantee there are languages where the result of case folding will look uppercase„ÄÇ

 So don't think of case folding as being uppercasing or lower casing it is its own thing but the„ÄÇ

 important thing to know about case folding is that after you've case folded two strings„ÄÇ

 if they differed only in case they will be the same after the fold„ÄÇ

 And of course case can also extend beyond what Unicode really handles„ÄÇ

 Unicode handles most of these cases for example the Greek Sigma which takes different forms„ÄÇ

 depending on where it occurs in a word the Turkic languages have both dotted and dotless„ÄÇ

 forms of the letter I and it's incredibly important to preserve the dot or absence of„ÄÇ

 the dot when you're doing a case transformation because those effect meaning German has this„ÄÇ

 character officially it's called the sharp s historically it didn't have an uppercase form„ÄÇ

 and so it uppercases to SS but this also means that case mappings in Unicode aren't transitive„ÄÇ

 because uppercasing this and then lower casing again won't get you back what you started„ÄÇ

 with and there's far more complexity in the language of that have case that Unicode just„ÄÇ

 doesn't handle and tells you you may need to have low-calaware rules there are situations„ÄÇ

 like for example Dutch where words that begin with ij have to title case that as a single„ÄÇ

 unit rather than as two characters and Unicode simply tells you you need to get low-calaware„ÄÇ

 rules for the specific language you're going to work with it handles some of these but„ÄÇ

 nowhere near all the complexity that exists in all the languages now finally we need to„ÄÇ

 understand since we're going to work with computers how we actually get this into a„ÄÇ

 computer which means how do we get it into a binary form how do we encode it and decode„ÄÇ

 it going between code points which are numeric values but kind of abstract to actual bits„ÄÇ

 and bytes and to do that we need a Unicode transformation format and there are a lot of„ÄÇ

 those I've listed some here the two you'll see most often are probably UTF-8 and UTF-16„ÄÇ

 but it's worth being aware that most of these are variable width they use different numbers„ÄÇ

 of bytes to encode different code points UTF-8 for example for a code point from the„ÄÇ

 ASCII range only needs one byte but for other code points may need up to four UTF-16 for„ÄÇ

 anything in the lowest numbered plane plane zero or the basic multilingual plane BMP as„ÄÇ

 you'll sometimes see it written uses two bytes for anything from higher numbered planes uses„ÄÇ

 four bytes because originally Unicode had just the one plane and it had two to the sixteenth„ÄÇ

 code points in it so there was an assumption that 16 bits ought to be enough for anybody„ÄÇ

 right well eventually Unicode added more planes and UTF-16 was developed with a scheme that„ÄÇ

 lets it still handle 16 bit units but sometimes use two of them per code point the exact mechanics„ÄÇ

 if you want to go look it up are called surrogate pairs and basically there is a segment of„ÄÇ

 plane zero of Unicode that set aside that will never be assigned and UTF-16 transforms„ÄÇ

 a code point bigger than 16 bits into two code points from that range and then you can„ÄÇ

 transform them back again to get back the original value and this is how UTF-16 handles those„ÄÇ

 code points that are larger than 16 bits but that also means that it too now is a variable„ÄÇ

 with encoding and of course we need to consider what kind of abstractions we're going to expose„ÄÇ

 to a programmer because there are different ways we can handle strings in programming languages„ÄÇ

 they might be sequences of bytes they might be sequences of the encodings code units or„ÄÇ

 they might be sequences of code points or sequences of graph themes and their trade-offs„ÄÇ

 involved in all of these one of the important things to be aware of though is depending„ÄÇ

 on the abstraction your language chose you may or may not be able to cause changes in„ÄÇ

 meaning or even completely invalidate a sequence of code points by cutting into it so for example„ÄÇ

 in a language like see where typically strings are exposed as a sequence of bytes if you„ÄÇ

 arbitrarily cut in the middle of that you might cut in the middle of a multi byte code unit„ÄÇ

 or you might cut in the middle of a code point that requires multiple bytes or you might„ÄÇ

 be cutting at a code point boundary but cutting in the middle of a graph theme that's made„ÄÇ

 up of multiple code points all of these operations can be unsafe and depending on which abstraction„ÄÇ

 your language exposes to you you may be at risk of different versions of these problems„ÄÇ



![](img/2a3245dcd9af276985281a539c729f78_17.png)

 now you might be wondering well what does Python do and you might also be wondering„ÄÇ

 well we're twenty minutes into this and you haven't really talked about Python I thought„ÄÇ

 this was a Python talk well okay let's talk about Python originally there was Python 2„ÄÇ



![](img/2a3245dcd9af276985281a539c729f78_19.png)

 and Python 2's story for Unicode was not that great in Python 2 the string type was a sequence„ÄÇ

 of bytes there was a separate type called the Unicode that was a Unicode string it offered„ÄÇ

 access to lots of features of Unicode it could represent any code point in Unicode or at least„ÄÇ

 sometimes could we'll get to that in a minute and you had to know to convert back and forth„ÄÇ

 between them and you had to know what encodings things came from and we're going to and Python„ÄÇ

 assumed ASCII by default for its byte strings and even if you told it otherwise still a„ÄÇ

 lot of third-party modules and other code didn't behave all that well when you presented„ÄÇ

 them with non-asky byte sequences or even sometimes with just Unicode instances and this„ÄÇ

 was generally a mess so now we have Python 3 and there was much rejoicing because in Python„ÄÇ

 3 there's only one string type and it is a Unicode string type there is still a separate„ÄÇ

 type for sequences of bytes and you can go back and forth between them you can take a„ÄÇ

 byte sequence and if you know the encoding you can decode it into a string or you can„ÄÇ

 take a string and encode it into bytes in a particular encoding but a lot of the issues„ÄÇ

 that used to exist in Python 2 especially with the sort of interchangeability that it had„ÄÇ

 for both Unicode and byte strings have been cleaned up except for one thing that persisted„ÄÇ

 for a few releases into the Python 3 series and it's this this is everybody's favorite„ÄÇ

 emoji the pile of poo and if you fire up a Python interpreter from Python 3„ÄÇ0„ÄÇ1„ÄÇ2 there's„ÄÇ

 a good chance you will see this result and you might be wondering what's going on here„ÄÇ

 well earlier versions of Python when you compiled the interpreter you made a choice as to how„ÄÇ

 it would store Unicode internally and effectively the choice was between UTF-16 and UTF-32 so„ÄÇ

 either 16-bit or 32-bit storage for Unicode these were called narrow and wide builds of„ÄÇ

 Python most people used a narrow build and that's where you would see this result because„ÄÇ

 that code point is too large to fit in a single 16-bit unit so an encoding like UTF-16 needs„ÄÇ

 to use a surrogate pair for it and split it across two replacement code points and Python„ÄÇ

 would expose this to you directly if you iterated over this you would see two code points from„ÄÇ

 the surrogate range in the basic multilingual plane instead of the original code point you„ÄÇ

 put in now fortunately that's been fixed Python 3„ÄÇ3 changed this implemented a spec from pep„ÄÇ

 393 which did away with the narrow and wide builds of Python if you want to know the details„ÄÇ

 and how that affected in memory storage and how it affects the capi of Python you can go„ÄÇ

 look up the pep one other nice takeaway is that it means Python strings use a lot less„ÄÇ

 memory now on average than they used to but the big thing for our purposes is it means„ÄÇ

 that a Python string now really is a sequence of code points where previously it was a sequence„ÄÇ

 of code units and this is actually a test I like to use with different languages when„ÄÇ

 I try them out is take a string like the pile of poo and ask the language how long is this„ÄÇ

 if you get an answer of one that means the language is probably working with either code„ÄÇ

 points or graph themes as its string abstraction if you get an answer of more than one then maybe„ÄÇ

 it's working with code units like Python used to with its 2-byte 16-bit encoding on narrow„ÄÇ

 builds or maybe even at something more complex like just exposing sequences of UTF-8 bytes„ÄÇ

 which will give you an even larger answer on some of the emoji but it's a good way to„ÄÇ

 quickly find out what is a language doing and what abstraction is it exposing when it„ÄÇ

 says it has Unicode support because that's an important thing to know now as far as Python„ÄÇ

 we now have strings which are sequences of code points which means that we can find out„ÄÇ

 information about them if we grab something out of a string so a string of length one it's„ÄÇ

 a single code point if we iterate a string we're iterating over code points if we if we„ÄÇ

 take a slice of a string or index into a string we're getting code points and we can actually„ÄÇ

 find out what's the numeric value of a code point and we can transform it into hexadecimal„ÄÇ

 as a tradition for representing Unicode code points there's also a module in the standard„ÄÇ

 library that's really useful called Unicode data and this gives us a lot of access to the„ÄÇ

 Unicode databases and the information that Unicode provides about its code points and„ÄÇ

 characters so we can ask questions like what's the name of this code point or what category„ÄÇ

 is it assigned to what's its bidirectional or combining rendering behavior all of which„ÄÇ

 can be useful information to find out we also have access from that module to Unicode normalization„ÄÇ

 and we can use any Unicode normalization form we want and so here for example we can take„ÄÇ

 that pre-composed u with umlaut character and decompose it into the two code points sequence„ÄÇ

 or we can take that pre-composed one half fraction and decompose it using compatibility„ÄÇ

 equivalence into that sequence of one slash two we also have access in Python to case folding„ÄÇ

 which is useful gives us access to case insensitive comparison and anytime you need to do a case„ÄÇ

 insensitive string comparison in Python this is what you should be reaching for a lot of„ÄÇ

 us probably developed habits from earlier days when we weren't working with Unicode of„ÄÇ

 upper casing or lower casing and a lot of us probably still do that when working with databases„ÄÇ

 because we may not have a case fold abstraction in our sequel libraries or in our database„ÄÇ

 but in Python we have that available and that's how we should be doing case insensitive comparisons„ÄÇ



![](img/2a3245dcd9af276985281a539c729f78_21.png)

 of strings so everything is wonderful right obviously well yes in a way Python three and„ÄÇ

 especially since three point three does a good job of implementing Unicode and exposing„ÄÇ

 it in a useful way and making it relatively easy for us to work with but there are still„ÄÇ

 traps and problems that we can fall into one of which I'm not normally a fan of absolute„ÄÇ



![](img/2a3245dcd9af276985281a539c729f78_23.png)

 statements but this is one that I will get pretty absolute on I will call this the golden„ÄÇ

 rule of working with text in any sort of programming language not just Python is to be aware of„ÄÇ

 your programs boundaries and do encoding and decoding there and only there and when I say„ÄÇ

 boundaries I mean things like if your program reads and writes files then that's a boundary„ÄÇ

 when it opens up and reads the contents of a file or writes the contents back onto the„ÄÇ

 file system if your program talks over a network that's a boundary when it sends information„ÄÇ

 out over that connection or receives information coming in those are the points where you should„ÄÇ

 do your encoding and decoding those are the points where you should be working with bytes„ÄÇ

 objects but once you have them encoded or decoded you should be working entirely with„ÄÇ

 strings internally you should not be passing around bytes objects or dealing with encoded„ÄÇ

 sequences of bytes at almost any cost because most older approaches and most older Python„ÄÇ

 code that had trouble making the jump two to three had trouble because of this because„ÄÇ

 of mixing of byte strings and Unicode strings or even just not using Unicode strings at all„ÄÇ

 in some cases and not thinking about encoding and decoding and where they needed to happen„ÄÇ

 so this is your golden rule if you take nothing else away look for the boundaries of your„ÄÇ

 program identify what they are do your encoding and decoding there and only there everywhere„ÄÇ

 else be working with strings be working with real Unicode now there is still some difficulty„ÄÇ

 every once in a while especially when it comes to working with files and especially on certain„ÄÇ

 types of Unix operating systems and these problems fall into a few different categories there„ÄÇ

 are some systems where there is no reliable way to ask the system what encoding it uses„ÄÇ

 for its file system so you know that something like a file name is a sequence of bytes but„ÄÇ

 you might not have any way of figuring out how to decode that into a sequence of Unicode„ÄÇ

 code points there are also file systems where technically there is not a requirement that„ÄÇ

 they be able to decode where a path can simply be any arbitrary sequence of bytes you want„ÄÇ

 and never validly decode in any known encoding and Python has made progress over the course„ÄÇ

 of the Python 3 release series with getting better at this there are some tips and tricks„ÄÇ

 and tools and now Python mostly does its best to let you treat the file system as UTF-8„ÄÇ

 with some tricks to handle potentially invalid or just completely undecodable paths actually„ÄÇ

 what Python does now is similar to what UTF-16 does where when it encounters a byte that„ÄÇ

 can't possibly decode as a sequence of code points it preserves it as is by transforming„ÄÇ

 it into a code point from the surrogate pair range and that lets it transform back into„ÄÇ

 the original byte again when it's time to write things on the file system or do other„ÄÇ

 encoding sometimes you can work around this by telling Python what encoding your file system„ÄÇ

 is using sometimes you just have to hope that it works because there are some systems that„ÄÇ

 are configured hopelessly but it is a thing that has gotten better it is a thing that now„ÄÇ

 mostly reliably works even on those badly configured systems which is a big leap forward„ÄÇ

 from where it was in the early days of Python 3 of course there are other problems you can„ÄÇ



![](img/2a3245dcd9af276985281a539c729f78_25.png)

 run into we've we've definitely seen examples of normalizing different forms that can sometimes„ÄÇ

 be a destructive operation depending on what language you're working with some languages„ÄÇ

 really rely on the combining and composing features of Unicode this example is Korean„ÄÇ

 but it's not the only language that does this that initial string is two code points„ÄÇ

 it's two composed characters effectively each one represents one syllable of the text and„ÄÇ

 each syllable is made up of three individual characters or three individual letters that„ÄÇ

 represent the consonants and vowels that go into that syllable and performing a decomposing„ÄÇ

 normalization can result in that sequence of as you see below six different code points„ÄÇ

 representing the constituent parts of those composed characters those single syllable representations„ÄÇ

 and depending on what system you feed them into they may render correctly or they may„ÄÇ

 not the terminal application I used to generate these examples handled this well and rendered„ÄÇ

 both of these strings the same way this slide does not so this is something to be aware of„ÄÇ

 and in general combining and composed forms pop up more often than people expect a lot„ÄÇ

 of emoji for example are made of combining sequences the country flags there is a set„ÄÇ

 of code points that are called the regional indicator symbols and they provide an alphabet„ÄÇ

 that lets you spell out two-letter country codes and then those render as the flags of„ÄÇ

 those countries so this is basically the sequence us spelled out in regional indicator„ÄÇ

 symbol code points if you wanted something like the flag of Canada you would spell CA„ÄÇ

 if you wanted the flag of France you would spell FR and that's how the flag emoji work„ÄÇ

 splitting these down the middle could just destroy the meaning because you wouldn't know„ÄÇ

 how to render it anymore the same thing is true of a lot of emoji for people for example„ÄÇ

 this is a family of four people but it's seven different code points under the hood and a„ÄÇ

 lot of the emoji for people are multi-code point sequences either composing groups of„ÄÇ

 people or composing on modifiers to indicate gender or skin tone or other attributes and„ÄÇ

 again splitting in the middle of them can be destructive it can change the meaning or„ÄÇ

 completely destroy the meaning of a sequence so we need to turn back to that concept we„ÄÇ

 saw earlier of the graph theme the minimally distinctive unit of meaning and unfortunately„ÄÇ

 Python doesn't directly give you a way to work with graph themes in strings but you can„ÄÇ

 still do it using third party libraries so for example this is a third party regular„ÄÇ

 expression library called regex you can pip install it and it offers a lot more support„ÄÇ

 for Unicode then Python's standard library regex module does including things like filtering„ÄÇ

 and matching on Unicode properties and most importantly for this case it provides a meta„ÄÇ

 character for matching Unicode graph theme clusters and this is actually defined in one„ÄÇ

 of the Unicode specifications it's supposed to be this capital X character but it means„ÄÇ

 that we can do things now like count the number of graph themes in a string or split on graph„ÄÇ

 themes or iterate over graph themes rather than risking splitting up a graph theme that's„ÄÇ

 made up of multiple code points but even in the Python standard library there is still„ÄÇ

 a lot of awareness of Unicode if we go back to for example the regex module in the Python„ÄÇ

 standard library most of us have probably written code like this where we're saying oh„ÄÇ

 okay I need to match something that you know looks like a year so it's a sequence of four„ÄÇ

 digits and it's going to be something like 2020 well it turns out Unicode has a much„ÄÇ

 broader concept of digit than what speakers of English and Western European languages„ÄÇ

 do so that second string for example pulls digit characters from four different blocks„ÄÇ

 and four different languages that are represented in Unicode but it still matches because according„ÄÇ

 to Unicode properties they are all digits and this is an important thing to be aware of„ÄÇ

 when you're working with Python in Python 3 where everything is Unicode and things are„ÄÇ

 mostly Unicode aware that you need to be explicit as the Zen of Python says explicit„ÄÇ

 is better than implicit but you need to make sure you understand what some of these things„ÄÇ

 mean like this digit medic character or the other regex medic characters and if what you„ÄÇ

 really wanted was only to match digits 0 through 9 from the Latin character set you can say„ÄÇ

 that but you do have to be explicit about it„ÄÇ There can also be difficulty with things„ÄÇ

 like performing string comparisons and working with different strings that potentially write„ÄÇ

 the same thing different ways and here it can be difficult to give any single answer because„ÄÇ

 the answer is usually context sensitive it depends on what you're doing„ÄÇ So for example„ÄÇ

 here this is a fairly simple algorithm but it comes from one of the Unicode technical„ÄÇ

 reports on security and this is for comparing things that might be used as identifiers things„ÄÇ

 like maybe variable names in a programming language or usernames in an account system„ÄÇ

 and this gives you a way to compare them in a case insensitive way and figure out which„ÄÇ

 ones should be considered equivalent and which ones should not„ÄÇ There are other ways to normalize„ÄÇ

 and prepare strings for comparison and for use and it really does depend on your use case„ÄÇ

 Python supports quite a few of them for example if you are working with domain names which can„ÄÇ

 be internationalized now there are modules in the standard library that support this the„ÄÇ

 encoding codeings„ÄÇidna module the puny code codec let you work with these and transform„ÄÇ

 internationalized domain names into an ASCII compatible form that's safe to transmit through„ÄÇ

 a lot of systems that maybe aren't aware of internationalized domain names„ÄÇ There are„ÄÇ

 also even trickier things that you can get by digging into third-party modules„ÄÇ For example„ÄÇ

 this is something that comes and goes where people will try to fool you by writing out„ÄÇ

 a domain name or maybe an email address or a username or some other identifier using a„ÄÇ

 mix of scripts where some of the characters look like each other but aren't actually the„ÄÇ

 same and Unicode actually includes a database for this called the visually confusing characters„ÄÇ

 file and there's a third-party module that you can go download that has this wonderful„ÄÇ

 function and it called is dangerous„ÄÇ I love that name that tells you when a string contains„ÄÇ

 code points from multiple scripts and some of them appear in that visually confusing„ÄÇ

 characters file some of them are confusable characters so you can notice when somebody's„ÄÇ

 trying to do something dangerous and that module also includes a lot of other information that„ÄÇ

 you can access so you can ask it to show you a list of what are the confusable characters„ÄÇ

 what was it that set this off what were the script properties that were being used in„ÄÇ

 this string that were being mixed together and there's really a whole wide world of things„ÄÇ



![](img/2a3245dcd9af276985281a539c729f78_27.png)

 out there but hopefully at this point you've got a handle on the core ideas of what goes„ÄÇ

 into Unicode what it is why it's complex and where that complexity comes from so you can„ÄÇ

 start thinking about that complexity in a productive way start anticipating where you„ÄÇ

 may need to do extra work where you may need to worry about something and how you can write„ÄÇ

 better more effective code and feel more confident about how you're using Unicode and of course„ÄÇ



![](img/2a3245dcd9af276985281a539c729f78_29.png)

 if you have any questions unfortunately this is an online presentation because PyCon had„ÄÇ

 to be canceled this year but I'm happy to have people reach out and ask me questions„ÄÇ

 I also keep a blog where I regularly rant about all sorts of things including Unicode„ÄÇ

 which has its own category there and finally I want to take a minute to just thank the„ÄÇ

 PyCon organizers and the PSF because they were really put in an impossible situation this„ÄÇ

 year and as sad as it is that the in-person version of PyCon 2020 had to be canceled it„ÄÇ

 really is incredible the way that they reacted and responded and were able to put together„ÄÇ

 this online track of talks as quickly as they did and as successfully as they did so if you're„ÄÇ

 watching this please be thankful for the PSF for the PyCon organizers and for all the„ÄÇ

 work they put in to putting this online and pulling off a remote PyCon 2020 on such short„ÄÇ

 notice and under the worst possible conditions„ÄÇ In the meantime stay safe hopefully I will see you at a PyCon in person sometime in the„ÄÇ

 future„ÄÇ Thank you„ÄÇ

![](img/2a3245dcd9af276985281a539c729f78_31.png)

 so much for watching„ÄÇ for watching„ÄÇ the PSF for watching„ÄÇ the PSF for watching„ÄÇ [BLANK_AUDIO]„ÄÇ



![](img/2a3245dcd9af276985281a539c729f78_33.png)