- en: P82：Tutorial Kimberly Fessel - It's Officially Legal so Let's Scrape the Web
    - 程序员百科书 - BV1rW4y1v7YG
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: P82：教程**金伯利·费塞尔** - 这已经正式合法了，所以让我们开始网络抓取 - 程序员百科书 - BV1rW4y1v7YG
- en: Hello， and welcome to this PyCon 2020 tutorial。
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 你好，欢迎来到这个PyCon 2020教程。
- en: '![](img/021740aa4daa35fb40acaf4dbe65ca57_1.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](img/021740aa4daa35fb40acaf4dbe65ca57_1.png)'
- en: '![](img/021740aa4daa35fb40acaf4dbe65ca57_2.png)'
  id: totrans-3
  prefs: []
  type: TYPE_IMG
  zh: '![](img/021740aa4daa35fb40acaf4dbe65ca57_2.png)'
- en: It's officially legal， so let's scrape the web。 My name is Kimberly Fessel。
    and I'll be guiding you through this tutorial。 I'm an instructor and senior data
    scientist at Metas Data Science Bootcamp。 and my previous， background was in natural
    language processing as well as applied mathematics。 So before we get started with
    learning about web scraping， I want to start off by giving。
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 这已经正式合法了，所以让我们开始网络抓取吧。我的名字是**金伯利·费塞尔**，我将带领你完成这个教程。我是Metas数据科学训练营的讲师和高级数据科学家，我的背景是自然语言处理和应用数学。因此，在我们开始学习网络抓取之前，我想先介绍一下。
- en: you a bit of a motivation about why you might want to learn this process。 So
    let's say that you've got an idea for learning more about what makes popular music
    popular。 and do you realize that Wikipedia has this really cool article that has
    a list of all。 of the most popular songs from 2019？ So Old Town Road and Sunflower。
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 给你一些动机，说明你为什么可能想学习这个过程。假设你想更多地了解什么使流行音乐受欢迎，你会发现维基百科有一篇非常酷的文章，列出了2019年所有最受欢迎的歌曲。所以《老镇路》和《向日葵》。
- en: these were all great hits last year。 And you realize that you can actually gather
    a lot of information about each of these songs。 if you follow the links， and then
    look over here in this sidebar。 We see we have the release date。 and we have the
    genre， the length， the songwriters， all， kinds of information about this song。
    So maybe we start up an Excel spreadsheet， and we start copying and pasting all
    this really。
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 这些都是去年非常受欢迎的歌曲。你意识到，如果你跟随链接，可以收集到关于每首歌的很多信息，然后查看这个侧边栏。我们看到有发行日期、流派、时长、词曲作者等各种信息。所以也许我们可以启动一个Excel电子表格，开始复制和粘贴所有这些有用的信息。
- en: useful information。 And then we go back to our list and visit the next song。
    Same information here。 we have release date and length and et cetera， so we start
    just， filling in that Excel sheet。 So copy， paste into the Excel sheet， copy，
    paste。 And we continue on， here's the third song。 same sort of information here，
    too。 All these web pages you kind of notice are structured pretty similarly。
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 有用的信息。然后我们返回列表，访问下一首歌曲。同样的信息，这里有发行日期、时长等等，所以我们开始填充那个Excel表格。复制、粘贴到Excel表格，复制、粘贴。我们继续，这里是第三首歌，信息也是类似的。你会注意到这些网页的结构都相当相似。
- en: And you just keep cruising along， but eventually you realize this is going to
    take some time。 If you scroll down， it turns out that there are a hundred different
    songs on this web， page。 So that would mean that you'd need to visit a hundred
    different pages for all these different。 songs to extract this information。 And
    that's just 2019 alone。
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能一直在继续，但最终你会意识到这将需要一些时间。如果你向下滚动，你会发现这个网页上有一百首不同的歌曲。这意味着你需要访问一百个不同的页面来提取这些不同歌曲的信息。而这仅仅是2019年的情况。
- en: you might start expanding your analysis to include 2018 and， 2017。 So my point
    is here that while there is a lot of useful information on the internet。 if your
    goal here is just a copy and paste all of that into an Excel sheet， it's going。
    to take you quite a bit of time。 So one way that we can work kind of smarter and
    not harder here is to learn about web scraping。
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会开始扩展你的分析，包括2018年和2017年。我的意思是，虽然互联网上有很多有用的信息，但如果你的目标只是将所有这些信息复制粘贴到Excel表格中，那将会花费你相当多的时间。因此，我们可以更聪明地工作，而不是更辛苦地工作，这就是学习网络抓取。
- en: What is web scraping？ It's basically the idea that we want to collect data from
    websites and we want to take that。 data and parse it into a meaningful format。
    And hopefully this whole process will be in an automated way so that we're not
    stuck doing。 a lot of manual work and copying and pasting。 And it turns out that
    yes。 you can do web scraping with Python， you can write a script and。
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 什么是网络抓取？基本上就是我们想从网站上收集数据，并将这些数据解析成有意义的格式。希望整个过程都是自动化的，这样我们就不必做很多手动工作和复制粘贴。事实证明，你可以用Python进行网络抓取，你可以写一个脚本。
- en: have it collect all of this information for you。 And that's exactly what we're
    going to learn today。 So I think the motivation for web scraping is pretty clear
    at this point， but just to be。 super explicit。 There's two main reasons that I
    can think of you'd want to learn web scraping。 The first is definitely to save
    time。 There's a lot of great useful information out on the internet。
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 让它为你收集所有这些信息。这正是我们今天要学习的内容。所以我认为，网页抓取的动机到目前为止是非常明确的，但为了更清楚一点，我可以想到你想学习网页抓取的两个主要原因。第一个绝对是节省时间。互联网上有很多有用的信息。
- en: but having someone manually， copy and paste that information just isn't an effective
    use of time。 The other reason why you might want to learn web scraping is it's
    really empowering。 It turns out that if you know about how to web scrape， you
    will have access to a lot。 more data sources。 So instead of just relying on someone
    else to put together a nice CSV file or maybe even。
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 但让某人手动复制和粘贴这些信息显然不是有效的时间利用。你可能想学习网页抓取的另一个原因是，它真的很有能力。事实证明，如果你了解如何进行网页抓取，你将能够访问更多的数据源。所以不必再依赖其他人整理一个好的CSV文件，或者甚至是。
- en: an API to collect data from， now you actually are empowered to gather data from
    whatever。 website you want。 So I hope you do realize that this tool。 this process
    of web scraping is super empowering， and really can help you leverage all of your
    cool data analyses。 Take all of that to the next level because you do now have
    access to a lot more data。
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 通过一个API来收集数据，现在你实际上可以从任何你想要的网站收集数据。因此，我希望你意识到，这个工具、这个网页抓取的过程是非常强大的，确实可以帮助你提升所有的酷数据分析。将所有这些推向一个新的高度，因为你现在可以访问更多的数据。
- en: So just to give you kind of an overview of how web scraping is going to work，
    we're going。 to dive into each of these components in a lot more detail， but just
    kind of in broad， strokes。 We have some kind of web page that we're interested
    in some of its data。 The data that lives on this web page， and in fact the whole
    web page here is entirely。
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，给你一个关于网页抓取工作原理的概述，我们将深入探讨每个组件，但大致来说。我们对某个网页感兴趣，它上面有一些数据。这个网页上的数据，事实上，整个网页在这里是完全。
- en: powered by HTML。 There's actually two other components to a lot of web pages，
    HTML。 CSS and JavaScript。 We're going to be pulling the HTML code and then making
    use of the structure of HTML to。 be able to find specific pieces of data that
    we want。 We don't want the entire web page in all of its text。
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 由HTML驱动。实际上，很多网页还有两个其他组件，HTML、CSS和JavaScript。我们将提取HTML代码，然后利用HTML的结构来找到我们想要的特定数据片段。我们并不想要整个网页的所有文本。
- en: We really just wanted the length of the song and songwriters， etc。 So that's
    the process of web scraping。 And we're going to use two Python open source tools
    to help us do this。 The first one is called requests。 Request is the package you
    use when you want to request information from the internet。 It will allow you
    to extract that HTML code that is powering this web page。
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们其实只想要歌曲的长度和词曲作者等信息。这就是网页抓取的过程。我们将使用两个Python开源工具来帮助我们完成这个任务。第一个叫做requests。Requests是你在想从互联网上请求信息时使用的包。它将允许你提取驱动这个网页的HTML代码。
- en: The other package is called beautiful soup。 A beautiful soup is the tool we're
    going to use to make to understand the structure of。 the HTML and to parse through
    that HTML string in order to extract the information we want。 So with those two
    tools， we'll be able to learn a great deal about web scraping。 So one question
    that might kind of be coming up in your mind is， is this even legal？
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个包叫做Beautiful Soup。Beautiful Soup是我们将用来理解HTML结构并解析HTML字符串以提取所需信息的工具。因此，通过这两个工具，我们将能够学习到很多关于网页抓取的知识。那么一个可能浮现在你脑海中的问题是，这样做是否合法？
- en: And up until really recently， this was kind of a murky question。 I do also want
    to preface this section with， I am not a lawyer。 This is not legal advice。 But
    I do want to tell you about some recent legal activities that kind of elucidate
    and。 get at this question。 Okay。 So why was this a murky question up until pretty
    recently？ Well。
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 直到最近，这个问题一直有些模糊。我想在这一部分之前强调一下，我不是律师。这不是法律建议。但我确实想告诉你一些最近的法律活动，这些活动阐明了这个问题。好的，那么为什么直到最近这个问题都比较模糊呢？嗯。
- en: it's because of something called the Computer Fraud and Abuse Act。 This was
    a law that was passed in 1986 as an anti-hacking law。 So at the time。 hacking
    was kind of on the rise， unfortunately。 And we needed a law to say that people
    cannot access the internet from their computer in。 an unauthorized way， or they
    can't access it in a way that exceeds their access authorization。
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这与一个名为计算机欺诈与滥用法的东西有关。这是一项在1986年通过的反黑客法。当时，黑客行为有上升的趋势，不幸的是。我们需要一项法律来说明人们不能以未经授权的方式从他们的计算机访问互联网，或者他们不能以超出其访问授权的方式进行访问。
- en: So that's what the law says， and it's super ambiguous， and because of its ambiguity，
    people。 were not really sure if this applied to web scraping or not。 But recently。
    as recent as the fall of 2019， the Ninth Circuit Court of Appeals ruled in。 favor
    of high QLabs versus LinkedIn。 So this was a lawsuit in which high QLabs was web
    scraping LinkedIn in order to provide。
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这就是法律的规定，它非常模糊，由于其模糊性，人们并不确定这是否适用于网页抓取。但最近，直到2019年秋季，第九巡回上诉法院对高QLabs与LinkedIn的案件作出了裁决。因此，这是一场诉讼，其中高QLabs为了向客户提供产品而抓取了LinkedIn的数据。
- en: a product to their clients。 So what the Ninth Circuit ruled is that web scraping
    of publicly available data does not。 violate the CFAA。 So this is a really recent
    ruling。 and I do expect to see more legal rulings in favor against， et cetera
    in this space。 But that is kind of what's going on right now in the legal climate，
    which hints the name。
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 向他们的客户提供产品。因此，第九巡回法院裁定，抓取公开可用的数据并不违反CFAA。这是一个非常近期的裁决，我预计在这个领域将会有更多的法律裁决支持或反对等。但这就是目前法律环境的状况，这也暗示了这个名称。
- en: of this tutorial。 All right， so let's take a look at some of the learning objectives
    for today。 The first thing we're going to learn how to do is just decipher basic
    HTML。 So if you don't have a lot of experience with HTML， I'm going to walk you
    through just the。 basics of how it goes。 Then we're going to learn how to retrieve
    information from the internet。
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 本教程的学习目标。好的，让我们来看看今天的一些学习目标。我们首先要学习的就是如何解读基本的HTML。如果你对HTML没有太多经验，我将带你了解基本知识。然后我们将学习如何从互联网检索信息。
- en: how to parse， that web data， and finally， the very last thing we'll learn how
    to do is gather and prepare。 that data systematically in a data pipeline。 So we're
    going to be able to scale up what we learned as far as how to parse through really。
    simple web pages， we'll be moving on to the internet， and then finally talking
    about how。 you could make a broad-scale pipeline to scrape lots and lots of information
    and conduct an。
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 如何解析网络数据，最后我们将学习如何系统地收集和准备这些数据。我们将在数据管道中实现这一目标。因此，我们将能够扩大我们在解析简单网页方面学到的知识，接下来我们将进入互联网，最后讨论如何建立一个大规模的数据管道来抓取大量信息并进行分析。
- en: analysis。 So the tutorial breaks down roughly into these three parts。 The first
    one being that HTML basics， this section will probably be about 20 minutes。 Then
    we'll move on to some scraping basics。 Once we know a little bit about HTML。 I'll
    start walking you through kind of just what's， the very basics of web scraping。
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 分析。因此，本教程大致分为这三个部分。第一部分是HTML基础，这部分大约将持续20分钟。然后我们将进入一些抓取基础知识。一旦我们对HTML有了一点了解，我将开始带你了解网页抓取的最基本知识。
- en: And this section will be about 40 minutes。 Finally we'll have the scraping pipeline
    section。 This one's definitely the most lengthy section of the tutorial。 It'll
    be anywhere from an hour to an hour and a half， somewhere around in there。 And
    that's where we're really going to scale up and start looking at how to scrape
    actual。
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 本部分将持续大约40分钟。最后我们将进入抓取管道部分。这绝对是本教程中最长的部分。它将持续一个小时到一个半小时，差不多在这个范围内。在这里我们将真正扩大范围，开始研究如何抓取实际的数据。
- en: websites and how to scrape multiple pages and combine all of that information。
    So a lot of good stuff in that section。 But for now， let's start at the beginning。
    Let's go back to the HTML basics。 I will say if you are really well-versed in
    HTML and you already know how to read this。 code， you can skip ahead in the video
    a bit。 Like I said。
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 网站以及如何抓取多个页面并整合所有信息。因此，这部分有很多有用的内容。但现在，让我们从头开始。回到HTML基础知识。我会说，如果你对HTML非常熟悉，并且已经知道如何阅读这些代码，你可以在视频中跳过一部分。如我所说的。
- en: it's going to be about 20 minutes to walk through these basics。 Okay。 So if
    you don't know this already， HTML code is what powers the web pages you see in
    your， browser。 So here's some example of HTML code。 This is what it looks like。
    And the cool thing is if I have this HTML file living somewhere on my computer
    and I open， it up。
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 大约需要 20 分钟来介绍这些基础知识。好的。如果你还不知道，HTML 代码是驱动你在浏览器中看到的网页的。所以这里有一些 HTML 代码的例子。这就是它的样子。有趣的是，如果我在我的电脑上某处有这个
    HTML 文件并打开它。
- en: my computer will load that into my browser and it will look like this。 So take
    a second to kind of digest， you know， what part of the code is controlling what。
    part of the browser？ You'll see that very first thing I have。 Welcome to PyCon
    2020。 It's a large font kind of header。 Then I have some text in blue and then
    finally a last paragraph about learning HTML。
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我的电脑会将其加载到浏览器中，效果如下。所以花点时间消化一下，你知道，代码的哪个部分控制着浏览器的哪个部分？你会看到我首先展示的内容。欢迎来到 PyCon
    2020。这是一个较大的字体标题。接下来我有一些蓝色的文本，最后一段是关于学习 HTML 的。
- en: So you can see how the code is directly relating to the browser。 But let's walk
    through even more in detail about what each piece of this code does。 So this was
    one of those example paragraphs that I showed you just on the previous page。 And
    really this paragraph breaks down into three main parts。 The first part we have
    is this， the P。
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 所以你可以看到代码是如何直接与浏览器相关联的。但让我们更详细地了解这段代码的每一部分的功能。所以这是我在上一页展示的示例段落之一。实际上，这段落可以分为三个主要部分。第一部分是这个，P。
- en: which is a tag。 Okay， so HTML code is comprised of lots and lots of different
    tags。 And this is just one example。 P means paragraph， a paragraph of text。 So
    tags are enclosed by angular brackets。 And you will see the letter P at the beginning
    and close by angular brackets and then a slash。 P at the end。 That's what lets
    HTML know that this section is done。
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是一个标签。好的，HTML 代码由很多不同的标签组成。这只是一个例子。P 代表段落，一段文本。因此，标签用尖括号括起来。你会看到字母 P 在开头，用尖括号闭合，然后有一个斜杠。P
    在结尾。这告诉 HTML 这一部分结束了。
- en: We have to enclose that with a backslash and the same tag。 Okay。 living inside
    the first set of angular brackets will be our attributes。 So this paragraph just
    has one attribute， the ID。 It's labeled as the HTML paragraph。 And there's lots
    and lots of different types of attributes you can have。
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们必须用反斜杠和同样的标签将其括起来。好的。生活在第一组尖括号内的是我们的属性。所以这个段落只有一个属性，ID。它被标记为 HTML 段落。你可以有很多不同类型的属性。
- en: You can actually have multiple attributes with one tag。 This will be things
    that will use pretty regularly in web scraping to find the information we're。
    looking for。 The final component of this tag， we have the inner HTML text。 So
    this is the part that actually renders in your browser and you would actually
    see this。
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上你可以在一个标签中有多个属性。这些将在网络爬虫中经常使用，以找到我们要寻找的信息。这个标签的最终组成部分是内部 HTML 文本。所以这是实际在你的浏览器中渲染的部分，你实际上会看到这个。
- en: text once the browser has loaded the code。 But first we will learn about HTML。
    So you'll find that text inside those angular brackets。 And that's really the
    main structure of the type of code you're going to see in HTML。 The general HTML
    element structure starts with a tag name， then has various attributes， maybe one。
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 文本在浏览器加载代码后显示。但首先我们将学习 HTML。所以你会在那些尖括号中找到文本。这实际上是你将在 HTML 中看到的代码类型的主要结构。一般的
    HTML 元素结构以标签名开始，然后有各种属性，可能只有一个。
- en: maybe two， maybe three。 Between those two excessive angular brackets。 we have
    the inner HTML text and that's what， appears when we load our browser。 I want
    to give you some sense of what are the common tags that you will encounter。 So
    here's a list。 And feel free to pause this video if you want to look over this
    list in more detail。
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 可能是两个，也可能是三个。在这两个多余的尖括号之间，我们有内部 HTML 文本，这就是当我们加载浏览器时出现的内容。我想给你一些关于你将遇到的常见标签的概念。所以这是一个列表。如果你想更详细地查看这个列表，请随时暂停这个视频。
- en: Here's some example tags that you're likely to see。 These that start with H，
    H1， H2， etc。 are all going to be headers。 And the main thing that these are conveying
    are really font size and kind of structure。 of the page。 So you can imagine you
    might have one main header and then various subsection headers。 The P tags， like
    we've been discussing， stand for paragraph and that's just going to be。
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一些你可能会看到的示例标签。这些以H开头的标签，H1、H2等，都是标题。它们主要传达的是字体大小和页面的结构。所以你可以想象你可能有一个主标题，然后有各种子标题。我们讨论过的P标签代表段落，它只会是。
- en: paragraphs of text。 A tags， which are also really common， those stand for anchor。
    And that's going to be where you see different links on your web page。 So if I
    want to link to another web page， I would use an A tag for that。 Div and span
    are basically just ways to section off your web page。
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 一段段文本。A标签也非常常见，代表锚点。这就是你在网页上看到不同链接的地方。所以如果我想链接到另一个网页，我会为此使用A标签。Div和span基本上只是将网页分隔开的方式。
- en: So div stands for division and span for span。 These are just sections that you
    can kind of break up what your web page kind of have。 little containers for different
    sections of your web page。 IMG is for images and then LI stands for list item。
    So anytime you might see bulleted lists or numbered lists， those will be tagged
    with this， LI。
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 所以div代表分区，span代表跨度。这些只是你可以用来分割网页的部分，为网页的不同部分提供小容器。IMG用于图像，LI代表列表项。因此，每当你看到项目符号列表或编号列表时，它们都会用LI标记。
- en: As far as attributes， by the way， this isn't an extensive list of tags。 These
    are just ones that you might commonly see。 Again for attributes， these are just
    some of them。 But ones that you're really common to see are things like ID。 ID
    is a unique identifier。 You should only have one element on the entire web page
    that is tagged with that ID string。
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 至于属性，顺便提一下，这不是一个全面的标签列表。这些只是你可能常见的标签。对于属性，这只是其中一些。但是你常见的属性有ID。ID是一个唯一标识符。整个网页上只应有一个元素标记有该ID字符串。
- en: So before I had that HTML paragraph， that should be the only tag on my whole
    web page。 that has that tag， excuse me， that attribute。 The class attribute， on
    the other hand。 can apply to multiple different tags。 Class attributes are typically
    used for styling。 But again。 we can exploit these tags when we're looking for
    things on our web page code。
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 所以在我之前有那个HTML段落，它应该是我整个网页上唯一有那个标签的。抱歉，那个属性。另一方面，class属性可以应用于多个不同的标签。class属性通常用于样式。但再说一次，当我们查找网页代码中的内容时，我们可以利用这些标签。
- en: Style would be if we want any extra styling。 Most styling these days happens
    in the form of CSS files。 which we won't be talking about， too much in this tutorial。
    But if you ever need some extra styling。 for example， the blue color， the blue
    font that， I was using before。 you might see these style attributes。 Href is going
    to be when we want to link to something else。
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 样式是我们想要任何额外样式的地方。现在大多数样式都是以CSS文件的形式出现。我们在这个教程中不会过多讨论。但如果你需要一些额外的样式，例如我之前使用的蓝色和蓝色字体，你可能会看到这些样式属性。Href是我们想要链接到其他内容时使用的。
- en: so hyperlink reference。 And then SRC would be for a source file， like an image
    source file。 So there you have it。 Now you can kind of understand what the basis
    for this code is and then how it renders in。 the browser。 So you basically just
    have various tags， their attributes， their text。 And you can kind of see how by
    using those different things， those different elements。
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 所以超链接引用。然后SRC用于源文件，例如图像源文件。所以你明白了。这就是代码的基础，了解它在浏览器中的渲染方式。因此，你基本上有各种标签、它们的属性和文本。你可以看到，通过使用这些不同的东西和元素。
- en: I can render what happens in my browser。 This is an example of a tag， the H1。
    Here we have a style attribute。 And this welcome to PyCon 2020 is actually our
    inner HTML text。 which appears in our， browser。 So I want to tell you about one
    other thing。 This is a little bit more complicated， but I think we'll see a nice
    clear example of what。
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我可以在浏览器中渲染发生的事情。这是一个标签的例子，H1。这里我们有一个样式属性。欢迎来到PyCon 2020实际上是我们的内HTML文本，这出现在我们的浏览器中。所以我想告诉你另一件事。这有点复杂，但我想我们会看到一个清晰的例子。
- en: I mean by this。 So the DOM， the document object model。 HTML code is written
    in such a way that you can start imagining that HTML code in a tree-like。 structure。
    So the DOM basically is defining each HTML element as an object。 And each of those
    objects live in this sort of tree-like structure。
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我的意思是这个。因此，DOM，即文档对象模型。HTML代码是以这样的方式编写的，你可以开始想象HTML代码呈树状结构。因此，DOM基本上是将每个HTML元素定义为一个对象。这些对象都存在于这种树状结构中。
- en: If we understand about that tree structure， what we can do is basically leverage
    the structure。 that the HTML page has in order to find information we want。 So
    we might move up a level or move down or move sideways to find other information
    once。 we can locate something along our page。 Let's actually put that into a visual
    to understand this a little bit more clearly。
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们了解这个树结构，我们可以基本上利用HTML页面的结构来寻找我们想要的信息。因此，我们可能会向上移动一级，或者向下或侧向移动，以找到其他信息，一旦我们能够定位到页面上的某个内容。让我们将这个视觉化，以更清晰地理解这一点。
- en: Let's say I have some example HTML code like this over on the left。 And if I
    rendered that in my browser， you can see over on the right what that would look，
    like。 By the way， HTML is often going to have what's called a head。 The head actually
    has a lot of various metadata， various pieces that aren't necessarily going。
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我在左侧有一些示例HTML代码。如果我在浏览器中渲染它，你可以在右侧看到它的样子。顺便提一下，HTML通常会有一个称为头部的部分。头部实际上包含许多不同的元数据，各种不一定会显示的部分。
- en: to show up on the web page， but can control other things about our web page。
    So this one that lives under the head， the title， is actually the title that you
    see in。 the browser tab。 So we see HTML DOM example。 And that is the title of
    this page。 The next component after the head is the body。 So now this is where
    all of that content is going to live that actually gets rendered。
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在网页上显示，但可以控制我们网页的其他内容。因此，位于头部下的这个标题，实际上就是你在浏览器标签中看到的标题。我们看到HTML DOM示例。这就是此页面的标题。头部之后的下一个组件是主体。因此，现在所有的内容都将在这里呈现。
- en: in the browser。 So you can see I have a division which contains an H3 tag。 Then
    I have another division called tools div and it has a paragraph and an anchor。
    If I represented this in the structure of a tree， the HTML DOM， what I mean is
    basically。 this HTML has two children， the head and the body。
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在浏览器中。所以你可以看到我有一个包含H3标签的div。然后我有另一个称为工具div的分区，里面有一个段落和一个锚。如果我在树结构中表示这个HTML DOM，我的意思基本上是，这个HTML有两个子元素，头部和主体。
- en: Because the HTML is actually wrapping around those two elements。 If I look in
    the body。 I find that it has two div children。 So now that I've drilled down to
    the body。 I can also see that it has two children。 One of those divs has two more
    children。 So you can start thinking about it like a little tree structure and
    to be more specific。
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 因为HTML实际上包裹着这两个元素。如果我查看主体，我发现它有两个div子元素。因此，既然我已经深入到主体，我也可以看到它有两个子元素。其中一个div有两个子元素。因此，你可以开始把它想象成一个小树结构，更具体一点。
- en: this tree structure。 So you see just what I said， the HTML is the root element。
    It has two children。 the head and the body。 The body， for example， has two div
    children。 And one of those div children has two more children， the P and the anchor
    tag。 So thinking about it in this tree structure is going to be really useful
    when we want to。
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这个树结构。所以你看到我说的，HTML是根元素。它有两个子元素，头部和主体。例如，主体有两个div子元素。其中一个div子元素有两个子元素，P和锚标签。因此，以这个树结构思考在我们想要提取内容时会非常有用。
- en: find information that is not labeled with a class or an ID on a webpage。 Let's
    keep this in the back of your mind that it's possible to traverse this tree up
    or。 down or sideways in order to get to the next element that might be something
    that we do。 want to extract。 Okay。 Thank you， got the HTML stuff。 Let's have a
    quiz。 All right。
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 查找网页上未标记类或ID的信息。请记住，可以在这个树中向上、向下或侧向遍历，以到达下一个可能想要提取的元素。好的，谢谢，你了解了HTML的内容。我们来做个测验。好吧。
- en: It is a tutorial after all。 Let's do a quiz first。 Okay。 So how about this？
    I'll actually just。 this is going to be the same code。 I'll just put it over on
    the left hand side there。 Okay。 First question， how many elements do we have inside
    the HTML body？ Did you get three？ I see three。 The H1 tag， the P tag and the A
    tag。 We have three different elements。
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 毕竟这是一个教程。我们先来做个测验。好的，那这样如何？我实际上会。这将是相同的代码。我会把它放在左侧。好的，第一个问题，我们的HTML主体中有多少个元素？你得到了三个吗？我看到了三个。H1标签、P标签和A标签。我们有三个不同的元素。
- en: three different tags that live inside the body。 What is the inner HTML of the
    header tag？
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 三个不同的标签位于主体中。头部标签的内部HTML是什么？
- en: And that's going to be welcome to your HTML quiz。 Third question。 what attributes
    does the paragraph have？ Did you get class and style？ Yep。 So we see two different
    attributes there for the paragraph tag。 The class is small paragraph and the style
    has that font size。 Okay。 Final question。
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 欢迎来到你的HTML测验。第三个问题：段落具有哪些属性？你得到了class和style吗？是的。所以我们在段落标签中看到了两个不同的属性。class是small
    paragraph，style有那个字体大小。好的，最后一个问题。
- en: thinking about the DOM， what is the parent element of the anchor tag？
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 想想DOM，锚标签的父元素是什么？
- en: So the anchor tag actually lives within the body element。 So its parent is the
    body。 Okay。 All right。 So hopefully that was a nice crash course。 If you hadn't
    seen a lot of HTML before it。 now you're starting to get familiar with the， fact
    that we have tags。 attributes and the inner text that actually appears in our
    browser。
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 所以锚标签实际上位于主体元素中。它的父元素是主体。好的。希望这对你来说是一个不错的快速课程。如果你之前没有接触过很多HTML，现在你开始熟悉我们有标签、属性和实际出现在浏览器中的内部文本这一事实。
- en: So I hope you feel a little bit more comfortable about deciphering basic HTML。
    We're going to do a lot more of this in the upcoming sections。 But just to review
    kind of what we've learned so far。 We learned how HTML code actually renders in
    the browser。
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我希望你对解读基本HTML感到更加舒适。我们将在接下来的部分中做更多这方面的内容。但为了回顾一下我们到目前为止所学的内容。我们学会了HTML代码是如何在浏览器中渲染的。
- en: And what we write in an HTML code tells the browser how to display the various
    different。 text elements。 We talked about some common HTML tags that you're likely
    to encounter。 as well as some， common attributes。 We talked about how those elements
    will contain inner HTML text。 And by the way， typically the inner HTML text is
    what you want to extract from your web， page。
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在HTML代码中写的内容告诉浏览器如何显示各种不同的文本元素。我们谈论了一些你可能会遇到的常见HTML标签，以及一些常见属性。我们讨论了这些元素将包含内部HTML文本。顺便说一下，通常内部HTML文本是你想从网页中提取的内容。
- en: If I was looking for the running time for one of those songs， that would be
    text that。 I'm trying to pull from one of these tag elements。 And we talked about
    that document object model。 I think this will become more clear when we start
    doing some of our web scraping examples。 But just kind of keep it in the back
    of your mind。
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我在寻找其中一首歌的播放时间，那将是我试图从这些标签元素中提取的文本。我们谈到了文档对象模型。我认为当我们开始做一些网页抓取的例子时，这将变得更加清晰。但请将其放在你的脑海中。
- en: There's this tree structure we can visit the parents or the children of various
    tags once。 we find them。 Okay， with that in mind， let's move on to our second
    section of this tutorial。 the scraping， basics。 So for the next two sections of
    this tutorial。 I'm going to switch away from slides and actually， work with some
    code。
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个树结构，我们可以访问各种标签的父标签或子标签。一旦找到它们，好的，有了这个考虑，我们进入本教程的第二部分：抓取基础。因此，在接下来的两个部分中，我将切换到代码，而不是幻灯片。
- en: And this code is available to you via Google Colab。 So go ahead and open up
    another tab if you'd like to interact with this code and type in。 this link bit。ly/picon2020_scrapingbasics。
    Okay， so once you've entered that link into your browser。 you should be taken
    to a page， that looks like this。 Scraping basics and it has our tutorial name。
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码可以通过Google Colab获取。如果你想与这段代码进行互动并输入，可以打开另一个标签，链接是bit.ly/picon2020_scrapingbasics。好的，一旦你在浏览器中输入了这个链接，你应该会看到一个页面，看起来像这样。抓取基础，并且有我们的教程名称。
- en: So at this point， you have a couple of different options。 As long as you're
    signed in under Google。 you can continue to use Google Colab and interact， with
    the code right here。 So if you are going to sign in with Google and I recommend
    that's kind of the most preferred。 way to do this is that you will be signed in
    under Google， you can make a copy of this。
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你有几种不同的选择。只要你已登录Google，你可以继续使用Google Colab并与这里的代码互动。如果你将要用Google登录，我建议这是一种比较优选的方式。你将会登录Google，可以复制这个。
- en: notebook just by saying save a copy and drive。 If you do that。 you'll be taken
    to a second tab where Google Colab has created for you， a copy。 You should now
    say copy of。 And this copy is yours to keep。 You can make any kind of changes
    you'd like to this text。 You can add notes。 You can add code。
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 只需点击保存副本到驱动器。如果你这样做，会打开第二个标签页，Google Colab为你创建了一个副本。你现在应该会看到副本。这个副本是你的。你可以对此文本进行任何修改。你可以添加注释。你可以添加代码。
- en: You can do whatever you want to do this copy。 And this will persist in your
    Google drive。 If for some reason you don't want to use a Google account to access
    this code， you do。 have the option of viewing this content on my GitHub page via
    Google， excuse me， a Jupyter。 notebook。 So you can click this link which will
    take you to GitHub to the Jupyter notebook that。
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以随意处理这个副本。这将保留在你的Google Drive中。如果出于某种原因你不想使用Google账户来访问这段代码，你可以选择通过我的GitHub页面查看这个内容，通过Google，抱歉，是Jupyter
    notebook。因此你可以点击这个链接，它会带你到GitHub的Jupyter notebook。
- en: contains this exact same content and you can download that to your computer，
    et cetera。 So if you want to work with this content locally， you can totally do
    that。 One word of caution。 the reason why that's not the preferred method is because
    you'll。 need to install a couple of different libraries。 For example。
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 包含完全相同的内容，你可以将其下载到你的计算机等。如果你想在本地处理这些内容，你完全可以这样做。不过有一点要注意，之所以不推荐这种方法，是因为你需要安装几个不同的库。例如。
- en: beautiful soup is one you'd have to install yourself。 So to avoid that。 we're
    going to use Google Colab。 If you don't want to use a Google account。 that's your
    second option is to look at this， Jupyter notebook。 The third option is just to
    follow along with this tutorial and not worry so much about， the coding。
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: beautiful soup是你需要自己安装的。因此，为了避免这个，我们将使用Google Colab。如果你不想使用Google账户，第二个选择是查看这个Jupyter
    notebook。第三个选择就是跟着这个教程，不必太担心编码。
- en: If you just want to see what is web scraping， you're just curious， you can continue
    following。 along without coding yourself。 That's totally fine too。 But I definitely
    do think that web scraping is one of those tasks that you're not going。 to necessarily
    learn super well until you try it out yourself。 So I would encourage you to。
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你只是想看看什么是网络爬虫，你只是好奇，你可以继续跟着。没有必要自己编写代码，这也完全可以。但我确实认为，网络爬虫是那种你不一定会学得很好的任务，直到你自己尝试一下。所以我会鼓励你去。
- en: at some point， attempt to do some web scraping coding with， your own fingers
    on the keys。 All right。 So I'm going to continue on with this copy of the notebook。
    So for the folks that are using the Google Colab version， you'll have a copy as
    well。 For those of you that have not used Google Colab yet， I just wanted to provide
    a brief。
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在某个时刻，尝试用自己的手指在键盘上进行一些网络爬虫编程。好的。我将继续使用这个笔记本的副本。因此，对于使用Google Colab版本的人，你们也会有一个副本。对于那些还没有使用Google
    Colab的人，我想提供一个简短的介绍。
- en: introduction to what this is。 Google Colab is really， really similar to a Jupyter
    notebook。 Really the idea is going to be that Colab will execute our Python code
    on the fly。 Because we'll be submitting code and immediately seeing the feedback
    from that code， it really。 allows us to understand， you know， do I have any errors
    or is this code getting exactly。
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 这是对其的介绍。Google Colab与Jupyter notebook非常相似。实际上，Colab的理念是能够即时执行我们的Python代码。因为我们会提交代码并立即看到反馈，这真的让我们能够理解，比如说，我是否有错误，或者这段代码是否完全。
- en: what I want it to get。 And that's one of the really nice things about having
    Jupyter notebook or Google Colab is。 that instant feedback。 Jupyter does persist
    across the cells。 so you'll see various cells throughout the notebook。 and just
    know that if you define a variable in one cell， the next cell will also know what。
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我想要得到的东西。这是使用Jupyter notebook或Google Colab的一个非常好的地方，便是即时反馈。Jupyter在单元之间是持久的，因此你会看到笔记本中有不同的单元。只需知道，如果你在一个单元中定义了一个变量，下一个单元也会知道。
- en: that variables value is。 The thing to remember about when you are executing
    these cells。 the way to like actually have， that Python code be evaluated is by
    pressing shift and then enter or you can press the。 play button。 Let me show you
    what that looks like quickly。 So here's our first cell of code。 I could either
    be clicked into the cell and hit shift enter or I can press this play button。
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 变量值是什么。记住，当你执行这些单元时，实际评估Python代码的方法是按下Shift键然后按Enter，或者你可以按播放按钮。让我快速给你展示一下那是什么样子。这是我们的第一个代码单元。我可以点击单元并按Shift+Enter，或者我可以按这个播放按钮。
- en: And one final thing about Google Colab， the text that you see here is actually
    with something。 called Markdown。 You can learn more about Markdown if you'd like。
    But basically the really nice thing here about Google Colab or Jupyter notebook，
    it allows。 me to add notes for you with text as well as Python code so I can describe
    exactly what。
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 关于Google Colab最后一件事，你看到的文本实际上是某种称为Markdown的东西。如果你想了解更多关于Markdown的内容，可以学习一下。但基本上，Google
    Colab或Jupyter Notebook的一个非常好的地方在于，它允许我用文本和Python代码为你添加注释，这样我可以准确描述。
- en: the next code is going to be doing。 So that's Google Colab。 Let's also talk
    briefly about what beautiful soup is because it becomes so integral to this。 entire
    tutorial。 Let's spend some time talking about it。 It's a beautiful soup is an
    open source Python library and it's only job really is to extract。
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个代码将会做什么。所以这就是Google Colab。让我们也简要讨论一下Beautiful Soup，因为它对整个教程变得至关重要。让我们花一些时间来讨论它。Beautiful
    Soup是一个开源的Python库，它的唯一工作就是提取。
- en: data from HTML files。 So you can use it to understand the structure of HTML
    by working with a parser。 I won't talk too much about the various different parsers
    and the different options you have。 but if you want to learn more about that you
    can click on this link。 We'll just be using the default LXML parser。 Also I wanted
    to mention that the beautiful soup documentation is awesome。
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 来自HTML文件的数据。因此，你可以通过使用解析器来理解HTML的结构。我不会过多讨论各种不同的解析器和你可以选择的不同选项，但如果你想了解更多，可以点击这个[链接](https://example.org)。我们只会使用默认的LXML解析器。我还想提到，Beautiful
    Soup的文档非常出色。
- en: So you should definitely check it out if there's ever anything custom that you
    want to do if。 you want to go beyond what I show you today。 This documentation
    is a great reference to have。 One other point that I definitely want to make clear
    at this at this juncture。 Beautiful soup does not actually gather information
    from the web。
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，如果你有任何自定义需求，绝对应该查看一下，如果你想超越我今天展示的内容。这份文档是一个很好的参考。我还想在此时明确一点，Beautiful Soup实际上并不会从网络上收集信息。
- en: We're going to use another package called requests for that。 What beautiful
    soup does is takes HTML string text and learns about its structure。 It parses
    through that structure in order to know where the various tags live and what。
    the DOM looks like。 It knows about all that structure but basically it's just
    taking HTML string and learning its。
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用另一个名为requests的包来实现这一点。Beautiful Soup的功能是接收HTML字符串文本并了解其结构。它解析该结构以了解各种标签的位置以及DOM的样子。它知道所有这些结构，但基本上它只是接收HTML字符串并学习它的结构。
- en: structure。 We're going to use a separate library to actually extract the HTML
    code from the web pages。 So even to hammer home that point a little bit further
    everything in this notebook is。 going to be HTML that we have locally。 We're not
    going to gather any HTML from the web yet。 We'll do that in the next section。
    So let's get started。
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用一个单独的库来实际从网页中提取HTML代码。因此，进一步强调这一点，这个笔记本中的所有内容都是我们本地的HTML。我们还不会从网络上收集任何HTML。我们将在下一个部分进行处理。那么，让我们开始吧。
- en: The first HTML that I want to parse with beautiful soup is this。 And it's a
    really simple HTML file that I created myself。 I've even called it simple HTML。
    And notice that I have these three quotations up here and here。 That just means
    that this is a long string。 It's just a string。
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我想用Beautiful Soup解析的第一个HTML是这个。这是我自己创建的一个非常简单的HTML文件。我甚至给它起了个名字叫简单HTML。注意我在这里和这里有这三个引号。这只是意味着这是一个长字符串。它只是一个字符串。
- en: It just so happens to have the structure of an HTML page where we have a head
    and a body。 and various different tag elements。 So let's go ahead and execute
    this cell。 Click into the cell and then hit shift enter。 Okay。 So now Google Colab
    knows about this variable simple HTML。
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 正好它具有HTML页面的结构，我们有一个头和一个主体，以及各种不同的标签元素。那么我们继续执行这个单元吧。点击进入单元，然后按下shift和enter。好的。现在Google
    Colab知道了这个变量simple HTML。
- en: One quick review before you move on to the fun beautiful soup part of this lecture。
    What tags do we see on this HTML page？ So I definitely at least see there's a
    div tag in H1。 ul which actually stands for unordered， list and then various different
    list items。 You might have seen this br and we're thinking is that a tag？ What
    is that？
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在你进入本次讲座有趣的beautiful soup部分之前，快速回顾一下。我们在这个HTML页面上看到什么标签？所以我至少看到有一个div标签和H1，ul，实际上代表无序列表，还有各种不同的列表项。你可能见过这个br，我们在想这是不是一个标签？那是什么？
- en: Br stands for break as in line break。 Okay。 So actually a tag or an element
    is just formatting。 Okay。 What about what attributes do you guys see？ All right。
    Looks like we have this style attribute right here。 And that's some extra styling
    going on in the header。
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: Br表示换行，就像换行一样。好的。所以实际上一个标签或元素只是格式化。好的。你们看到什么属性？好的。看起来我们这里有这个style属性。这是头部的一些额外样式。
- en: And the final question is what's the inner HTML text of the header？
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一个问题是头部的内部HTML文本是什么？
- en: Hopefully you can find that we only have one header tag here。 So the inner HTML
    text of this header tag is today's learning objectives。 All right。 So I actually
    want to show you what this HTML would look like if we rendered it in the browser。
    And by that I just mean open this HTML file somewhere and let the browser create
    the page。
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 希望你能发现这里只有一个头标签。那么这个头标签的内部HTML文本是今天的学习目标。好的。所以我实际上想给你展示一下，如果我们在浏览器中渲染这个HTML会是什么样子。我只是想在某个地方打开这个HTML文件，让浏览器创建这个页面。
- en: We can actually do that in Python by using this display。 This is what that page
    would look like if I opened it as a browser tab。 I have today's learning objectives
    and then I just have the four different learning objectives。 that we talked about
    earlier。 So what if I actually wanted to scrape this page and extract each of
    those learning objectives。
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们实际上可以通过使用这个显示在Python中做到这一点。这就是如果我将其作为浏览器标签打开时该页面的样子。我有今天的学习目标，然后我只有四个不同的学习目标。那我们之前谈论过的。如果我实际上想抓取这个页面并提取每一个学习目标怎么办？
- en: and maybe save it in a Python list？ We can use beautiful soup to accomplish
    that。 So first off we have to import beautiful soup。 We're going to give it an
    alias BS。 So I'll be referring to beautiful soup as BS throughout the rest of
    this document。 Once we've imported that， all we really have to do is wrap the
    simple HTML string in this。
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 并且也许将其保存在一个Python列表中？我们可以使用beautiful soup来完成这个任务。那么首先我们必须导入beautiful soup。我们将给它一个别名BS。所以在本文档的其余部分我将称beautiful
    soup为BS。一旦我们导入它，我们真正需要做的就是用这个包装simple HTML字符串。
- en: BS command and we're going to save the output of that soup。 So if I printed
    soup out for you。 it doesn't look super impressive， does it？ It kind of looks
    like beautiful soup didn't do anything。 It just looks like the same thing as what
    the HTML was。 But don't worry。 beautiful soup has actually learned a lot more
    at this point。 So if I check the type of soup。
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: BS命令，我们将保存那个soup的输出。如果我为你打印soup，它看起来并不特别令人印象深刻，对吧？它看起来好像beautiful soup没有做什么。它看起来和HTML是一样的。但别担心。实际上，beautiful
    soup在这一点上已经学到了更多的东西。所以如果我检查soup的类型。
- en: it is no longer a string。 Now it is a beautiful soup object。 And that just means
    that beautiful soup knows about the structure of the HTML and I can find。 various
    elements if I'd like。 Speaking of that， let's try to find something by the tag。
    Now that I had this beautiful soup object called soup， I'm going to use my first
    command。
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 它不再是一个字符串。现在它是一个beautiful soup对象。这意味着beautiful soup了解HTML的结构，如果我想的话，可以找到各种元素。说到这里，让我们尝试通过标签找到一些东西。现在我有了这个叫做soup的beautiful
    soup对象，我将使用我的第一个命令。
- en: called find。 So I just take the beautiful soup object dot find and then I'm
    asking it to look for an。 H1 tag to see if it can find it。 Indeed there was an
    H1 tag on my page that was today's learning objectives。 And you'll notice that
    soup is actually giving me the full tag。 It would have the tag。 the attributes
    and the inner HTML。 All of that would come back because it's giving me the full
    element tag。
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 叫做 find。所以我只需使用 beautiful soup 对象的 dot find，然后让我查看 H1 标签以确认它是否可以找到。确实，我的页面上有一个
    H1 标签，内容是今天的学习目标。你会注意到，soup 实际上给了我完整的标签。它将包含标签、属性和内部 HTML。所有这些都会返回，因为它给我的是完整的元素标签。
- en: So if I actually check the type of this， this is a beautiful soup element tag。
    What I often want to know is not this full tag， the H1 and etc。 I'm really usually
    just trying to grab some text off of a web page。 So I can do that by now instead
    of just requesting the H1 tag， then I'll use this property dot。
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我实际上检查这个的类型，这就是一个 beautiful soup 元素标签。我通常想知道的并不是这个完整的标签，H1 等等。我通常只是想从网页上提取一些文本。所以现在，我不仅仅请求
    H1 标签，而是会使用这个属性 dot。
- en: text。 So I just want the inner HTML text from this tag。 Okay。 And now that the
    result of dot text is indeed a string， which is something that maybe we。 would
    store and then later do some analyses on。 So that's pretty cool。 I already know
    how to find information from an HTML file。
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 文本。所以我只想要这个标签的内部 HTML 文本。好的。现在 dot text 的结果确实是一个字符串，这可能是我们会存储的东西，然后进行一些分析。所以这真的很酷。我已经知道如何从
    HTML 文件中查找信息。
- en: We can look for the header text if we'd like。 What happens if we start looking
    for those list items？
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们愿意，可以查找标题文本。那么如果我们开始查找那些列表项，会发生什么呢？
- en: So you remember we had header， we had an unordered list and then we had various
    different list。 items。 What if I tried to do the same thing for the list items？
    So it did find a list item。 but it only found one of them。 So this is the first
    really word of caution that I'm giving you here。 Beautiful soup will only return
    the first matching element if you're using the command， find。
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 所以你记得我们有标题，有一个无序列表，然后有各种不同的列表项。如果我试图对列表项做同样的事情，会怎样呢？它确实找到了一个列表项，但只找到了其中一个。这是我在这里给你的第一个真正的警告。如果你使用命令
    find，Beautiful soup 只会返回第一个匹配的元素。
- en: It only returns one thing if it finds something that matches what you've asked
    for。 So instead of just using find， oftentimes what's useful is to use find all。
    Here our original goal was to actually gather all of today's learning objectives
    and put。 those into a list。 So we can do that with a command called find all。
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 如果它找到与你请求的内容匹配的东西，它只会返回一个结果。因此，通常情况下，使用 find all 更为有用。我们最初的目标是收集今天的所有学习目标，并将它们放入一个列表中。所以我们可以通过一个叫做
    find all 的命令来实现。
- en: Now instead of just saying find， I'm saying find underscore all。 And what this
    will do is actually match all of the different tags that are list items。 And you'll
    see that I have these square brackets surrounding this match set。 And this is
    actually a result set。 You can think of this as a list。 It's pretty similar。
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我不是仅仅说 find，而是说 find underscore all。这将实际匹配所有不同的列表项标签。你会看到，我有这些方括号围绕着这个匹配集。这实际上是一个结果集。你可以把它想象成一个列表。它非常相似。
- en: You can actually loop over this result set if you'd like。 You could refer to
    the result set by its index numbers。 You could do similar things to what you can
    do with a list。 So find all is going to give you back a result set with all of
    the elements on the entire。
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你愿意，实际上可以对这个结果集进行循环。你可以通过索引号引用结果集。你可以做与列表类似的事情。因此，find all 将返回包含所有元素的结果集。
- en: page that match whatever you're asking soup to find。 Another word of caution。
    you cannot just apply dot text to a result set。 So if I tried to find all of the
    list items and then just do dot text。 this is going to， give me an error。 And
    what's happening here is that I'm trying to extract the text of each element。
    but I'm， applying it to the full result set。 And that's just not going to work。
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 页面上匹配你让 soup 查找的内容。另一个警告是，你不能仅仅将 dot text 应用于结果集。所以如果我尝试查找所有的列表项，然后直接使用 dot
    text，这将会给我一个错误。发生的事情是，我试图提取每个元素的文本，但我将其应用于整个结果集。这是行不通的。
- en: Instead we actually need to look for the dot text property of each element individually。
    So one way to do that， we could actually loop over the result set and just print
    out for。 every item I find in the result set， print out the item dot text。 So
    grab the text string。 Okay。 That totally works。 Each of these are now strings
    and they are those learning objectives that we were trying。
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，我们实际上需要单独查看每个元素的 dot text 属性。所以一种方法是，我们可以遍历结果集，并打印出每个我在结果集中找到的项目的 item dot
    text。抓取文本字符串。好的，这完全有效。这些现在都是字符串，它们是我们想要的学习目标。
- en: to find from the beginning。 Or I could use a nice list comprehension。 So here
    I'm， again。 I'm applying dot text to every single item in the results that find，
    all。 And now I've done exactly what I wanted to do。 I saved all the learning objectives
    in a Python list。 Each of those learning objectives being strings。 Awesome。
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 从一开始就找到。或者我可以使用一个不错的列表推导式。所以我又来了。我对结果中的每一个项目应用了 dot text，全部保存下来了。现在我完成了我想做的事情。我将所有学习目标保存在一个
    Python 列表中。每个学习目标都是字符串。太棒了。
- en: So before we move to our first set of exercises， I just want to be very clear
    by saying， you， know。 the two most common mistakes that I see when people are
    trying to do web scraping。 is using find when you really want to find all。 Remember
    find only gives you one item。 If you want everything that matches your criteria，
    you have to use find all。
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们进入第一组练习之前，我想明确说，知道。人们在尝试进行网页抓取时，我看到的两个最常见的错误是使用 find 而不是使用 find all。记住，find
    只给你一个项目。如果你想要所有匹配你的标准的内容，你必须使用 find all。
- en: The other really common thing I see people doing is applying dot text to a result
    set。 And you have to do that element wise either with a loop or with a list comprehension
    or。 something like that。 So keep those two things in mind。 And let's go ahead
    and try to tackle our first exercises。
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 我看到的另一个常见错误是对结果集应用 dot text。你必须逐个元素进行操作，使用循环或列表推导式，或者类似的东西。所以记住这两点。让我们继续尝试解决我们的第一个练习。
- en: Just make the screen a little bit bigger for you so you can read it。 Alright。
    so let's try some new HTML。 So this is a new string for you workshop HTML。 And
    it's just some new HTML text。 And you're going to try to complete the next couple
    of exercises。 By the way， if I render this in my browser， it would look like this。
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 只是把屏幕放大一点，以便你可以阅读。好的。让我们尝试一些新的 HTML。这是一个新的字符串给你作为研讨会 HTML。它只是一些新的 HTML 文本。你将尝试完成接下来的几个练习。顺便说一下，如果我在我的浏览器中渲染它，它会看起来像这样。
- en: So I have the things that we're going to do today on our agenda and a couple
    of tools。 that we're going to be learning about。 So first question for you is
    to find the header。 Parse through the workshop HTML with beautiful soup and extract
    the header and save it as。 a variable。 Make sure that you only save the header
    text and check that with the type。
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我今天在议程上要做的事情和一些我们将要学习的工具。第一个问题是找到标题。用 Beautiful Soup 解析研讨会 HTML，并提取标题并保存为一个变量。确保你只保存标题文本，并检查其类型。
- en: Check the type of your variable to make sure that you have a string。 The second
    exercise that you can do is to now find all the different paragraphs in the。 workshop
    HTML and print out whatever text you find。 And then finally， as a bonus。 this
    one is a little bit more challenging。 Create a list of all of today's agenda items。
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 检查你的变量的类型，确保它是字符串。你可以做的第二个练习是现在找到研讨会 HTML 中的所有不同段落，并打印出你找到的任何文本。最后，作为奖励，这个稍微有挑战性。创建一个今天所有议程项目的列表。
- en: Make sure that you only store the text of the agenda items。 See if you can figure
    out a clever way to do that。 So I'm going to be proceeding by showing you the
    solutions。 Go ahead and hit pause。 If you are following along and coding， go ahead
    and hit pause and work on these exercises。
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 确保你只存储议程项目的文本。看看你能否想出一个聪明的方法来做到这一点。所以我将通过展示解决方案来继续进行。请暂停。如果你在跟着编码，请暂停并完成这些练习。
- en: Come back when you have a solution。 So here we go。 For the first exercise。 the
    first thing we wanted to do is actually just parse this HTML， by using beautiful
    soup。 So let's call this something like maybe workshop soup， something like that。
    And all we have to do is take beautiful soup and apply it to our workshop HTML。
    All right。
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 有了一个解决方案后再回来。所以我们开始吧。第一个练习。我们想做的第一件事实际上是使用 Beautiful Soup 解析这个 HTML。所以我们叫它 workshop
    soup，类似这样的名字。我们要做的就是将 Beautiful Soup 应用到我们的研讨会 HTML 上。好的。
- en: So that will do it。 Now workshop soup is a beautiful soup object。 All right。
    We can now use workshop soup to find the header。 So let's try to do that workshop
    soup。 The way that I do that， there's only one H1 tag on this page。 So I can just
    use find and I'm looking for H1。 You'll see that this pulls back a beautiful soup
    tag。
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这就完成了。现在工作坊 soup 是一个漂亮的 soup 对象。好的。我们现在可以使用工作坊 soup 来查找标题。让我们尝试这样做，工作坊 soup。我这样做的方式是，这个页面上只有一个
    H1 标签。所以我只需使用查找，我在寻找 H1。你会看到这拉回了一个漂亮的 soup 标签。
- en: So really what I want to do is only extract the text。 So I need to do also dot
    text。 And let's save that as workshop header。 All right。 And then finally。 the
    last question was to make sure that we do have a string here。 Go ahead and do
    type of workshop header。 And it is a string indeed。 By the way。
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我真正想做的就是提取文本。所以我还需要使用 dot text。让我们把它保存为工作坊标题。好的。最后一个问题是确保我们这里有一个字符串。继续，查看工作坊标题的类型。确实是一个字符串。顺便提一下。
- en: the solutions are also all provided at the bottom of the notebook。 So you have
    those as well。 All right。 Find the paragraph。 Okay。 Let's look for all the different
    paragraphs in this workshop HTML。 So since the question's phrase is all the paragraphs，
    I'm guessing there's not going to。 be just one。 Let's try to find all。 So workshop
    soup。 And now instead of just doing find。
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 解决方案也都在笔记本的底部提供。所以你也可以找到这些。好的。找到段落。好的。让我们寻找这个工作坊 HTML 中的所有不同段落。由于问题的表述是“所有段落”，我猜不会只有一个。让我们试着找到全部。所以工作坊
    soup。现在不是只做查找。
- en: I'm going to do find all and look for those paragraphs。 Yep。 So there are two
    different paragraphs in this HTML。 If I want to only print out the text of each
    of these paragraphs， I actually have to apply。 dot text to each item individually。
    So let's actually loop over this。
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 我要做查找所有并寻找那些段落。是的。所以在这个 HTML 中有两个不同的段落。如果我只想打印出每个段落的文本，我实际上必须对每个项目单独应用 dot text。所以让我们实际上遍历一下。
- en: Let's say for P in workshop soup， dot find all P。 Now I'm just going to print
    out E dot text。 Great。 So in that way， we just printed out the actual text， which
    is a string for each of these paragraph。 elements。 The final exercise is a little
    bit more tricky， and I'll show you why。 It turns out the agenda items are tagged
    as li list items。 But if we try to do workshop soup。
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们说对于 P 在工作坊 soup，dot find all P。现在我只要打印出 E dot text。太好了。这样，我们就打印出了每个段落元素的实际文本，它是一个字符串。最后的练习有点棘手，我会告诉你原因。事实证明，议程项目被标记为
    li 列表项。但是如果我们尝试做工作坊 soup。
- en: dot find all， li， you might have tried this， it turns， out that there are not
    only agenda items。 but also the tools that we're going to use today。 So I'll show
    you a better way to do this coming up。 But for now， one thing that you could do
    to kind of hack your way through this question。 is actually just slice this result
    set。 I said this behaves a lot like a list， and it sure does。
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: dot find all，li，你可能尝试过这个，结果证明不仅有议程项目，还有我们今天要使用的工具。所以我会在后面展示更好的方法。但目前，有一件你可以做的事情是，稍微破解一下这个问题，实际上就是切片这个结果集。我说这个行为与列表非常相似，确实如此。
- en: You can slice。 We actually only want the first three of these。 That's my agenda
    items。 So if I wanted to print out， it stores me store the text for this。 Let's
    write agenda items。 And let's use a list comprehension。 Oops。 Oh， li。 And remember。
    we're slicing that down to just the first three。 Great。 All right。 So again。
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以切片。我们实际上只想要这其中的前三个。这是我的议程项目。所以如果我想打印出来，我需要存储文本。让我们写下议程项目。并且我们将使用列表推导式。哎呀。哦，li。并且记住，我们将其切片到前三个。太好了。好的。所以再次。
- en: I'll show you a better way to do that in just a sec。 But if you recognize that
    you only want to do the first three and use the slice， that。 works just fine。
    All right， let's move on to a bit more complicated example。 And really。 the next
    example goes right along with the problem that we encountered in the。
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我会在稍后给你展示更好的方法。但如果你意识到你只想要前三个并使用切片，那样就很好。好的，让我们进入一个稍微复杂的例子。实际上，下一个例子与我们在。
- en: agenda and these agenda items。 A lot of times you can't just rely on the tag
    names alone。 So there's going to be lots of list items。 There's going to be lots
    of paragraphs and anchors and all of these things。 You're going to have to be
    able to drill down very specifically and look for information。 that you want from
    the web page。 So we have to have more tools than just looking for tags alone。
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 日程和这些日程项目。很多时候你不能仅仅依赖标签名称。所以会有很多列表项，会有很多段落和锚链接，以及所有这些东西。你需要能够非常具体地深入查找你想要的信息。因此，我们需要比仅仅查找标签更多的工具。
- en: So we're going to use this little bit more complicated file。 And the reason
    why I have this not just typed out as text is because I want you to actually。
    look at this file。 So when you execute this command right here。 this will pull
    a file from my GitHub called， piconinfo。html。 So if you got piconinfo。html saved。
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们将使用这个稍微复杂一点的文件。之所以没有将其直接输入为文本，是因为我希望你能实际查看这个文件。所以当你在这里执行这个命令时，这将从我的 GitHub
    中拉取一个名为 piconinfo.html 的文件。所以如果你保存了 piconinfo.html。
- en: that means everything worked out。 And what it's done right now。 if you are using
    the Google Colab version， this is saved， that file within Google Colab。 you need
    to actually also download that to your computer。 So this next cell is just downloading
    that file so that you can have access to it。 So for me。
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着一切都运作良好。而目前它所做的事情是，如果你使用的是 Google Colab 版本，这个文件保存在 Google Colab 中。你实际上还需要将它下载到你的电脑上。所以下一个单元只是下载那个文件，以便你可以访问它。对我来说。
- en: it's just popped up right over here。 It should be downloaded to wherever downloads
    appear on your computer。 I'm going to just go ahead and click on this so you can
    see it。 So this is the file。 It's a little bit more complicated than the test
    examples we've seen before。 And it's just a little fake what's happening at picon
    today。
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 它刚好出现在这里。应该下载到你电脑上下载文件出现的地方。我将直接点击这个，让你可以看到它。所以这是文件。它比我们之前看到的测试示例稍微复杂一点。而且今天在
    picon 上发生的事情有点虚假。
- en: This tutorial was scheduled for April 15th。 So here are some other tutorials
    that were going to be happening on the same day。 as well， as some tutorials that
    were going to be happening on April 16th。 So you can see there's various just
    different links to these tutorials。 You can actually click on these and you'll
    be taken to the picon webpage。
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 这个教程原定于 4 月 15 日。因此，这里还有一些将在同一天发生的其他教程，以及一些将在 4 月 16 日发生的教程。所以你可以看到这些教程的各种不同链接。你实际上可以点击这些链接，它们会带你到
    picon 网页。
- en: And so this is like starting to really simulate an actual webpage that you might
    see and you。 might want to gather data from。 So that's the idea。 Let's go back
    over to our Colab notebook。 And what we're going to do now is just read in that
    HTML file。 Basically that's just saying now I have access to the HTML string。
    And if I were to print that out。
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这开始真正模拟一个你可能看到的实际网页，而你可能想要从中收集数据。这就是这个想法。让我们回到我们的 Colab 笔记本。现在我们要做的就是读取那个
    HTML 文件。基本上这就是说我现在可以访问 HTML 字符串。如果我打印出来。
- en: it looks really similar to what we were using before。 There's just a little
    bit more fancy styling going on。 But it is still an HTML string file。 We have
    the head， we have the body， and various different tags on here。 So we can parse
    this with beautiful soup。 And so again just wrapping that HTML with beautiful
    soup and saving the output as picon。
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 它看起来和我们之前使用的非常相似。只是在样式上稍微复杂了一点。但它仍然是一个 HTML 字符串文件。我们有头部，有主体，以及各种不同的标签在这里。所以我们可以用
    Beautiful Soup 来解析这个。再次将这个 HTML 包裹在 Beautiful Soup 中，并将输出保存为 picon。
- en: soup。 So what if I wanted to gather all of the links for tutorial events that
    are happening today？
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 汤。那么如果我想收集今天发生的所有教程活动的链接呢？
- en: The first thing I might notice， let's go back up here。 So here are a selection
    of today's events。 And you'll see that I have the various titles of those tutorials。
    They're actually inside an anchor tag。 So I have the A here and an A here。 So
    this is actually inner HTML text of anchor tags。
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 我可能注意到的第一件事是，让我们回到这里。所以这里是今天活动的选择。你会看到我有这些教程的各种标题。它们实际上在一个锚标签内。所以我在这里有 A，在这里也有
    A。这实际上是锚标签的内部 HTML 文本。
- en: So my first strategy might just be to find all of the A tags。 Let's see what
    happens。 Let's try that。 Whoa。 All right， so this is what I mean by a lot of times
    the tag alone will not be enough。 to help you delineate the information that you
    actually want。 So it turns out that there's a ton of links on this page。
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我第一个策略可能就是查找所有的 A 标签。让我们看看会发生什么。试试这个。哇。这就是我所说的很多时候，仅凭标签是不够的。来帮助你界定你真正想要的信息。结果显示，这个页面上有很多链接。
- en: There's things about going to the Python software foundation， et cetera。 There's
    tomorrow's events。 not just today's。 So we have to be a lot more specific about
    the types of anchor tags we would like to。 retrieve。 So one thing you can do is
    actually find things by attribute。 Let me go back up to the actual code and show
    you。
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 有关于前往 Python 软件基金会等的信息。还有明天的事件，而不仅仅是今天的。因此，我们必须更加具体地说明我们希望检索的锚标签类型。你可以通过属性查找东西。让我回到实际代码中给你展示。
- en: If we were to look at this code a little bit longer， you'll see that all of
    today's events。 are actually contained within this division。 And its ID is today。
    So we can leverage this。 We can look for this division about today and then look
    for the anchor tags within this， division。 Okay。 So if I have an ID that is unique
    to some element on my page， now instead of saying find and。
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们再仔细看看这段代码，你会发现所有今天的事件实际上都包含在这个 division 中。它的 ID 是 today。所以我们可以利用这一点。我们可以查找关于今天的这个
    division，然后在这个 division 中查找锚标签。好的。如果我有一个在页面上唯一的元素 ID，那么现在可以说查找和。
- en: then a tag name， I'll say find where ID is equal to today。 Great。 And so the
    return of this kind of operation， the return of this find is still a beautiful。
    soup element tag。 But now it's just the whole div tag rather than an individual
    P or header。 So what we can do is we can continue searching now within this beautiful
    soup element。
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 然后是标签名称，我会说查找 ID 为 today 的元素。太好了。这样操作的返回结果，依然是一个美丽的 soup 元素标签。但现在返回的是整个 div
    标签，而不是单个的 P 或 header。所以我们可以继续在这个美丽的 soup 元素中进行搜索。
- en: So this is one of the really powerful things about beautiful soup。 It doesn't
    have to be that you have the perfect strategy for extracting each individual thing。
    you want from the webpage directly。 You can actually extract larger chunks of
    your HTML code and then continue to drill down。 to the elements that you actually
    want。 So what we're going to do is actually take this today div and now look for
    a tags within。
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是美丽的 soup 其中一个非常强大的地方。你不必有完美的策略来直接从网页中提取每一个你想要的东西。实际上，你可以提取较大的 HTML 代码块，然后继续深入查找你真正想要的元素。所以我们要做的是取这个
    today div，并在其中查找 a 标签。
- en: today div。 So you'll see instead of querying the full soup， I'm looking at today
    div only and then。 I'm doing find all of the anchor tags。 Okay。 So it's going
    to be really powerful to just drill down deeper into only the information。 you
    want。 And by the way， you can really use pretty much any attribute that you like。
    Here is if you wanted to find something by class instead of ID， all you have to
    do here。
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: today div。所以你会看到，除了查询完整的 soup，我只查看 today div，然后。我正在查找所有的锚标签。好的。所以深入查找你想要的信息会非常强大。顺便提一下，你几乎可以使用任何你喜欢的属性。如果你想按类查找而不是
    ID，只需在这里执行。
- en: is just do class underscore。 There are two items that have the class is equal
    to events。 It's actually today's and tomorrow's are both considered events on
    this webpage。 So you'll see you have two divs that satisfy this query。 And finally。
    if you have like multiple attributes that you want to match on， you could pass。
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 只需做 class underscore。实际上有两个项目的类为 events。今天的和明天的都被视为这个网页上的事件。所以你会看到有两个 div 满足这个查询。最后，如果你想匹配多个属性，你可以传递。
- en: that with a little dictionary。 So this is saying find all of the elements in
    my Python soup where class is equal to events。 and ID is equal to tomorrow。 So
    I'm just passing that as a dictionary with the attribute name as the key and the
    attribute。 value as the value。 Okay。 So that actually only match one element。
    but it's still in a result set because we use， to find all。 Just keep those kind
    of things in mind。
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是使用一个小字典。所以这段话的意思是查找我的 Python soup 中所有类为 events，且 ID 为 tomorrow 的元素。所以我只是将其作为一个字典传递，属性名称作为键，属性值作为值。好的。实际上，这只匹配一个元素，但它仍在结果集中，因为我们使用了，来查找所有。请记住这些事情。
- en: So that can be really helpful if you're looking for specific things on a webpage。
    The other thing you might be interested in and this happens pretty frequently
    is that。 you might want to extract an attribute value itself。 So if I were to
    look through， so remember。 if I query today div and look for all of the， anger
    tags。
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在网页上寻找特定的东西，这可能非常有帮助。你可能感兴趣的另一件事是，这种情况发生得相当频繁。你可能想要提取一个属性值本身。所以如果我去查看，记住。如果我查询今天的
    div 并查找所有的链接标签。
- en: I'm going to get all of today's or a selection of today's tutorials that we're，
    going to happen。 And that has the full a tag。 If I wanted to extract just the
    inner HTML text， I would say link。text and that would just， give me the name of
    the tutorial。 But it's pretty common that we， you know。 oftentimes we want the
    text， but sometimes we， also want things like， what is this link to？
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 我将获取今天的所有教程或选择其中的一部分，这些都将发生。并且这包含完整的标签。如果我想提取仅仅是内部 HTML 文本，我会说 `link.text`，这将仅给我教程的名称。但这很常见，我们，知道。我们往往想要文本，但有时，我们也想要像，这个链接指向什么？
- en: What is the hyperlink that this text is associated with？ So you can do that
    pretty easily as well。 Let's just find the very first instance of one of the tutorial
    anchor tags。 All right。 If I now take this result and look for the Href attribute，
    what comes out of this is the。 link itself。 Okay。 So I'm basically querying this
    anchor tag and looking for this attribute and you just。
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 这个文本关联的超链接是什么？所以你也可以很容易做到这一点。让我们找到第一个教程链接标签的实例。好的。如果我现在取这个结果并查找 Href 属性，出来的是链接本身。好的。所以我基本上是在查询这个链接标签并寻找这个属性，你只需。
- en: use square brackets with the attribute name right inside。 That will return you
    whatever value has been assigned to that attribute。 And just， you know。 for clarity
    here， the type of that， this is actually a string now。 Google Collabs just being
    very clever and recognizing that that's a link。
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 使用方括号将属性名称放在里面。这将返回分配给该属性的任何值。而且，知道。为了清楚，这实际上是一个字符串。谷歌 Colab 非常聪明地识别这是一个链接。
- en: You could actually click on this， but that is a string。 All right。 So the way
    that I could gather up all of the different links for today's events， it's really。
    similar to what I do when I gather the text， except for now instead of doing link。text，
    I'm。 doing link Href。 Okay。 I'm referencing that attribute。 And I can put that
    in the list comprehension。
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 你实际上可以点击这个，但这只是一个字符串。好的。所以我可以收集今天活动的所有不同链接，方法与我收集文本时非常相似，只是现在我不是在做 `link.text`，而是做
    `link.href`。好的。我在引用那个属性。我可以将其放入列表推导中。
- en: And there I have all of today's links。 All right。 So we saw some really cool
    stuff。 We saw how you can， you know， find a larger division or something like
    that and then drill。 down into， you know， very specific anchor tags or paragraphs
    that you're interested in。 We also saw how you could extract attribute values。
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 而我有今天的所有链接。好的。所以我们看到了一些非常酷的东西。我们看到你可以，知道，找到一个更大的分区或类似的东西，然后深入，知道，你感兴趣的非常特定的链接标签或段落。我们还看到你如何提取属性值。
- en: So let's do a couple of exercises to make sure that the concepts are solidified。
    I want you to try to create a list of tuples for tomorrow's events。 The first
    element in your tuples should be the event title and the second element in。 your
    tuple will be the event link。 And you're going to have a list for everything that's
    happening according to this webpage。
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 所以让我们做几个练习，以确保概念得到巩固。我希望你尝试为明天的活动创建一个元组列表。你元组中的第一个元素应该是活动标题，第二个元素将是你的元组中的活动链接。你将为根据这个网页发生的所有事情准备一个列表。
- en: tomorrow， the selection of tutorials for tomorrow。 So number five， finding the
    event headers。 So go back through， check out the code and figure out a strategy
    for extracting the header。 text for today's and tomorrow's events。 And there it's
    giving you the hint that you should reference the events class。 So I'm going to
    walk through the solution。 Be sure to hit pause if you'd like to participate。
    Okay。
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 明天，明天的教程选择。所以第五，找到事件标题。返回查看代码，并找出提取今天和明天事件标题文本的策略。那里给你提示，你应该引用事件类。所以我将逐步讲解解决方案。如果你想参与，请务必按暂停。好的。
- en: So for this first one， what I'm really trying to do is extract not only the
    text， but also。 the link。 So let's build this up。 Let's kind of work through it。
    So I have， it's called PyCon soup。 I believe。 If I want to look for tomorrow's
    events， the nice thing is that that is contained within。 that tomorrow div。 So
    I could do something like find ID equals tomorrow。 Let's just look at that。
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 所以对于第一个，我实际上想提取的不仅是文本，还有链接。所以让我们来构建这个。让我们逐步处理一下。我有，它叫 PyCon soup。我相信。如果我想查看明天的事件，幸运的是，这包含在明天的
    div 中。所以我可以做类似 find ID = tomorrow 的操作。让我们看看这个。
- en: That is the division that has all of the information about what's happening
    tomorrow。 So really what I have to do now is query the result of this and now
    look for each of those。 anchor tags。 Okay。 So I could definitely save this result
    as something like tomorrow div or something like。 that。 I could totally save it。
    Or I can actually just chain methods together。
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 这是包含明天发生的所有信息的部分。所以我现在要做的就是查询这个结果，并查看每一个锚标签。好的。我可以将这个结果保存为类似明天 div 的东西，或者我可以将方法连接在一起。
- en: We'll talk more about this in the next section as well。 But I can just take
    this result and immediately now look for the anchor tags by doing this。 So whatever
    comes out of this first bit is now just typed right into the next command。 So
    now I'm finding all of the anchor tags。 That gives me a result set with three
    different events that are listed for tomorrow。
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在下一部分会进一步讨论这个。但我可以直接拿这个结果，立即寻找锚标签。所以下一条命令就直接用这个结果。因此，我现在正在寻找所有的锚标签。这给了我一个结果集，其中列出了三个明天的事件。
- en: And finally I want to come up with tuples for the event titles and the links。
    So this can just definitely be a list comprehension。 Let's call it tomorrow tuples。
    And I'm going to be building up a list comprehension from this。 Let's say for
    link in PyCon soup。 And so for each of these links， the first element of this
    tuple is going to be link。text。
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我想为事件标题和链接生成元组。这绝对可以是一个列表推导。我们称之为明天元组。我将从这里构建一个列表推导。假设对于 PyCon soup 中的链接。对于这些链接，元组的第一个元素将是
    link.text。
- en: And the second element is going to be link href。 And let's just check and make
    sure that we did that correctly。 Cool。 All right。 Finding the event headers。 So
    it turns out that there are， let's go back and look。 I believe there are two different
    headers contained in this file。 There we go。 All right。 Oh， yes。 So there's an
    h2 right here and then another h2 here。 Okay。
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个元素将是 link href。让我们检查一下，确保我们这样做是正确的。很酷。好的。寻找事件标题。所以结果是，让我们回去看看。我相信这个文件中包含两个不同的标题。太好了。哦，是的。这有一个
    h2 在这里，还有另一个 h2 在这里。好的。
- en: So I want to extract the divs that have the events class and then drill down
    into these， h2 headers。 I actually think I probably could do this without the
    events class， but let's just follow。 the directions。 All right。 All right。 So
    let's go ahead and drill down into those two divs that have the events class。
    What I would do is say， hey， PyCon soup。 I want you to find all of the tags that
    satisfy the following。
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我想提取包含事件类的 div，并深入这些 h2 标题。我实际上觉得我可能可以不使用事件类，但让我们还是遵循指示。好的。好的。所以让我们深入这两个包含事件类的
    div。我要说，嘿，PyCon soup。我希望你找到所有满足以下条件的标签。
- en: The class has to be equal to events。 All right。 So that gives me two different
    tags。 Now I'm going to be looking through each of these and finding the h2。 Okay。
    Let's actually。 Let's do this。 See， event days。 Let's call it that。 All right。
    So now what I actually have to do is cycle through each of those event days and
    look for， the h2。
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 这个类必须等于事件。好的。这给了我两个不同的标签。现在我将查看每一个，并找到 h2。好的。让我们来实际操作一下。看，事件日期。就叫它这个吧。好的。现在我实际上需要做的是遍历每一个事件日期，并寻找
    h2。
- en: Okay。 Let's try that。 Okay。 So I'm looking for， for each of the event days。
    So for each day。 Let's do it like this for day and event days。 That'll work。 For
    those。 I'm actually going to apply this dot find h2。 Okay。 Let's make sure that
    worked。 All right。 Great。 So what I did here is just for each of those elements
    that I've had in my results set。
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 好的。让我们尝试一下。所以我在寻找每一个事件日期。对于每一天。让我们这样做，针对日期和事件日期。这可以工作。对于这些。我实际上会应用这个 .find h2。好的。让我们确认一下是否成功。好的。很棒。所以我在这里做的是针对结果集中每一个元素。
- en: Now I'm doing a find on each of those individually and I'm looking for my h2
    tag。 Finally。 I do want just the text for that。 So I really could have， let's
    just call this headers。 And we can just do the dot text since this is a find。
    Great。 So those are strings and kind of like keeping all of this in mind， the
    find alls， the finds。
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我正在单独对每一个进行`find`操作，我在寻找我的h2标签。最后，我确实只想要那部分文本。所以我可以称之为`headers`。我们可以直接使用`.text`，因为这是一个`find`操作。很好。所以这些是字符串，考虑到这一切，`find_all`和`find`。
- en: doing the dot text。 I think you guys are off to a really great start at this
    point。 These are a lot of the core building blocks of what happens in web scraping。
    So that was the end of the basic web scraping。 Let's head back to the deck and
    recap what we learned。 That was the scraping basics part of this tutorial。
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 进行文本处理。我认为你们目前进展非常顺利。这些都是网页抓取中的核心构建模块。这是基础网页抓取的结束。让我们回到幻灯片，回顾一下我们学到的内容。这是本教程的抓取基础部分。
- en: So the main things that you learned in that is how to parse HTML。 So we took
    a look at a lot of example HTML files and then finally we had kind of one。 more
    involved example。 But you did learn things certainly got experience looking at
    beautiful soup。 And you learned how to locate elements with find and with find
    all。
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 所以你在这方面学到的主要内容是如何解析HTML。我们查看了许多示例HTML文件，最后我们有一个更复杂的示例。但你确实学到了一些内容，并在使用Beautiful
    Soup时获得了经验。你学会了如何使用`find`和`find_all`来定位元素。
- en: Find really just giving you back the first match for whatever tags or attributes
    you're。 putting in there， whatever first element matches that， that's what you'll
    get with find。 Whereas find all will give you all of the different elements that
    match your query。 We learned how to find things by either tag or attribute or
    both。 That works too。
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '`find`实际上只是返回你所放入的标签或属性的第一个匹配项，不管是什么。匹配的第一个元素就是你通过`find`得到的结果。而`find_all`会给你所有匹配你查询的不同元素。我们学习了如何通过标签、属性或两者来查找东西，这也能奏效。'
- en: We learned how to retrieve attribute links。 And this turns out to be really。
    really important for things like web scraping pipelines， and for larger data science
    projects。 Sometimes what we'll do is actually scrape links so that we can then
    go follow those links。 and scrape those links。 So it turns out to be really important
    to kind of have all of these levers in play to be。
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 我们学习了如何检索属性链接。这实际上对于网页抓取管道和更大规模的数据科学项目非常重要。有时我们会抓取链接，以便可以跟随这些链接并抓取它们。因此，拥有这些工具实际上非常重要。
- en: able to get links so that we can follow links， etc。 So that was your scraping
    basics。 Now we're going to move on to the final portion of this tutorial which
    is a scraping pipeline。 section。 Next one is going to be a different Google Colab
    notebook and you can access that Google。 Colab notebook here。 Go ahead and again
    open another tab and type in this bit。
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 能够获取链接，以便我们可以跟随链接，等等。这就是你的抓取基础。现在我们将进入本教程的最后部分，即抓取管道部分。下一个将是一个不同的Google Colab笔记本，你可以在这里访问那个Google
    Colab笔记本。请再次打开一个新标签页，输入这段。
- en: ly/picon2020_scraping_wicky。 Okay， so following that link should bring you to
    this webpage。 And so you'll see this is slightly different than the other notebook
    because we have this。 Wikipedia icon right here。 So you should be seeing a notebook
    that has that icon。 But similarly to what we did last time， if you are going to
    follow along with this Google。
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: ly/picon2020_scraping_wicky。好吧，跟随这个链接应该能带你到这个网页。所以你会看到这个笔记本与其他的略有不同，因为我们这里有这个维基百科图标。所以你应该看到一个有这个图标的笔记本。但与我们上次所做的类似，如果你想继续跟着这个Google。
- en: Colab notebook， you can just create a save a copy in your drive。 And that'll
    bring you up another tab that is your copy that is yours to keep。 If you are not
    signed into Google and you don't want to do that， you can just follow。 this link
    here。 Again， it'll take you to my GitHub page and you can download the Jupyter
    notebook。
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在Colab笔记本中，你可以创建并保存一份副本到你的云端硬盘。这将打开另一个标签页，显示你的副本，供你保留。如果你没有登录Google而且不想这样做，你可以跟随这个链接。再次，它会带你到我的GitHub页面，你可以下载Jupyter笔记本。
- en: And if you are following the third option of just following along with the video，
    no problem。 That's totally fine too。 Okay， so for this section， this is going
    to be a bit more involved。 This section is this is kind of the meat and the bones
    of what we're going to be talking。 about for this tutorial。 I actually want to
    start off with by investigating a webpage。 All right。
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你选择第三个选项，只是跟着视频学习，那也没问题。这完全可以。好吧，对于这一部分，这将会更加复杂。这一部分是我们在本教程中要讨论的核心内容。我实际上想要从调查一个网页开始。好的。
- en: so so far we've just parsed through some test HTML。 Now we're actually going
    to be retrieving information from the internet and parsing。 through HTML that
    we find on real web pages。 So let's go ahead and open up an actual webpage is
    the one that we're going to be parsing。 And it is going to be the Wikipedia page
    for the state of Pennsylvania。
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们只解析了一些测试用的 HTML。现在我们将实际从互联网上获取信息，并解析我们在真实网页上找到的 HTML。让我们打开一个实际的网页，这就是我们要解析的网页，它将是宾夕法尼亚州的维基百科页面。
- en: So go ahead and click on this link。 And that'll bring you to this wiki page
    for Pennsylvania。 So it turns out that you can investigate and inspect all of
    the HTML that is powering， this page。 So what I want you to do is go to somewhere
    in the middle of the page。 You're not on a link or anything， just somewhere in
    the middle there。
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 所以请点击这个链接。这将带你到宾夕法尼亚的维基页面。结果证明，你可以调查和检查所有为这个页面提供支持的 HTML。所以我希望你去页面的中间部分。不要在链接上，只是在中间的某个地方。
- en: And right click and you should see this option to inspect。 So if you're not
    using Chrome。 you'll probably see something like inspect element。 If you are using
    something like Safari。 you might have to actually update your system， preferences
    to allow you to see this。 You basically just have to enable developer commands
    to be shown up in your browsing。
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 右键点击，你应该会看到这个检查选项。如果你不是在使用 Chrome，可能会看到类似“检查元素”的东西。如果你使用的是 Safari，你可能需要更新系统偏好设置，以允许你看到这个。你基本上只需要启用开发者命令以在浏览中显示。
- en: So if you're using Safari， definitely take a pause here and go check out how
    you could， do that。 Because what you'll want to do is either inspect or inspect
    element， click on that。 And what will pop up over here in the sidebar or maybe
    in the bottom bar if you're using。 Firefox or something like that？ Now you can
    actually see all of the HTML that powers this page。
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 所以如果你使用 Safari，确实要暂停一下，去检查一下你该怎么做。因为你想要做的就是检查或者检查元素，点击那个选项。在侧边栏或者底部栏中会弹出一些内容，如果你在使用
    Firefox 或类似的浏览器，你现在可以看到支撑这个页面的所有 HTML。
- en: And wow， already the highlighting。 Even more than that。 you can actually figure
    out what piece of HTML goes with what piece， of the page。 So if you just kind
    of scroll through this HTML， your browser will highlight what part。 of the page
    is being controlled by that code。 So for example， let's say this right here。
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 哇，已经有高亮显示了。甚至更多，你实际上可以弄清楚哪个 HTML 片段对应于页面的哪个部分。所以如果你只是稍微滚动一下这个 HTML，你的浏览器将高亮显示由那段代码控制的页面部分。例如，就像这里这样。
- en: this H1 tag is the very top heading and it， has the word Pennsylvania， the name
    of the article。 So have fun with this。 Play around。 Look around here a little
    bit。 This is what we're going to be requesting using the package called requests。
    And then we're actually going to be parsing this HTML with beautiful soup。 Okay。
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 这个 H1 标签是最顶部的标题，包含了“宾夕法尼亚”这个词，文章的名称。所以好好玩玩这个。四处逛逛。这就是我们将要使用名为 requests 的包进行请求的内容。然后我们将用
    beautiful soup 来解析这个 HTML。好的。
- en: let's go back over to the code。 So it's pretty amazing。 All the things that
    we've learned so far with these really basic sort of fake web pages。 that I just
    made up are now going to be applied to actual web pages。 We'll actually be able
    to pull that HTML code into this Google Colab notebook and actually。
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回到代码那边。真是令人惊叹。到目前为止，我们从这些非常基本的虚假网页上学到的所有知识，现在将应用于实际网页。我们将能够将 HTML 代码提取到这个
    Google Colab 笔记本中，并且实际上。
- en: look for various bits of information。 So we are going to be scraping Wikipedia
    for this tutorial。 I do want to preface this with if you are going to do a large
    scale project with Wikipedia。 like really large scale， you probably just want
    to download this information。 You probably do not want to actually scrape Wikipedia。
    This is really just for educational purposes。
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 查找各种信息。所以我们将在这个教程中抓取维基百科。我想在这里预先说明，如果你打算进行一个大规模的维基百科项目，比如说真的很大规模，你可能只是想下载这些信息。你可能不想真正地抓取维基百科。这仅仅是为了教育目的。
- en: And you can follow this link I have for you here to learn more about how you
    can just。 download this information。 You don't actually have to scrape it。 But
    for us， we're just learning。 So we're just going to take advantage of the really
    nice structure that Wikipedia has。 And we're going to be scraping just a couple
    of pages here。 Okay。
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以跟随我为你提供的这个链接，了解如何下载这些信息。你其实不需要抓取它。但是对我们来说，我们只是学习。所以我们将利用维基百科的良好结构。我们只会抓取几页内容。好的。
- en: So I've been saying all along that beautiful soup does not actually retrieve
    information。 from the internet。 For that， we actually have to use another package
    called requests。 So let's go ahead and import this package。 Request is pretty
    simple in nature。 There's really only one command we need from requests to get
    the things that we'd like。
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 我一直在说beautiful soup实际上并不从互联网上检索信息。为此，我们需要使用另一个包，称为requests。让我们导入这个包。requests本质上很简单。我们只需要从requests中使用一个命令来获取我们想要的内容。
- en: So all we have to do is whatever page we're interested in retrieving the HTML
    from， we'll。 save that in some variable。 So here is the URL that links us to that
    Pennsylvania web page。 Once we have that， we're just going to say requests。get
    and then the URL。 So what comes back from that will store in this variable called
    response。
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们要做的就是从我们感兴趣的页面中获取HTML，将其保存到某个变量中。这是链接到宾夕法尼亚网页的URL。一旦我们有了这个，我们就会说requests.get，然后是URL。返回的内容将存储在这个名为response的变量中。
- en: So response actually gives us a couple of different things。 Let's first look
    at what response。status code gives us。 So response。status code basically tells
    you if your request was successful or not。 These are the various codes that you'll
    sometimes see if you launch a page that doesn't exist。 You might get a 404。 This
    is the same set of codes。 But a 200 is actually successful codes。
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 所以response实际上给我们提供了几个不同的内容。让我们首先看看response.status code给我们的信息。response.status
    code基本上告诉你请求是否成功。如果你访问一个不存在的页面，可能会得到404。这是一组相同的代码，但200表示成功的代码。
- en: That means our request went through and we were able to retrieve the HTML。 If
    you want to know more about those codes， you can follow this link。 But the part
    that we actually probably want to know about a little bit more is what was。 the
    HTML that we got back by using requests。 And that's going to be stored under this
    text property。
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着我们的请求成功了，我们能够检索到HTML。如果你想了解更多关于那些代码的信息，可以跟随这个链接。但我们实际上想更多了解的是通过使用requests获得的HTML。这将存储在这个text属性下。
- en: So let me just print out what the first 200 characters of text is going to be。
    Cool。 Is it starting to look a little bit like the HTML code that we've been studying
    this whole， time？
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 让我打印出文本的前200个字符。酷，是不是开始看起来有点像我们一直在研究的HTML代码？
- en: Yeah。 We have HTML here and then we have a head。 Eventually we would see a body
    and various different tags on the page。 So this is a string。 This is our HTML
    string that creates the Pennsylvania webpage。 Let's go ahead and save this HTML
    into this variable called page。 And just to reiterate。 now that we've requested
    this information， what we got back from 。text， is just a string。
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 是的。我们这里有HTML，然后我们有一个head。最终我们会看到body和页面上的各种标签。所以这是一个字符串。这是创建宾夕法尼亚网页的HTML字符串。让我们把这个HTML保存到一个名为page的变量中。再重申一下，现在我们请求了这个信息，从.text中得到的只是一个字符串。
- en: So it's a really， really long string， but it is just a string。 All right。 So
    now that we have that HTML string loaded into Google Colab， the natural thing
    would。 be to go ahead and parse it with beautiful soup。 So that's just kind of
    the workflow。 You'll see a webpage that you're interested in， some kind of great
    information that you'd。
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这是一个非常非常长的字符串，但它就是一个字符串。好的。现在我们已经在 Google Colab 中加载了那个 HTML 字符串，自然的做法是使用 Beautiful
    Soup 进行解析。这就是工作流程。你会看到一个你感兴趣的网页，一些你想要的伟大信息。
- en: like to collect。 You will use requests to pull that HTML code into your Jupyter
    notebook or your Google。 Colab or your Python script。 Then you'll use beautiful
    soup to make sense of that HTML and to learn the structure about。 where all the
    tags are and， you know， what the DOM looks like and what kind of attributes， we
    see。 those kinds of things。 So let's go ahead and we do have to re-import beautiful
    soup。
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 我喜欢收集。你将使用请求将 HTML 代码提取到你的 Jupyter notebook、Google Colab 或 Python 脚本中。然后你会使用
    Beautiful Soup 来理解这些 HTML 内容，学习结构，了解所有标签的位置，以及 DOM 的样子和我们所见的各种属性，这些内容。我们现在继续，需要重新导入
    Beautiful Soup。
- en: Remember we were on a separate notebook the last time we used it。 But we're
    going to do the same thing。 We're going to parse this HTML string with beautiful
    soup and we'll save it under soup。 So commands that we learned in the previous
    section still work， even though we got this。 HTML from the internet。 If I do something
    like soup。findh1。
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 记得我们上次使用它时是在一个单独的 notebook 中。但我们将做同样的事情。我们将使用 Beautiful Soup 来解析这个 HTML 字符串，并将其保存在
    soup 下。因此，我们在前一部分学到的命令仍然有效，即使我们从互联网获得了这个 HTML。如果我做类似于 soup.findh1 的事情。
- en: that's going to be that header that we're just inspecting， on the webpage。 Pennsylvania。
    that big name of the article that appears at the top of the page。 And if I want
    to just look at this inner HTML text， it's still the same thing， just。text。 Great。
    So this is kind of the game you're going to be playing。 You'll find some information
    on a webpage。
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 那就是我们在网页上检查的那个标题，宾夕法尼亚州。那篇文章在页面顶部显示的大名字。如果我想查看这个内部 HTML 文本，它仍然是同样的东西，只是.text。很好。这就是你要玩的游戏。你将在网页上找到一些信息。
- en: You will click it to inspect what that code looks like。 You'll try to figure
    out a strategy of how you could extract that one piece of text or。 that one link
    from the HTML。 And then you'll use beautiful soup to find that information。 Let's
    try an example together and then I'll have you do an example on your own。 Okay，
    so as a class。
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 你会点击它检查那段代码的样子。你会尝试制定一个策略，看看如何从 HTML 中提取那一段文本或那一个链接。然后你将使用 Beautiful Soup 找到那些信息。让我们一起尝试一个例子，然后我会让你自己做一个例子。好吧，作为一个班级。
- en: as a tutorial class here， let's try to find the disambiguation link。 So Wikipedia
    often has these links in the upper part of the page that says， if this isn't。
    the Pennsylvania you are looking for， go check this list of other possible Pennsylvanias。
    So let's go back over to the Pennsylvania tab。 There we go。
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个教程类，让我们尝试找到消歧义链接。因此，维基百科通常在页面的上部有这些链接，说明如果这不是你正在寻找的宾夕法尼亚州，请查看其他可能的宾夕法尼亚州列表。所以让我们回到宾夕法尼亚州的标签。我们到了。
- en: And you'll see the disambiguation link is right here， Pennsylvania。 If I wanted
    to collect that link， the text there and that link， what I'm going to do here。
    is come up with a strategy。 What do I need to look for with beautiful soup in
    order to collect that link？
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 你会看到消歧义链接就在这里，宾夕法尼亚州。如果我想收集那个链接，文本和链接，我要做的就是制定一个策略。我需要在 Beautiful Soup 中寻找什么来收集那个链接？
- en: And the way I can do this is just right click on， let's actually do it over
    top and then。 right click here， over the thing that I actually want to know about。
    And then I will inspect just this one element。 This is a really nice tip。 Instead
    of scrolling through all of that HTML， it's a lengthy page。
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 我能做到这一点的方法是右键单击，让我们实际上在顶部进行，然后在我真正想了解的元素上右键单击。接着我将仅检查这一元素。这是一个很好的小技巧。不需要浏览所有
    HTML，页面很长。
- en: By right clicking exactly the part that you want to know about and then clicking
    inspect。 that should take you to exactly the portion of code that produces what
    you see in the， browser。 So we can see this is an anchor tag， which we might expect
    since it is a link。 And we're looking through this to see， is there anything unique
    about this disambiguation。
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 通过右键点击你想了解的部分，然后点击检查。这样应该可以让你直接到达产生浏览器中所见部分的代码。我们可以看到这是一个锚标签，我们可能会期待，因为它是一个链接。我们正在查看这一点，看看这个消歧义是否有什么独特之处。
- en: link that we might exploit with beautiful soup？ And by that I mean there's going
    to be tons of anchor tags throughout this whole page。 Is there anything special
    about this one particular anchor tag that I might look for？ So what I see。 we
    definitely have an Href， great， all the anchor tags are going to have， the Href，
    fine。 And we have this class right here， class， Mw-disambig。
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可能会利用的链接与beautiful soup？我的意思是，整个页面中会有大量的锚标签。这个特定的锚标签有什么特别之处吗？所以我看到的。我们确实有一个Href，太好了，所有的锚标签都会有Href，没问题。还有这个类，类，Mw-disambig。
- en: So that's something that might be specific to this one link。 If I go back to
    beautiful soup and look for the class， Mw/disambig， perhaps I can pull。 out this
    disambiguation link。 Let's try it。 Okay， so we're trying to extract a link。 First
    off。 let's just try the naive approach， maybe it'll work。 Soup。find， right？ Might
    as well try it。
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能是这个链接特有的内容。如果我回到beautiful soup并寻找类，Mw/disambig，也许我可以提取出这个消歧义链接。我们试试。好的，我们正在尝试提取一个链接。首先，先尝试一下简单的方法，也许能奏效。Soup.find，对吧？不妨试试。
- en: see what happens。 So find is just going to return us one element。 but perhaps
    this is the first or only element， on the page that has that class。 So we're finding
    something by class and it's Mw-disambig。 Let's just see what happens。 Awesome。
    So that is the exact element we were looking for。 And if I want to extract just
    the link。
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 看看会发生什么。因此，find只会返回我们一个元素。但也许这是页面上唯一或第一个具有该类的元素。所以我们通过类找到了一些东西，它是Mw-disambig。我们来看看会发生什么。太棒了。这正是我们要找的元素。如果我想提取这个链接。
- en: all I have to do now is do soup。find， class_equals， Mw-disambig。 And now I'm
    just looking for the link， so I can reference this as in I want the Href attribute。
    And there we go。 So one interesting thing， and I think this is a good spot to
    bring this up。 oftentimes， websites instead of having， if the website is linking
    to something internal， instead。
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 我现在要做的就是soup.find，class_equals，Mw-disambig。现在我只是在找链接，所以我可以将其作为我想要Href属性的引用。好了。那么一个有趣的事情，我认为这是个好时机提到这一点。通常，网站在链接到某个内部内容时，。
- en: of having the full en。wicapedia。org/wickey/bobaba， they'll just have the part
    kind of like a。 path within their own site。 So they won't have the first part
    about Wikipedia。org because that's implied since it is on the， same website。 So
    we'll do something special whenever we try to follow these links with requests。
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 并不是使用完整的en.wikipedia.org/wickey/bobaba，而只是保留类似于路径的部分。在他们自己的网站内。所以他们不会提到关于Wikipedia.org的第一部分，因为那是隐含的，因为它在同一个网站上。因此，每当我们尝试通过requests跟随这些链接时，我们会做一些特殊的处理。
- en: We'll have to tack on a little bit extra in the beginning to make sure that
    we're actually。 following a true URL， but more on that later。 Okay， for now， this
    is exactly what we wanted。 All right。 So your exercise， your first exercise for
    this notebook is to extract the latitude and。 longitude for Pennsylvania。 So if
    you look in this upper right-hand corner of the Pennsylvania page。
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要在开头加一点额外的内容，以确保我们实际上是在跟随一个真实的URL，但稍后会详细说明。好的，目前为止，这正是我们想要的。好的。那么，你的练习，你在这个笔记本上的第一个练习是提取宾夕法尼亚的纬度和经度。所以如果你看一下宾夕法尼亚页面的右上角。
- en: you'll see that， we have a set of coordinates。 And what you're going to try
    to do is actually come up with a strategy for extracting those。 coordinates with
    beautiful soup。 So go ahead and pause here if you'd like。 I'm about to go through
    the solution。 Okay， so as the hint says here。 right-click on the coordinates directly
    and then select， inspect or inspect element。
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 你会看到，我们有一组坐标。你要做的是为提取这些坐标制定一个策略，使用beautiful soup。如果你愿意，可以在这里暂停。我将要介绍解决方案。好的，正如提示所说。右键点击坐标，然后选择检查或检查元素。
- en: So let's go back over to the webpage。 Now I'm interested in collecting the lat
    long for Pennsylvania。 So I'm going to right-click directly here， click inspect，
    and that will take me to the。 exact piece of HTML code that produces this latitude
    and longitude。 Great。 And so this one you can see is actually a span， a little
    bit different tag than we've seen， so far。
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 所以让我们回到网页上。现在我对收集宾夕法尼亚的经纬度感兴趣。因此我将直接右键单击这里，点击检查，这将带我到生成这个经纬度的确切HTML代码。太好了。你可以看到这是一个跨度，与我们之前看到的标签略有不同。
- en: But the unique thing about this span is that it has been tagged with this class
    equals。 geo dash deck。 So let's try to look for class equals geo dash deck and
    see if we find this latitude and。 longitude。 Okay。 So soup dot find。 And we're
    looking for a class again。 Geo dash deck。 So it turns out the text for this is
    living right here。 And we can just now do dot text。 Great。
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 但是这个跨度的独特之处在于它被标记为这个类等于geo-dash-deck。所以我们来尝试查找类等于geo-dash-deck，看看我们是否找到这个经纬度。好的。所以
    soup.dot.find。我们再次查找一个类。geo-dash-deck。结果是这个文本正好在这里。现在我们可以做.dot.text。太好了。
- en: By the way， one more thing that I do want to mention。 You can actually mix strategies。
    So if you're looking for a certain class and a certain tag or something like that
    or certain。 attributes and a tag， you can do both with the same query。 So for
    this example。 I'm looking for a span that has the class geo dash deck。 If I want
    to be very specific。
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 顺便说一句，我想提到的另一件事是，你实际上可以混合策略。因此，如果你在寻找某个类和某个标签，或者类似的东西，或者标签中的某些属性，你可以用相同的查询同时做到这两点。因此在这个例子中，我在寻找一个具有类geo-dash-deck的跨度。如果我想非常具体。
- en: I can do that。 And this also totally works。 Sometimes you possibly could have
    a class that applies to multiple kinds of tags。 So this is one way to be a little
    bit more specific。 Okay。 Cool。 So now we're going to dive in a little bit further。
    What else can we do with beautiful soup？
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 我可以做到这一点。这也完全可行。有时你可能会有一个类适用于多种标签。因此，这是一个更具体的方式。好的，酷。那么现在我们将深入一点。我们还可以用美丽汤做些什么？
- en: What are the tricks of the trade here？ We definitely already learned about find
    and find all。 We've been using those a lot。 But one thing I really want to bring
    up again is how you can chain methods together。 So you can take the results of
    a find， then find all， then maybe cycle through all of。 those and do another find。
    I mean， you can keep going as long as you like with these。
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的行业技巧是什么？我们肯定已经学习了查找和查找所有。我们已经频繁使用这些。但是我真的想再次提到的是如何将方法链在一起。因此，你可以先获取一个查找的结果，然后查找所有，然后可能循环遍历所有这些，再做另一个查找。我的意思是，只要你喜欢，就可以继续下去。
- en: The results of find and find all are still going to be beautiful soup objects
    and they。 are still queryable。 So let's try one example of that。 There is a big
    table。 Let me show you on the page。 There is this huge table that's on the side。
    This is the first table that's included in this webpage is all this Pennsylvania
    information。
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 查找和查找所有的结果仍然是美丽汤对象，并且它们仍然可以查询。那么我们来尝试一个例子。这里有一个大表格。让我在页面上给你展示。这里有一个巨大的表格在侧边。这是这个网页中包含的第一个表格，包含了所有的宾夕法尼亚信息。
- en: We have state， Commonwealth of Pennsylvania。 We have nicknames and mottos and
    all kinds of different stuff。 So you can see how this table， this one table might
    be really ripe for web scraping because。 this table is actually going to repeat
    on other states pages and we might be able to。 exploit the structure of this webpage
    and collect information about Pennsylvania through。
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有州，宾夕法尼亚联邦。我们有昵称、座右铭和各种不同的东西。所以你可以看到这个表格，这个表格可能非常适合网络爬虫，因为这个表格实际上将在其他州的页面上重复，我们可能能够利用这个网页的结构，通过这个表格收集有关宾夕法尼亚的信息。
- en: just this one table。 So let's go back and try to try to look for something within
    the table。 So just to show you， if I just try to find a table， there are multiple
    tables on this。 page but that one happens to be the very first one。 So using a
    find， we'll bring back。 this is quite long actually， we'll bring back the full，
    here we go。
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 仅仅通过这个表格。因此，让我们回去，尝试在表格内寻找一些东西。为了演示，如果我只尝试查找一个表格，这个页面上有多个表格，但那个恰好是第一个。所以使用查找，我们会返回。其实这相当长，我们将返回完整的，来吧。
- en: the table that we were just looking at。 By the way， this predify。 you might
    want to use this sometimes。 If you are looking at a lot of HTML code。 it basically
    just does nice indentations so， that you'll see one tag on each line。 So that's
    helpful too。 Alright so we know that that is the very first table that one big
    column on the right。
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚查看的那个表格。顺便提一下，这个预处理工具你可能有时会用到。如果你在查看大量 HTML 代码，它基本上会进行漂亮的缩进，让你看到每个标签在单独一行上。这也很有帮助。好的，我们知道这是第一个表格，右边有一个大列。
- en: By the way， this is a table， table element but it is still a beautiful soup
    element tag。 It is a gigantic one but yes， it is still a table tag that wraps
    around all of this。 Okay。 so it turns out that tables are often composed of things
    like headers， data elements， and rows。 So when you start seeing these like TH
    that's table header， TD， table data， TR， table row。
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 顺便提一下，这是一个表格，表格元素，但它仍然是一个美丽的汤元素标签。它是一个巨大的表格，但确实，它仍然是一个包裹这一切的表格标签。好的。结果表格通常由标题、数据元素和行组成。所以当你开始看到这些像
    TH 的东西时，那就是表头，TD 是表数据，TR 是表行。
- en: So you can see these patterns show up a lot， a lot of times you'll want to collect
    things。 from tables。 So I did what I mentioned that here。 If we try to extract
    the very first TH element within this main table。 what we find is Pennsylvania。
    So if you remember from that big table， the very top of it。 it said Pennsylvania
    and that's， what we're extracting here。
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 所以你会看到这些模式经常出现，很多时候你会想从表格中收集东西。所以我在这里做了我提到的。如果我们尝试提取这个主表格中的第一个 TH 元素，发现的是宾夕法尼亚。所以如果你记得那个大表格，它的最顶部写着宾夕法尼亚，而这就是我们在提取的内容。
- en: So I'm basically just querying only that table， only looking for the first TH
    element and then。 returning the text， the data element， the first TH element we
    have here a TV is actually going。 to be the next line which said state。 So I certainly
    could save the elements I'm finding。 I could save them in variables and then continue
    working with them。
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我基本上只是查询那个表格，只查看第一个 TH 元素，然后返回文本，数据元素，第一个 TH 元素在这里是一个 TV，实际上将是下一行，写着州。所以我当然可以保存我找到的元素。我可以将它们保存在变量中，然后继续处理。
- en: But you can definitely also just chain commands together if that's more your
    style。 So what's happening here is I'm going to look through the entire webpage，
    the entire soup。 I'll find the first table。 I'll take that result and pipe it
    into this next command which is looking for the table。 header of that one particular
    table。 Okay， and then pulling the text which should say Pennsylvania。
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 但你绝对也可以根据自己的风格将命令串联在一起。所以这里发生的事情是，我将查看整个网页，整个 soup。我会找到第一个表格。然后把这个结果传递到下一个命令中，它是在查找那个特定表格的表头。好的，然后提取文本，应该是宾夕法尼亚。
- en: Okay， so this is also totally valid。 You can either save each step as its own
    Python variable or you can just continue piping information。 to the next， find
    or find all。 Also as I mentioned。 so there's going to be multiple different table
    rows in that table。 So let's take this first main table and find all of the table
    rows。
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，所以这也是完全有效的。你可以将每个步骤保存为自己的 Python 变量，或者你可以继续将信息传递到下一个查找或查找所有。正如我提到的，那个表格中会有多个不同的表行。所以让我们先查看这个主表格，并找到所有的表行。
- en: And I'm going to be looping over those and just printing out the text。 I'm actually
    just going to take the first 10 because those are pretty big table。 So now you
    start seeing we're gathering all the different information from the Pennsylvania，
    table。 We already have the nicknames， the anthem， these kinds of things。
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 我将遍历这些内容并打印出文本。实际上我只会取前10个，因为那些表格相当大。所以现在你开始看到我们正在收集宾夕法尼亚表格中的所有不同信息。我们已经有了昵称、国歌等这些内容。
- en: This is where really pulling out data that's interesting to you can be a strategy
    here。 It's definitely what we're trying to do。 All right， here's just one， you
    know， crazy example。 You could really chain as many of these together as you want。
    This is saying， okay。 the whole web page， whole web page， I want to find the first
    division， that has the ID content。
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 这里真正提取你感兴趣的数据可以是一种策略。这确实是我们想要做的。好吧，这是一个疯狂的例子。你可以将这些连在一起，任意连接。这表示，好的，整个网页，整个网页，我想找到第一个
    ID 为 content 的分区。
- en: which only me one。 Then within that content， I'm going to look for the body
    content。 Then I'm going to look for the Mw-content-text and I'm going to look
    for a div that has an。 attribute called role and its attribute value should be
    note。 Go ahead and give me the text of all of that。 Okay， we're back to the disambiguation
    link。
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 仅仅是我一个人。然后在那段内容中，我要寻找主体内容。接着我会寻找 Mw-content-text，并且我会寻找一个有属性叫 role 的 div，它的属性值应该是
    note。去吧，给我所有的文本。好的，我们回到了消歧义链接。
- en: So sometimes there are better strategies than others， right？
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 所以有时候有比其他策略更好的策略，对吧？
- en: So there are often multiple ways to do web scraping or multiple ways to find
    the information。 you want。 So I would definitely say some strategies are better
    than others。 Okay。 so that's chaining methods together， chaining those different
    commands together。 Let's also talk about something that we're going to have to
    go back to the HTML DOM that。
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 所以往往有多种方式进行网页抓取或找到你想要的信息。我绝对会说，有些策略比其他策略更好。好的，所以这是将不同的方法链在一起，将那些不同的命令串联起来。我们还要讨论一些需要回到
    HTML DOM 的事情。
- en: we were talking about before。 Instead of locating something by attribute， you
    know。 name or by tag or by something like， that， sometimes the information you
    want to find is not really tagged with anything。 It doesn't have attributes。 It
    doesn't have， you know， any， you know， unique identifiers， right？
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前在讨论的事情。与其通过属性，比如名称或标签之类的来定位，有时候你想找到的信息并没有真正的标签。它没有属性，也没有任何独特的标识符，对吧？
- en: There's nothing really special about that piece of the HTML code。 If you ever
    come across that。 there's a couple of things that you can do to still find the，
    information you want。 It just might take a different strategy。 So let's try this。
    So I just showed you how the very first table in the Pennsylvania Wikipedia page
    is that， you know。
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 这段 HTML 代码并没有什么特别之处。如果你遇到它，有几件事情你可以做来仍然找到你想要的信息。只是可能需要不同的策略。让我们试试这个。所以我刚刚向你展示了宾夕法尼亚维基百科页面上的第一个表格，那个，你知道的。
- en: big， long thing on the right。 And we can find all of the table data。 Let me
    actually show you again what that's going to look like。 Here's the first 10 of
    those。 So lots of different table data elements here。 If I actually just look
    at the 10th one here。 what I would find is this date。 This is actually the date
    that Pennsylvania was admitted to the union。
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 右边那个很长的东西。我们可以找到所有的表格数据。让我再给你展示一下这看起来会是什么样子。这是前十个表格数据元素。所以这里有很多不同的表格数据元素。如果我实际上只看第十个，我发现的是这个日期。这实际上是宾夕法尼亚州被接纳入联邦的日期。
- en: So one strategy for finding different information that you're interested in。
    If I wanted to extract this， I can basically find all of the table data and count
    over。 what's my index number， right？ And I realized， okay， it's index number nine，
    bring back the date。 Totally fine。 But what happens if someone edits that Wikipedia
    page and pushes everything down by one element。
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 所以找到你感兴趣的不同信息的一个策略是。如果我想提取这些，我基本上可以找到所有的表格数据并统计我的索引号，对吧？然后我意识到，好的，索引号是九，带回来日期。完全没问题。但是如果有人编辑了那个维基百科页面，把所有内容向下推了一位，那会发生什么呢？
- en: right？ Strategies like this where you're locating information by index number，
    you know， can。 be fine in the short term。 But if you're building code， you know。
    that I use today and I use in a month or two or， three。 this can really kind of
    have issues because if something gets edited and gets pushed， off。
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 对吧？像这样通过索引号定位信息的策略，短期内是可以的。但是如果你在构建代码，今天使用，几个月后或者三个月后使用，这可能会带来问题，因为如果某些内容被编辑并被推走。
- en: even by one， you know， I would get totally different information if I was looking
    at something。 in the eighth position。 Yeah， this is not the date and neither is
    whatever is in the 10th position。 right？ That's the capital， right？ So locating
    stuff by index number does work。 Sometimes that's what you'll just need to do。
    But there are other strategies。
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 即使只是向下推一位，如果我在第八个位置查看，我将得到完全不同的信息。是的，这不是日期，第十个位置的也不是。不对吧？那是首都，对吧？所以通过索引号定位确实可行。有时你就是需要这么做。但是还有其他策略。
- en: What you can do instead is actually locate information by text。 So let me go
    back to the webpage。 actually， and show you what we're doing here。 Scroll down
    a little bit。 Here's our date。 December 12， 1787。 If I look to the left of this
    date。 I see that the date is labeled as admitted to union， to the union。
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以做的是通过文本定位信息。所以让我回到网页上，实际上，给你展示我们在做什么。向下滚动一点。这是我们的日期，1787年12月12日。如果我看一下这个日期的左侧，我看到这个日期被标记为“admitted
    to the union”。
- en: So maybe what I can do is just search through the page and look for this phrase
    admitted。 to the union and then collect the date because it is beside of this
    text。 So that's going to be the strategy here that we're going to try to apply。
    Look for the text and then look for the date that should live beside the text。
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 所以也许我可以做的是在页面上搜索这个短语“admitted to the union”，然后收集日期，因为它就在这个文本旁边。所以这将是我们要尝试应用的策略。查找文本，然后查找应该与文本相邻的日期。
- en: So a word of caution on the sort of text matching approach。 If you are using
    this soup。find text equals， that has to be an exact match。 We can't just use admitted。
    We get nothing back。 It didn't match anything on our webpage。 We have to actually
    provide the full admitted to the union。 And now it has matched and it tells us，
    yes， I did find this exact phrase as one of the。
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 所以对文本匹配方法有一点警告。如果你使用这个方法，`find text equals`，必须是完全匹配。我们不能只用“admitted”。我们什么也得不到。它在我们的网页上没有匹配到任何内容。我们必须实际提供完整的“admitted
    to the union”。现在它匹配了，告诉我们，是的，我找到了这个确切的短语作为其中之一。
- en: text elements on the webpage。 So having this exact match is kind of a deal breaker
    sometimes or it is a source of stress。 because what can happen with that one？
    It wasn't so bad。 It just says admitted to the union。 no problem。 Sometimes there
    are non-printing characters。 things that you can't see but are in the HTML， code。
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 网页上的文本元素。所以拥有这个确切的匹配有时会是一个决定性的因素，或者是压力的来源。因为那样会发生什么？情况并不太糟糕。它只是说“admitted to
    the union”，没问题。有时会有非打印字符，一些你看不见但在HTML代码中的东西。
- en: like weird spacing or maybe there's a bullet or something like that。 So every
    once in a while。 even this sort of really specific exactly what you see on the，
    webpage doesn't always work。 So if it turns out that this was not the exact phrase
    that's used in the code， this would。 have returned nothing。 So you might try to
    use regular expressions instead。
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 像奇怪的空格，或者可能有一个项目符号之类的。所以偶尔，即使是这种非常具体的网页上看到的内容也不一定有效。所以如果发现这不是代码中使用的确切短语，这将不会返回任何内容。因此你可能需要尝试使用正则表达式。
- en: So if you're not super well versed with regular expressions， no problem。 You
    don't have to know too much about them for this tutorial。 But I would definitely
    recommend getting more familiar with them。 In web scraping。 regular expressions
    turn out to be really， really handy， really useful。
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 所以如果你对正则表达式不是很熟悉，没问题。对于这个教程，你不需要了解太多。但是我确实建议你更熟悉它们。在网页抓取中，正则表达式确实非常方便，非常有用。
- en: Especially just really basic red jacks。 For example here。 I'm just compiling
    down this really simple pattern which is the word， admitted。 And I can match on
    this red jacks pattern which is called admitted red jacks。 Okay。 There we go。
    great。 So if you are using red jacks like this， you don't necessarily have to
    have the full text。
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 尤其是非常基本的红色标记。例如这里。我正在编译这个非常简单的模式，就是单词“admitted”。我可以在这个叫做“admitted red jacks”的红色标记模式上进行匹配。好的，太好了。所以如果你像这样使用红色标记，你不一定需要完整的文本。
- en: to match。 It will just match that red jacks pattern and then give you back this
    admitted to the union。 So sometimes this is a little bit easier to do is to work
    with red jacks。 Especially if you just can compile down one or two words that
    you do see in there that。 should work too。 So this may not look too exciting。
    We have this admitted to the union。
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 进行匹配。它将只匹配这个红色标记模式，然后给你返回“admitted to the union”。所以有时使用红色标记工作会更容易，特别是如果你只编译一个或两个你能看到的词，这也应该有效。所以这看起来可能不是很令人兴奋。我们有这个“admitted
    to the union”。
- en: Is it a string？ I mean， it just matched some text on my page。 What is this useful
    for？
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个字符串吗？我的意思是，它只是匹配了我页面上的一些文本。这有什么用呢？
- en: If I actually save this as this admitted variable and I check the type， this
    is not actually。 just a string。 It is still a beautiful soup element。 It is an
    element called a navigable string。 And what that means is beautiful soup still
    knows what this string lives next to and how。 the string is positioned in the
    DOM。 So what we are actually going to do。
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我实际上把这个保存为这个被接受的变量，然后检查类型，这其实并不是仅仅一个字符串。它仍然是一个Beautiful Soup元素。它是一个称为可导航字符串的元素。这意味着Beautiful
    Soup仍然知道这个字符串旁边是什么，以及字符串在DOM中的位置。所以我们实际上要做的是。
- en: remember that admitted to the union was in the table。 and just next to it was
    the date that we actually care about。 So let's try to use this navigable string
    to just look for what is the next thing that。 you come across。 It is our date。
    Okay？ So this is a really cool strategy。
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 记住被接纳到联盟的日期在表中。就在旁边的是我们实际关心的日期。所以我们来试试使用这个可导航字符串来看看你遇到的下一个东西。这是我们的日期。好吗？所以这是一个非常酷的策略。
- en: Sometimes you can find a piece of text that is just next to the thing you are
    trying to， pull out。 And if you can find an navigable string like this， you can
    just look to the next element。 and there will be your data。 And of course， this
    is still a TV tag。 So if we want just the date string， we could do dot text。 Okay。
    Cool。
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 有时你可以找到一个文本，它正好在你想提取的东西旁边。如果你能找到这样的可导航字符串，你可以查看下一个元素，那就是你的数据。当然，这仍然是一个TV标签。所以如果我们只想要日期字符串，我们可以用`.text`。好的，酷。
- en: So there is lots more you can do about position。 It doesn't have to just be
    the next element in the code。 You could do things like look for the parent element。
    So remember the HTML DOM。 you could look to see of this navigable string， where
    did this， guy come from？ Who is its parent？
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 关于位置你可以做的事情还有很多。它不一定只是代码中的下一个元素。你可以做的事情包括查找父元素。所以记住HTML DOM。你可以查看这个可导航字符串，这个家伙来自哪里？他的父元素是谁？
- en: That works too。 And you'll see admitted to the union is the text of this tag。
    Okay。 And we can continue chaining these on just to show you one more example。
    We could say。 what's the parent of the parent？ Etc。 Okay。 So keep these in mind。
    you can look for parents the next level up。 You could look for children the next
    level down。
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 这也可以工作。你会看到被接纳到联盟是这个标签的文本。好的。我们可以继续链式调用这些，仅仅是再给你一个例子。我们可以说，父元素的父元素是什么？等等。好的，所以记住这些，你可以寻找上一级的父元素。你也可以寻找下一级的子元素。
- en: You could look for siblings as in we both share a parent or the previous sibling
    as in， you know。 we share a parent and you were performing in the code， etc。 Okay。
    So a couple of exercises for you。 You're going to be using similar strategies
    of text matching to try to find the capital。 of Pennsylvania。 So try not to use
    just the index position number or anything like that。
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以寻找兄弟元素，就像我们共享一个父元素，或者寻找前一个兄弟元素，就像，你知道的。我们共享一个父元素，而你在代码中执行，等等。好的，给你几个练习。你将使用类似的文本匹配策略来尝试找到宾夕法尼亚州的首府。所以尽量不要仅仅使用索引位置号码或类似的东西。
- en: Try to use this text matching approach to find a capital。 Then you're going
    to try to print out the text of the first three references at the very。 bottom
    of the page。 So come up with a strategy， inspect those references and see if you
    can figure out a。 strategy to extract those references and then print them out。
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试使用这个文本匹配方法来找到一个首府。然后你将尝试打印出页面底部的前三个引用的文本。所以想出一个策略，检查那些引用，看看你能否想出一个策略来提取那些引用，然后将它们打印出来。
- en: As an added bonus and this is admittedly a little bit more difficult， try to
    also print。 out those external links。 So anything that those references link to，
    print that out as well。 Okay。 So let's try the solutions here。 So if we look at
    the Pennsylvania webpage。 here we have a capital right here。 So the first thing
    I might try。
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 作为额外的奖励，这确实有点困难，试着打印出那些外部链接。所以任何那些引用链接到的东西，也打印出来。好的。我们来试试解决方案。所以如果我们查看宾夕法尼亚网页。这里我们有一个首府就在这里。所以我可能会尝试的第一件事。
- en: I'm trying to find Harrisburg as the capital of Pennsylvania。 There we go。 So
    what I'm going to do is say， okay， soup， find me text that looks like capital。
    Might as well try it。 This might work。 You never know。 We'll try something。 try
    the most basic thing and then if it doesn't work you can kind of， make it more
    complicated。
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 我正在尝试找到哈里斯堡作为宾夕法尼亚州的首府。好了。那么我将说，好吧，Soup，找到看起来像首府的文本。也许可以试试。这可能有效。你永远不知道。我们会尝试一些东西。先试最基本的东西，如果不行，你可以让它变得复杂一些。
- en: Awesome。 So it matched capital， which is an addable string。 And let's just check
    and see if that capital is the next thing。 Are we so lucky？ Yes， we are。 The next
    thing right after the capital is actually Harrisburg， which is what we want， it。
    So if we want to extract just the text， we could do dot text。 Great。 As far as
    the references。
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 太棒了。所以它匹配了capital，这是一个可添加的字符串。让我们检查一下看看这个capital是不是下一个。我们那么幸运吗？是的，我们是。紧接在capital后面的实际上是Harrisburg，这就是我们想要的。所以如果我们想提取文本，可以使用.dot
    text。太好了。至于引用。
- en: we are going to have to scroll to the bottom of the page。 So pardon my scroll。
    This is never easy here。 Let's try to use this guy。 No。 All right。 I'm just going
    for it。 Okay。 we're getting there。 There we go。 There we go。 Okay。 So here are
    those references。 So what I was getting at is can you just print out the text
    from these references？
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将不得不滚动到页面底部。所以请原谅我的滚动。这在这里从来都不容易。我们试试这个家伙。不。好的。我直接上了。好的。我们快到了。来了。好了。这里是那些引用。所以我想说的是你能否直接打印出这些引用的文本？
- en: So let's try to inspect this and see what we've got。 Okay。 So you might have
    tried something with the citations that might work。 We have reference text。 What
    else do you have？ What I'm looking for is maybe we have an overarching container。
    There we go。 Okay。 Now this ordered list that has class references。 Let's try
    it。 Let's see if that works。
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 所以让我们尝试检查一下这个，看看我们得到了什么。好的。你可能尝试过与引用相关的东西，可能会有效。我们有引用文本。还有什么呢？我在寻找的是也许我们有一个整体容器。来了。好的。现在这个有类references的有序列表。我们试试。看看是否有效。
- en: And within that， we are looking for， leave it is called site。 Yes。 This site
    text might do it for us。 Okay。 Let's try that。 So I'm looking for class references
    and then I'm going to be looking for site tags within。 whatever I find。 Okay。
    All right。 Let's try to find class references。 Did that work？ Oh boy。
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 而在那里面，我们要寻找，叫做site。是的。这个站点文本可能对我们有用。好的。我们试试。于是我在寻找类references，然后我将在里面寻找站点标签。无论我找到什么。好的。好的。让我们尝试找到类references。那有效吗？哦，天哪。
- en: Yes it did。 All right。 Great。 So that looks good。 That looks like it pulled
    back all those list items that are citations。 Right。 Now let's try to find all
    of the site tags。 I'll just pull right out。 How many did I want？
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，确实如此。好的。太好了。看起来这提取了所有的列表项，也就是引用。对吧？现在让我们试着找出所有的站点标签。我想要多少个？
- en: Three I think。 All right。 Let's try that。 Great。 Looking like I did find the
    references。 And now if I want to actually print out the text， since I just want
    to print that out。 let's actually just loop over this result set。 So let's go
    back over here for reference in all of these。 Print the reference。 That text。
    Great。 So something along these lines。 Oh this is the right thing。
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 我想是三个。好的。我们试试。太好了。看起来我找到了引用。现在如果我想实际打印文本，因为我只想打印这个。让我们实际遍历这个结果集。所以我们回到这里，引用所有这些。打印引用。那个文本。太好了。所以大致是这样的。哦，这就是正确的东西。
- en: Yes it is。 Perfect。 Symbols of Pennsylvania， elevation， et cetera。 So that'll
    work。 That'll do it。 Okay。 Great。 For an added bonus， can you also print out all
    of the external links from these three。 references？ I think for this one I'm actually
    going to save this the first three， since I'm only。 looking for links from the
    first three。 Let's actually save this as ref three or something like that。
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，确实如此。完美。宾夕法尼亚州的符号，高度等等。所以那会有效。那就行。好的。作为额外的奖励，你能否也打印出这三个引用中的所有外部链接？我想对于这个，我实际上会保存前三个，因为我只在找前三个的链接。我们实际将其保存为ref三之类的。
- en: All righty。 And then what I'm going to be doing。 So I need to loop through this
    result set。 Let's now just go for reference in ref three。 Since we are just looking
    to print these out。 let's now try to say for reference。find all， of the a text。
    Or link in。 Great。 Now we'll try to print out the link。text。 Let's see what happens。
    Oh did I want the。
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 好的。那么我将要做的事情是。我要遍历这个结果集。现在我们就用ref三。因为我们只是想打印这些。现在我们试着说for reference.find all，的a文本。或链接在。太好了。现在我们将尝试打印链接文本。看看会发生什么。哦，我想要的是。
- en: No I wanted the links。 Not the text。 Great。 Let's try href。 Awesome。 Okay。 So
    it looks like I have some external links and some internal links。 So if I want
    to be super specific about this， let's see if there's anything。 I think these
    external links are going to have a class。 It's called external。
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 不，我想要的是链接。不是文本。太好了。我们试试href。太棒了。好的。所以看起来我有一些外部链接和一些内部链接。如果我想具体一点，让我们看看有没有什么。我觉得这些外部链接会有一个类。叫做external。
- en: Let's make that modification。 So within this find all of the a tags， I'm actually
    also。 I'm only looking for those that have class equal to external。 Awesome。 So
    I think that did it。 We got all the links that link externally。 One thing that
    I did take advantage of there that I want you to be aware of。 Notice that this
    link that I found has external space text。
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们做那个修改。因此在这个查找所有的a标签中，我实际上也只是在寻找那些类等于外部的标签。太棒了。所以我想那样做到了。我们获得了所有指向外部的链接。我在那里利用的一件事是我希望你意识到的。注意到我找到的这个链接有外部空间文本。
- en: But yet I only provided external as the class I was looking for。 Whenever you
    see a space like that。 it means that this tag belongs to multiple classes。 So
    the space is signifying that I have a couple different classes。 So I really。 For
    this one I was just looking for the external class because I thought maybe that
    link to。
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 但我只提供了外部作为我所寻找的类。每当你看到这样的空格，它意味着这个标签属于多个类。因此空格表示我有几个不同的类。因此，对于这一点，我只是寻找外部类，因为我认为也许那个链接会。
- en: external links。 So you can take advantage of that。 You don't have to provide
    this full external space text。 You can just use external。 Okay。 All right。 So
    we're really starting to heat things up here。 Now we have methods for grabbing
    HTML from the web。 We know how to parse through it。 We know how to inspect。 What's
    all this for？ All right。 Ultimately。
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 外部链接。因此你可以利用这一点。你不必提供完整的外部空间文本。你可以只使用外部。好的。好的。所以我们真的开始加热了。现在我们有从网络抓取HTML的方法。我们知道如何解析它。我们知道如何检查。这一切都是为了什么？好的。最终。
- en: what all this web scraping is for is， you know， for data preparation。 We want
    to be able to take this data and now do something with it。 We want to either maybe
    aggregate the data and learn something about some kind of trend。 Maybe we want
    to visualize the data or it might serve as fuel for our machine learning， methods。
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 这一切网络抓取的目的在于数据准备。我们希望能够利用这些数据并进行某种操作。我们想要汇总数据并了解某种趋势，或者可能我们想要可视化数据，或者它可能成为我们机器学习方法的燃料。
- en: Maybe we're going to apply a model to it。 But ultimately I need to somehow make
    this data ready for analysis。 So there's a couple of things I have to do to take
    care of that。 The first one。 unfortunately enough， is a data cleaning issue。 So
    right now everything I'm pulling from， you know。 dot text or with the H-ref or
    any of， those things， they're all just a bunch of strings。 All right。
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 也许我们要对其应用模型。但最终我需要以某种方式让这些数据准备好进行分析。因此，我必须处理几件事情。第一个，不幸的是，是数据清理问题。所以现在我提取的所有内容，来自于，你知道的，点文本或H-ref或任何那些东西，它们都是一堆字符串。好的。
- en: Remember the HTML itself is a string。 So everything I'm extracting are also
    strings。 So I need to convert my numerical data into numbers， my date time values
    and the date time。 values and do things like that。 So this is a really common
    part of web scraping， again。 for better or for worse。 You do have to spend some
    time doing data cleaning。
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，HTML本身是一个字符串。所以我提取的所有内容也是字符串。因此，我需要将我的数值数据转换为数字，日期时间值和日期时间值，做这样的事情。所以这是网络抓取中非常常见的部分，再次强调，无论好坏，你确实需要花一些时间进行数据清理。
- en: So we're going to do that。 And then where web scraping really starts to be beneficial
    is if you can collect and store。 data from multiple different web pages。 So in
    our case， we've just been looking at Pennsylvania。 but what if we could collect，
    and store the same information for all 50 states？
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们要这么做。然后，网络抓取真正开始变得有益的是如果你可以收集和存储来自多个不同网页的数据。在我们的案例中，我们只是在查看宾夕法尼亚州，但如果我们能为所有50个州收集和存储相同的信息呢？
- en: Now we're starting to talk about real analyses that we can do。 Maybe I have
    a map of where the latitude and longitude is for every single state。 That's probably
    somewhere else。 But， you know， I can start visualizing trends that I find。 Because。
    you know， state admittance and area have anything to do with each other。 You know。
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们开始谈论可以进行的真实分析。也许我有每个州的纬度和经度地图。这可能在其他地方。但你知道，我可以开始可视化我发现的趋势。因为，你知道，州的入境和面积是有关系的。你知道的。
- en: there's all kinds of different things that you could analyze once you have the
    information。 for all of those 50 states。 Okay。 So we're going to do both of these
    things。 The second thing I was talking to you about， about looking at multiple
    pages， we'll do in。 the next section。 For now， we are going to work on this data
    cleaning issue converting those strings into numbers。
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你拥有这些信息，就可以分析各种不同的事情，针对所有50个州。好的。所以我们将做这两件事。我之前提到的关于查看多个页面的第二件事将在下一节进行。现在，我们将处理这个数据清洗问题，将这些字符串转换为数字。
- en: or date times。 So， if you remember in the last section。 we were gathering up
    what the date admitted was， for Pennsylvania。 So we use that by looking for that
    navigable string that we saved as admitted。 We look to the next element and then
    extracted the text。 So there's that。
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 或者日期时间。所以，如果你还记得在上一节中，我们收集了宾夕法尼亚州的加入日期。我们通过查找我们保存的可导航字符串“admitted”来使用它。然后查看下一个元素并提取文本。就是这样。
- en: In order to have Python recognize this as a date time， I have to do a couple
    of things。 First off。 I don't really want these parentheses。 Those are really
    just there telling me that Pennsylvania was the second state admitted。 to the
    Union。 But I really don't want that if I'm trying to convert this to a date time。
    So the first thing I'm going to do is split up this string by space。 And by the
    way。
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让 Python 将其识别为日期时间，我需要做几件事。首先，我并不想要这些括号。那些只是告诉我宾夕法尼亚是第二个加入联邦的州。但如果我想把它转换为日期时间，这些信息是多余的。所以我首先要做的是按空格分割这个字符串。顺便提一下。
- en: all of the data cleaning things I'm going to do for this particular case might。
    be different for your particular case。 So you're going to have to develop your
    own strategy depending on what kind of data you。 are looking at。 I just wanted
    to show you an example of what this process might be like。 Okay。 So we're going
    to split this on the spaces， which I'll do here。
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 我将在这个特定情况下进行的所有数据清洗工作可能与你的特定情况不同。所以你需要根据你所查看的数据类型来制定自己的策略。我只是想给你展示这个过程可能是什么样子。好的。所以我们将按空格进行分割，我将在这里执行。
- en: And we're going to leave off the last element。 So I don't want those parentheses。
    Second。 I'm just going to leave off the last element。 Now I'm going to rejoin
    this date back up and put those spaces back in。 Great。 This is still a string。
    So I want to now convert this to a date time。 There's a package that's pretty
    good for this， this date util that dot parser。
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将省略最后一个元素。所以我不想要那些括号。第二，我将省略最后一个元素。现在我将重新连接这个日期，并将空格放回去。很好。这个仍然是一个字符串。所以我现在想将其转换为日期时间。有一个非常好的包，叫做
    dateutil 的 parser。
- en: And really that's this job is to parse strings and try to turn them into dates。
    So we're going to use this。 And really this package is really good because all
    I have to do now is parse the string and。 it will try to auto detect what that
    date is。 And I did a good job。 Now it's converted it into a Python date time。
    And the real benefit of having this in a Python date time is that now we can do
    all kinds。
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 这项工作的真正目标是解析字符串并尝试将它们转化为日期。所以我们将使用这个。而且这个包非常好，因为我现在只需解析字符串，它会尝试自动检测日期是什么。我做得不错。现在它已经转换为
    Python 日期时间。将其转化为 Python 日期时间的真正好处在于，我们现在可以进行各种操作。
- en: of different analyses on this。 We could sort these by which state came first
    and which one was last。 something like that。 This is a date time object。 And we
    can also， it also has its own properties。 For example， if we just wanted to extract
    the year， all we have to do now is just say， dot year。 And that will come out
    as an integer。 So converting strings into dates is a really common task that you
    might need to do for。
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 进行各种不同的分析。我们可以按哪个州最先加入、哪个州最后加入进行排序。类似这样的。这是一个日期时间对象。它也有自己的属性。例如，如果我们只想提取年份，我们只需说
    .year。那将以整数形式返回。因此，将字符串转换为日期是你可能需要做的一个非常常见的任务。
- en: web scraping。 We also find that oftentimes what we need to do is take our strings
    and convert them。 into numbers。 If I have something like dollar values or next
    up， I think population and area。 something， like this， oftentimes I'll need to
    convert those strings into integers。 And unfortunately oftentimes it's not as
    easy as just saying int around the string。
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 网页抓取。我们也发现，通常我们需要做的就是将字符串转换为数字。如果我有一些像美元值这样的东西，或者接下来，我想说的是人口和面积。像这样的东西，通常我需要将这些字符串转换为整数。不幸的是，通常这并不是简单地对字符串使用
    int。
- en: Sometimes you'll have to like clean up the string a little bit。 So let's see
    an example of that。 Looking back over， actually we're going to go back over to
    the Pennsylvania web page。 And go back up to the top again， pardon the scroll。
    I'll try to go quickly to not hurt your eyes。 Alrighty。 Here we go。 Okay， cool。
    What I'm trying to do right now is look for the population of Pennsylvania。
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 有时你需要稍微清理一下字符串。所以让我们看看这个例子的情况。回顾一下，实际上我们要回到宾夕法尼亚州网页。再回到顶部，抱歉要滚动。我会尽量快速，以免伤害你的眼睛。好了。现在来吧。好的，很酷。现在我试图找出宾夕法尼亚州的人口。
- en: How many people live there？ Oh， here we go。 Okay， it's right here。 Total population。
    12 million people。 So if I use the same trick I used before。 what I'd be looking
    for is the word total。 And here's one of those examples。 If I can't just look
    for the word total， this bullet actually shows up in the inner HTML。
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 有多少人住在那里？哦，这里来了。好吧，正好在这里。总人口。1200 万人。所以如果我使用之前的那个技巧，我要寻找的是“总数”这个词。这里就是其中一个例子。如果我不能只找“总数”这个词，这个项目实际上会出现在内部
    HTML 中。
- en: text of this element。 So just looking for the word total would not work here。
    We're actually going to use one of those regex patterns to see if we can find
    a total somewhere。 Okay。 So we compiled that。 Okay， so here's what I mean。 We
    have the bullet。 We have some spacing characters。 Let's try our strategy。 Let's
    just go to the next element here。
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 这个元素的文本。因此，仅仅寻找“总数”这个词在这里是行不通的。我们实际上将使用其中一个正则表达式模式来看看我们是否能在某处找到总数。好的。我们编译了这个。好的，我的意思是。我们有项目。我们有一些空格字符。让我们试试我们的策略。我们就去下一个元素。
- en: So the next element after total was an area。 And if I look back over at the
    web page。 you'll see what error we made。 Yes， total does lead to the population。
    but the total also leads to the area because it's， under the total category there。
    Okay。 So let's just save that for now。 That might be useful later。 I'm going to
    save that as area text。
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 所以总数之后的下一个元素是一个区域。如果我回头看看网页，你会看到我们犯了什么错误。是的，总数确实指向人口，但总数也指向区域，因为它在总数类别之下。好的。那我们先保存一下。这可能以后会有用。我将其保存为区域文本。
- en: But let's now look for population。 So let's preface this by saying this is one
    of the most complicated examples that I'm going。 to show you， but just to show
    you all the really cool things that you could do。 Let's inspect this population
    and see if we can come up with a different strategy。 Okay。 So the population lives
    in this TD and it's not labeled。
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 但是现在让我们来寻找人口。因此，先说这是我将要展示的最复杂的例子之一，但只是为了向你展示你可以做的所有非常酷的事情。让我们检查这个人口，看看我们能否想出一个不同的策略。好的。这个人口生活在这个
    TD 中，并没有标记。
- en: There's no nice labels like a class or an ID for a fixed point。 It lives next
    to this total。 but there's another total that happens earlier up on the page。
    We could find all of the examples of total and then go to the second one or something，
    like that。 But let's try something else。 If we go up， let's see， maybe not there。
    How about here？
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 没有像类或 ID 这样的固定点的漂亮标签。它就生活在这个总数旁边，但页面上较早的位置还有另一个总数。我们可以找到所有总数的例子，然后去第二个之类的。但是让我们尝试其他方法。如果我们向上移动，看看，可能不是那里。这里怎么样？
- en: If we actually go above， so here's the table data， the next parent of that is
    going to be。 this table row。 The parent of that one， actually it's not a parent。
    It looks like it is the previous sibling or something like that， is eventually
    going。 to lead us to the population somewhere in here。 Okay。
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们实际上向上移动，所以这是表格数据，它的下一个父元素将是这个表格行。那个的父元素，其实它不是一个父元素。它看起来像是前一个兄弟或类似的东西，最终会引导我们找到人口，某处在这里。好的。
- en: So we're going to use the relationships between population and the actual population
    value。 to pull this population value。 We're going to have to use the structure
    of the HTML to achieve this。 Okay。 It turns out if we take population， move up
    to its parent， that'll get us up here to this。 header， then we have to go one
    more level up to the parent of the parent， then we look。
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们将利用人口与实际人口值之间的关系来提取这个人口值。我们将不得不利用 HTML 的结构来实现这一点。好的。事实证明，如果我们取人口，向上移动到它的父元素，这样就能到达这个头部，然后我们还需要再向上移动一层到父的父元素，然后我们再看。
- en: for the next sibling。 And now within that sibling， okay， this is the one we
    wanted。 the total with the population， number。 All we have to do now is find this
    TV tag and extract its text。 That's the text we're looking for。 Okay。 So if that
    seemed convoluted， I do not blame you。 I think that that was the most complicated
    example I have on here。
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 对于下一个兄弟元素。现在在那个兄弟元素中，好的，这就是我们想要的，带有人口数字的总数。我们现在要做的就是找到这个TV标签并提取它的文本。这就是我们要找的文本。好的，如果这看起来复杂，我不怪你。我认为这是我这里最复杂的示例。
- en: But sometimes you do have to be really creative with your strategy。 If I can
    find population。 move two levels up， then move to the next sibling of that element。
    then find the next table data and finally the text。 So sometimes your strategy
    has to be really creative here。 Just to show you an example of that。
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 但有时你确实需要在策略上发挥创造力。如果我能找到人口数据，向上移动两个层级，然后移动到该元素的下一个兄弟元素，再找到下一个表格数据，最后提取文本。有时你的策略必须非常有创意。只是给你展示一个这样的例子。
- en: Okay。 All right。 But finally， this population is still a string， right？
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 好的。但最后，这个人口数据仍然是一个字符串，对吗？
- en: To convert this to an integer， we actually have to get rid of those commas and
    then convert。 it to an integer。 So I'm going to take the population text。 I'm
    going to replace all the commas with nothing and then wrap all of that in integer。
    All right。 Awesome。 So now population is an integer and we can do some data analysis
    with this。
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 要将其转换为整数，我们实际上必须去掉那些逗号，然后转换为整数。所以我会取出人口文本，把所有逗号替换为空，然后把这一切都包裹在整数中。好的，太棒了。现在人口就是一个整数，我们可以用这个进行一些数据分析。
- en: If we collected all the populations from all the states， now we'd have maybe
    a cool data。 story to tell。 So oftentimes there's going to be tasks that you have
    to perform so often in web scraping。 that it's just easier to write a little function
    to help you repeat those tasks。 So I've written two little functions here。 The
    first one to convert date strings into dates and the second one to convert number。
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们收集所有州的人口数据，现在也许会有一个很酷的数据故事可以讲。因此，网络抓取中经常会有一些任务需要你反复执行，这时候编写一个小函数来帮助你重复这些任务会更简单。我在这里写了两个小函数，第一个是将日期字符串转换为日期，第二个是将数字字符串转换为数字。
- en: strings into numbers。 I'm using a little bit of regex and if you're not familiar
    with that。 don't worry about， it too much。 I'm basically just matching alphanumeric
    characters and spaces and commas for the dates and I'm。 matching digits， commas
    and dollar signs for the numbers。 That'll help me kind of weed out extraneous
    characters that I don't want。
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 我使用了一些正则表达式，如果你不熟悉这个，不用太担心。我基本上只是匹配日期中的字母数字字符、空格和逗号，而对于数字，则匹配数字、逗号和美元符号。这将帮助我排除不需要的多余字符。
- en: Then I can replace things like dollar signs and commas with nothing and convert
    to integer。 or use that date utility parser to convert my date strings。 Yes。 All
    right， so let's save that。 Great。 And we can actually test and see if these functions
    are doing what we want them to do。 Remember that area text that I saved earlier
    which has how large Pennsylvania is。
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我可以将美元符号和逗号替换为空，然后转换为整数，或者使用日期工具解析器来转换我的日期字符串。好的，让我们保存这个。很好。我们实际上可以测试一下这些函数是否在做我们想要的事情。还记得我之前保存的文本区域吗？里面有宾夕法尼亚州的面积。
- en: If I just want to extract the square miles， it has a lot of different information
    here。 but I just want to pull out the first little bit about the square miles。
    Let's try my two-inch function。 And it turns out because of the way I've written
    the regular expression。 I'm only matching， digits and commas so it stops after
    this first little bit and I'm able to convert that。
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我只想提取平方英里，它这里有很多不同的信息，但我只想提取有关平方英里的第一部分。让我们试试我的两个英寸函数。结果是，由于我编写的正则表达式的方式，我只匹配了数字和逗号，因此在这第一小部分后就停止了，我能够进行转换。
- en: to an integer。 Okay。 So that was the portion of the data cleaning for Web scraping。
    I certainly think that that's one of the things that you have to struggle through。
    It's a part of Web scraping。 You do have to do a fair amount of cleaning and converting
    the data you retrieve from。 the internet into a meaningful way。 The other thing
    that you should definitely spend time thinking about before you scale up。
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 转换为整数。好的，这部分是网络抓取的数据清理。我认为这是你必须经历的一个方面。这是网络抓取的一部分，你确实需要对从互联网上获取的数据进行相当程度的清理和转换，使其变得有意义。在你扩大规模之前，确实应该花时间考虑的另一个事情。
- en: to a larger Web scraping process is how are you going to store the data？
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 更大规模的网页抓取过程中的一个问题是你打算如何存储数据？
- en: I'm actually going to put all the information I've found so far into a little
    dictionary。 And hopefully it'll eventually be clear why I'm doing this。 But for
    now。 I'm going to create this Pennsylvania dictionary and it just contains everything，
    I've found so far。 I have the state name。 I have the population， the date that
    Pennsylvania was admitted to the Union as well as the square。
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 我实际上打算将迄今为止找到的所有信息放入一个小字典中。希望到最后会清楚我为什么这样做。但现在，我将创建这个宾夕法尼亚字典，它包含了我迄今为止找到的所有内容。我有州名，我有人口，宾夕法尼亚加入联邦的日期以及平方。
- en: miles。 So speaking of why would I want to put information in a dictionary like
    this？ Well。 if you ever work with the library called pandas， which a lot of folks
    do， you can take。 dictionaries like this， put them into lists， and then wrap those
    into a pandas data frame。 Great。 And so now everything you know about pandas，
    if you do know about pandas， you can apply。
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 英里。说到为什么我想把信息放在这样的字典中？好吧。如果你曾经使用过一个叫做pandas的库，很多人都在用，你可以将像这样的字典放入列表，然后将其封装到一个pandas数据框中。这很棒。因此，现在你知道的关于pandas的一切，如果你确实了解pandas，你都可以应用。
- en: to this information about Pennsylvania。 So you can imagine how if you scale
    up to more and more states。 you're going to have more， and more information here
    to play with in this pandas data frame。 pandas also has this really nice functionality
    that if you want to save something to a CSV。 it's very simple。 You just take the
    data frame， do dot to CSV， excuse me。 And then you provide。
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 关于宾夕法尼亚的信息。所以你可以想象，如果你扩大到越来越多的州，你会在这个pandas数据框中有越来越多的信息可以玩。pandas还有一个非常好的功能，如果你想将某些内容保存为CSV，非常简单。你只需将数据框做成dot
    to CSV，抱歉。然后提供。
- en: what do I want to name this file？ This one just contains information about Pennsylvania。
    So Penn State information dot CSV。 If I was working in Jupiter notebook。 that
    would have saved directly to whatever directory， I was working out of。 Since I
    am in Google Colab， you'll see that that file has been saved within Google Colab。
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 我想把这个文件命名为什么？这个文件仅包含关于宾夕法尼亚的信息。所以Penn State信息.dot CSV。如果我在Jupyter Notebook中工作，它会直接保存到我正在使用的任何目录中。由于我在Google
    Colab中，你会看到那个文件已经保存在Google Colab中。
- en: and I can extract that file and download it by using this files dot download。
    Okay。 So that actually does go straight to my computer。 Okay。 All right。 A couple
    more exercises。 Let's try some of these things。 Go back to the page for Pennsylvania。
    develop a strategy for extracting the median household， income for the state of
    Pennsylvania。
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 我可以提取那个文件并通过使用这个files.dot.download下载它。好的。所以这实际上会直接发送到我的电脑。好的。好吧，再来几个练习。让我们尝试一些这些事情。回到宾夕法尼亚的页面。制定一个提取宾夕法尼亚州中位家庭收入的策略。
- en: So it's going to be a text string from that string converted to an integer。
    For part two of this question， can you update the state DF that contains the information。
    about Pennsylvania to now also include the median household income for people
    that live。 in Pennsylvania？ Okay。 So here are the solutions。 Hit pause if you'd
    like to try those out yourself。
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，它将是一个文本字符串，从该字符串转换为整数。对于这个问题的第二部分，你能否更新包含关于宾夕法尼亚州信息的state DF，现在还包括居住在宾夕法尼亚州的人的中位家庭收入？好的。这是解决方案。如果你想自己尝试一下，请暂停。
- en: Let's actually go back to the page and see if we can come up with a strategy
    for extracting。 the median household income。 Okay。 That's right here， median household
    income。 and then we have a dollar figure。 So I think oftentimes， you know， we
    could definitely。 let's go ahead and inspect it。 Oftentimes， especially if you've
    got a huge table like this， yeah。
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，让我们回到页面上，看看我们是否可以制定一个提取中位家庭收入的策略。好的，就在这里，中位家庭收入。然后我们有一个美元数字。所以我认为很多时候，我们确实可以。让我们继续检查一下。很多时候，尤其是如果你有这样一个巨大的表格，是的。
- en: these aren't tagged， super well。 We do see this median household income over
    here。 So maybe we can try that text matching approach as well。 Okay。 Let's try
    that。 Soop。find。 Next。 All right。 So we found it median household income by doing
    the text matching definitely worked here。 So let's see。 The way that this has
    usually been working is if we can find this string。
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 这些没有被标记得很好。我们确实在这里看到这个中位家庭收入。所以也许我们可以尝试那个文本匹配的方法。好的。让我们试试。Soop.find。下一个。好的。所以我们通过文本匹配找到的中位家庭收入在这里确实有效。那么，让我们看看。通常的工作方式是，如果我们能找到这个字符串。
- en: the next element， should have our data。 But it doesn't。 So unfortunately with
    this one。 the next element is this division that doesn't really have， anything
    in it。 But if you stick with it and now actually try the next element， ah， there
    is our dollar。 figure that we were looking for。 Okay。 So sometimes you do you
    have to just be persistent with it。
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个元素应该包含我们的数据。但它并没有。所以不幸的是，这个下一个元素是一个没有任何内容的分区。但是如果你坚持下去，然后实际上尝试下一个元素，啊，我们正在寻找的美元数值在这里。好的。有时候，你确实需要坚持不懈。
- en: If it wasn't in that next， maybe the next one。 All right。 So we really just
    want the text from this。 Let's write this。 Okay。 But we have this dollar sign
    and also this weird number four。 So if we want to convert this to an integer，
    let's see if our two int will work。 And it does awesome。 So let's actually call
    this one MHI text so that we can be very explicit for our future。
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 如果不在下一个，也许在下下一个。好的。所以我们真的只想要这个文本。让我们写下来。好的。但是我们有这个美元符号，还有这个奇怪的数字四。所以如果我们想把它转换为整数，让我们看看我们的两整数函数是否能工作。效果不错。所以我们实际上将其称为MHI文本，以便在将来能够非常明确。
- en: cell。 And this one is the MHI， the actual integer itself。 Great。 Okay。 So we
    have the mean household income， which is what we wanted。 Let's update the state
    DF to include this as well。 So there are a couple different strategies here。 I'm
    going to take the hint， which says add the median household income to the dictionary。
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 单元格。这个是MHI，实际的整数本身。太好了。好的。所以我们得到了我们想要的中位家庭收入。让我们更新州数据框以包括这个。所以这里有几种不同的策略。我将采取提示，添加中位家庭收入到字典中。
- en: and then recreate state DF。 So we have the Pennsylvania dictionary。 And the
    way that you can add information to the dictionary is now just give this a key。
    and provide the value。 Now I'm going to recreate the state data frame。 Okay。 And
    the median household income now has its own column。 All right。 So we are almost
    there。
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 然后重建州数据框。所以我们有宾夕法尼亚字典。你可以向字典添加信息的方式就是给它一个键，然后提供值。现在我将重建州数据框。好的，而中位家庭收入现在有了自己的列。好的。我们快到了。
- en: We're almost to the point where we can start collecting all kinds of information
    about all。 kinds of different states。 Let's talk a little bit about what the next
    considerations will be in your pipeline。 So for this next step， I'm going to try
    to collect the name， the data admitted， the population。 the area and the median
    household income for lots and lots of different states in the， U。S。
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 我们几乎到了可以开始收集关于各种不同州的所有信息的阶段。让我们谈谈在你的管道中下一个考虑因素。对于下一步，我将尝试收集名称、成立日期、人口、面积和中位家庭收入，以获取美国许多不同州的数据。
- en: Because I'm trying to find the same information on lots of different pages，
    what I'm going。 to do is actually save these as functions。 And by the way， before
    we get started with that。 let me just show you one other page here。 Let's see。
    I think I have。 Let's actually see and make sure that this strategy is going to
    work。 Okay。
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我正在尝试在许多不同的页面上找到相同的信息，所以我将把这些保存为函数。顺便说一下，在我们开始之前，让我给你看另一个页面。让我们看看。我想我有。让我们实际看看，确保这个策略会有效。好的。
- en: So here's why this is going to be a good strategy for us。 If I go to another
    page of a state within the United States， I see the same setup。 The page looks
    almost the same。 It just has different information filled in for this state。 So
    the state name is still setting up here， the admitted to the union。
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是为什么这对我们来说将是一个好策略的原因。如果我去到美国另一个州的页面，我看到相同的设置。页面看起来几乎相同，只是填入了不同的信息。因此，州名仍然在这里设置，加入联邦的时间。
- en: It's labeled exactly the same as it was on the Pennsylvania page。 The capital。
    everything looks really， really similar， the total area， you know， all of these。
    various things that I'm looking for。 Okay。 So our strategy is going to be try
    the same functions that we just had for Pennsylvania。 but with a different URL，
    okay， with a different state's URL。
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 它的标签与宾夕法尼亚页面上的完全相同。大写字母。所有的东西看起来真的，真的很相似，总面积，你知道，我在寻找的各种东西。好的。所以我们的策略将是尝试我们刚才用于宾夕法尼亚的相同功能，但使用不同的URL，好的，用不同州的URL。
- en: And hopefully the functions that we're creating are generic enough that we'll
    go ahead and。 be able to extract information for all 50 states in a really methodical
    sort of way。 So the first step is to actually go ahead and here， literally just
    putting all of the。 different information together。 I'm creating a function that
    really matches what we did in the previous steps for Pennsylvania。
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 希望我们创建的功能足够通用，以便能够以一种非常系统的方法提取所有50个州的信息。所以第一步实际上是将所有不同的信息结合在一起。我正在创建一个真正与我们在宾夕法尼亚州所做的相匹配的函数。
- en: but I'm just writing them as functions so they can continue using them for other
    states as， well。 So the first function gets the name of the state。 I did have
    to add in a tiny bit of red jacks here。 There are a couple of states that are
    just kind of have slightly different patterns， so。 that's why you see this red
    jacks here。 I have my admitted to the union strategy and I'm converting that to
    a date。
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 但我只是将它们写成函数，以便它们可以继续用于其他州。所以第一个函数获取州的名称。我确实需要在这里添加一点红色插图。有几个州的模式略有不同，所以您会看到这个红色插图。我有我的入联策略，并将其转换为日期。
- en: I can get the population， which involved that weird like parent， parent， net
    sibling kind， of thing。 And I have the area as well as the income， which we just
    did。 Because I now have these as functions。 what I can do is just apply them to
    a different， state's table from their beautiful soup。 Okay。 So let's try， for
    example， New York state， the page for New York。
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 我可以获得人口，这涉及到那种奇怪的父级、父级、净兄弟类型的东西。我有区域以及我们刚刚处理过的收入。因为我现在将这些作为函数，所以我可以将它们应用于不同州的Beautiful
    Soup表格。好的，比如说，试试纽约州的页面。
- en: So what I'm going to do is now just request the information from the Wikipedia
    page for。 the state of New York。 And I'll parse that with beautiful soup here。
    So NY soup refers to the beautiful soup object that was created by parsing the
    HTML from。 the New York state webpage。 Okay。 New York state follows the same idea
    as Pennsylvania。
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我现在要做的是从纽约州的维基百科页面请求信息。我将在这里用Beautiful Soup解析它。所以NY soup指的是通过解析纽约州网页的HTML而创建的Beautiful
    Soup对象。好的，纽约州遵循与宾夕法尼亚州相同的思路。
- en: The very first table on the page refers to that right column。 So we'll save
    it as NY table。 And the way I've set up these functions， you're welcome to set
    up your functions in a different。 way。 That is kind of the video web scraping。
    People will have different approaches and different ideas here。 But the way I
    set up my functions， I need to pass that table now into this get named function。
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 页面上的第一个表格指的是右侧列。所以我们将其保存为NY table。根据我设置这些函数的方式，您可以以不同的方式设置您的函数。这就是网络抓取视频。人们在这里会有不同的方法和不同的想法。但我设置函数的方式，我现在需要将该表格传入这个获取名称的函数。
- en: Let's see if it works。 Nice。 And I can get the population for the state of New
    York。 This should actually be an integer。 Okay。 So because I'm trying to scale
    this up， and this is where。 you know， again， you have， to be really creative，
    get really make good use of writing functions。 I'm actually going， to write a
    function that will take in the state's URL。
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看它是否有效。很好。我可以获得纽约州的人口。这实际上应该是一个整数。好的。因为我试图扩展这个，而这是您知道的，您必须非常有创意，充分利用编写函数。我实际上将编写一个接受州URL的函数。
- en: So if I know the URL to the Wikipedia page， I'll pass that into this function。
    Then I will parse that URL with this function。 I will request the HTML using requests。
    I will use beautiful soup to parse the page。 And then I will find the very first
    table on that page。 Now that I have the table， I'll create a blank dictionary
    and just fill that in with all。
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我知道维基百科页面的URL，我会将其传入这个函数。然后我会用这个函数解析该URL。我将使用requests请求HTML。我将使用Beautiful
    Soup解析页面。然后我会找到该页面上的第一个表格。现在我有了表格，我会创建一个空字典，并用我想要的所有不同信息填充它。
- en: of the different information that I want。 Finally， I'll be returning that dictionary
    as state info。 So because I've written everything in this really nice way where
    I'm using functions。 to gather all of my information， all I have to do now is
    say get state info， pass in the。 New York URL and out pops all of the information
    I wanted for New York。
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我会将该字典作为州信息返回。因为我以这种很好的方式编写了一切，使用函数收集我的所有信息，所以现在我只需说获取州信息，传入纽约的URL，所有我想要的信息就出来了。
- en: So that's where you're headed。 As you start scaling up。 as you start doing larger
    web scraping projects， you're going。 to be writing functions to gather information
    from lots of different pages in a systematic， way。 The only issue now， the final
    thing before we really have the full pipeline to this function。
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这就是你将要前往的地方。当你开始扩大规模，进行更大的网络抓取项目时，你将会编写函数，以系统的方式从许多不同的页面收集信息。现在唯一的问题是，在我们真正拥有这个函数的完整流程之前，最后一件事。
- en: the only extra thing I have to do is supply a link。 So before。 I just told you
    what the link was for New York State。 And it might be simple。 It might be， you
    know。 road_island。 Maybe that's all I need to add to this link and I'm done。 For
    the example of New York State， if I just went to the New York page， that's not
    going。
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 我唯一需要额外做的就是提供一个链接。因此，在此之前，我刚刚告诉你纽约州的链接是什么。可能这很简单，可能就是`road_island`。也许这就是我需要添加到这个链接的内容，然后就完成了。以纽约州为例，如果我只是去纽约页面，那是不够的。
- en: to give me exactly what I want。 I have to know that I need to visit this New
    York underscore and then state。 So how would I know what that link was and how
    would I be able to traverse all of these。 different pages in this systematic way？
    Luckily enough。 some of the best web scraping projects are really born from having
    lists， of links。 Okay。
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 为了给我提供我想要的确切内容，我必须知道我需要访问这个纽约州的链接。那么我如何知道这个链接是什么，以及如何以这种系统化的方式遍历所有不同的页面呢？幸运的是，一些最佳的网络抓取项目确实是通过拥有链接列表而诞生的。好的。
- en: So this is what I was just talking about。 The fact that the New York URL does
    not just follow the normal naming convention about New。 underscore York。 We actually
    also have this little bit about state。 Instead of just guessing these links， what
    we're actually going to do is collect these。 links from a different page and then
    follow each link to grab the information。
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我刚才提到的。纽约的URL并不遵循关于“New underscore York”的正常命名约定。我们实际上还有关于“state”的小部分。与其猜测这些链接，我们实际上要做的是从另一个页面收集这些链接，然后跟随每个链接以抓取信息。
- en: So let's actually check out this other Wikipedia page。 This Wikipedia page is
    a list of states and territories in the United States。 And if I scroll down this
    page a little bit， I'll eventually come to a table that has all。 of the 50 United
    States。 And the cool thing is here， if I click on one of these links。
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们实际查看这个维基百科页面。这个维基百科页面是美国各州和领土的列表。如果我向下滚动这个页面一点，我最终会找到一个包含所有50个州的表格。很酷的是，如果我点击其中一个链接。
- en: I am taken to the page all， about that state。 And here's the information I'm
    trying to gather from that right table。 So if I can scrape this page and gather
    up all of these state links， then I'll be able。 to visit each link and turn and
    gather up all my information。 Let's go back over to the code。 That's exactly what
    we're going to be doing in the next section。
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 我会被带到关于该州的页面。这是我试图从右侧表格收集的信息。因此，如果我能抓取这个页面并收集所有这些州的链接，我就能够访问每个链接，逐个收集所有信息。让我们回到代码。这正是我们在下一部分要做的事情。
- en: Here's my URL that links to that states and territories of the US。 I'm still
    using requests to get the HTML and I'm still using 。txt to just extract the HTML，
    string。 Then I'm using beautiful soup to parse through that page。 I won't go through
    in detail how I got to the next bit of code， but it's the exact same。
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我的URL，链接到美国的州和领土。我仍在使用requests获取HTML，并且我仍在使用.txt仅仅提取HTML字符串。然后我使用beautiful
    soup解析该页面。我不会详细讲解我如何得到下一段代码，但它是完全一样的。
- en: thing we've been doing。 I did a right click， inspect。 that I developed a strategy
    that would help me be able to find， all of those different links。 This one's actually
    the second table on the page， which is why I have this index。 And then I did a
    find all for the row headers。 That's going to be each of those states。 Okay。 Yes。
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 我们一直在做的事情。我右键点击并检查，制定了一项策略，以帮助我找到所有不同的链接。这实际上是页面上的第二个表格，这就是我有这个索引的原因。然后我做了一个查找所有的行头。这将是每一个州。好的，是的。
- en: this is going to be all of the different states。 So if I look for， let's just
    try this first one。 make sure this is good。 Yep。 If I just look at the very first
    state row and I extract the anchor tag。 you have to look， at it this a little
    bit more to find it。 There's a couple things going on here。 Ah， here we are。 It's
    at the end。 This is the anchor tag I want to extract。
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 这将是所有不同的州。所以如果我查找，让我们尝试第一个，确保这是好的。是的。如果我只查看第一行州并提取锚标签，你必须仔细查看，找到它。这里有几个事情在发生。啊，我们在这里。它在最后。这是我想提取的锚标签。
- en: It has the word Alabama and it has the link to Alabama's webpage within Wikipedia。
    So I can find the link that I'm trying to follow。 And once I have that。 now I
    can use requests and beautiful suits to go grab my information， about Alabama。
    So I'm going to do this。 I'm going to grab all these different state links for
    every row in that table。
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 它包含“阿拉巴马”这个词，并且有指向阿拉巴马维基百科网页的链接。所以我可以找到我想要跟踪的链接。一旦我有了那个，现在我可以使用请求和 BeautifulSoup
    来获取有关阿拉巴马的信息。所以我要这样做。我将为表格中的每一行抓取所有不同的州链接。
- en: which is， going to give me 50 different links that all point to the different
    states。 Let's just show you those first five links that I gathered。 Pretty much
    what you'd expect here。 But eventually， you know， the Washington page or the New
    York page are not as simple as just， this。 as this pattern。 Okay。 But I can't
    just put this pattern into requests。
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 这将给我提供 50 个不同的链接，指向不同的州。让我先给你展示我收集的前五个链接。这几乎是你预期的内容。但是最终，华盛顿页面或纽约页面并不像这个模式那样简单。好的，但我不能只把这个模式放到请求中。
- en: If I am going to eventually scrape each of these web pages， what I have to do
    is append， to this。 to this little stub link。 I have to append en。wicapedia。org
    to each of these stubs。 So my base URL for this specific website is the Wikipedia
    link。 And I will just create。 I'll just add to each of these stub links， I'll
    add the full Wikipedia， so that when I use request。
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我最终要抓取每一个网页，我需要做的就是把 `en.wikipadia.org` 附加到每一个小链接上。因此，我这个特定网站的基本 URL 就是维基百科链接。我将创建，我将把完整的维基百科地址添加到每一个小链接上，这样当我使用请求时。
- en: it knows exactly where to go。 So I'm just doing that with a list comprehension
    here。 And now you see that I have a full URL and I could use requests to actually
    pull the HTML。 for each of these pages。 And just to be sure that we did this properly
    for everything。 I'll show you the bottom five。 And yes， for example， Washington
    does not just point to the state。
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 它知道确切的去向。所以我在这里用列表推导式做这个。现在你可以看到我有了完整的 URL，我可以使用请求来实际拉取每个页面的 HTML。为了确保我们对每件事都做得正确，我将向你展示底部的五个链接。是的，例如，华盛顿并不仅仅指向该州。
- en: There's lots of things I have to do with Washington。 All right。 And to be sure
    that we've got everything， we do have 50 different links in that list。 So the
    nice thing is now I have all of these lists in， excuse me， all of these links
    in， a list。 And if I'm going to look for information about each state， all I have
    to do is cycle。
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 我需要处理许多与华盛顿相关的事情。好吧，为了确保我们得到了所有内容，我们在那份列表中确实有 50 个不同的链接。因此，好的地方是，现在我有了所有这些链接，在一个列表中。如果我想要查找每个州的信息，我需要做的就是循环。
- en: through this link list and visit each page。 This is what I mean by a full web
    scraping pipeline is that you'll be visiting each link。 to collect information。
    One final note that I definitely want to bring up because unfortunately it does
    happen。 often with web scraping。 What can happen is， let's say I get to one of
    these pages and it just so happens that。 someone forgot to write in the population
    of Arizona or something like that。
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这个链接列表访问每个页面。这就是我所说的完整网页抓取管道，你将访问每个链接以收集信息。最后，我一定要提到的一点，因为不幸的是，这种情况确实经常发生。发生的情况是，假设我访问其中一个页面，恰好有人忘记写亚利桑那州的人口或类似的东西。
- en: What if there is a missing value on one of these pages？
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 如果其中一个页面上缺少一个值怎么办？
- en: If I've written everything in these sort of generic functions and I'm trying
    to visit。 every single link， if I'm missing a value， if I go with my current setup，
    if I'm missing。 some value that I'm looking for， that could kill my entire pipeline。
    Python would stop。 throw an error， and everything I had scraped so far would be
    lost。
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我在这些通用函数中写了所有内容，并且我尝试访问每一个链接，如果我遗漏了一个值，如果我按照我当前的设置操作，缺少了我正在寻找的某个值，这可能会破坏我的整个管道。Python
    会停止，抛出一个错误，而我到目前为止抓取的所有内容将会丢失。
- en: So I really want to try to avoid that situation if possible， and I would highly
    encourage you。 to somehow adjust your code to prepare yourself for this possibility
    that something might be。 missing from one of these pages。 So we're going to use
    try and accept pairs in order to try to protect us from this heart。 break。 If
    you haven't seen try except before I wanted to demo that just quickly。 For example。
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我真的希望尽可能避免那种情况，我会强烈建议你以某种方式调整你的代码，为这种可能性做好准备，以防某些信息可能缺失自这些页面之一。因此，我们将使用try和accept对来保护我们免受这种心碎。如果你之前没见过try和except，我想快速演示一下。例如。
- en: here's this really generic sort of basic function that just squares a number。
    So if I plugged in three， I would get nine back。 But what happens if I try to
    square the word high with the way that this is written。 it， will throw me an error
    because that's not an allowed， I can't multiply high by high。 If I wanted to make
    this function a little bit more robust to the possibility that someone。
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个非常通用的基本函数，只是平方一个数字。所以如果我输入3，我会得到9。但是如果我尝试用这种方式平方单词“high”，它会抛出一个错误，因为那是不允许的，我不能把“high”乘以“high”。如果我想让这个函数更健壮，以防有人。
- en: submits a string， of course I could assert in the beginning that I have a number，
    something。 like that， or maybe I could put this try and accept statement。 So what
    this function will do is try to complete the square like I asked it to。 But if
    it has a type error， which is what would happen if I submitted a string， if that。
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 提交一个字符串，当然我可以在开始时断言我有一个数字，类似这样的东西，或者我可以放置这个try和accept语句。那么这个函数将尝试像我要求的那样完成平方。但如果它有类型错误，如果我提交了一个字符串，那将发生什么。
- en: kind of error happens， it should return no can do。 Okay。 so I'm trying something
    if it doesn't work and I see this type of error， you should。 print something out。
    So with this robust version of my square function。 now if I try to square the
    word high， we will hit that accept because we have a type error and Python will
    print out no can。
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 这种错误发生时，应该返回“无能为力”。好的。那么我正在尝试一些东西，如果它不起作用，我看到这种类型的错误，你应该。打印出一些信息。因此，使用我这个更健壮的平方函数。如果我尝试对单词“high”进行平方，我们将遇到接受，因为我们有一个类型错误，Python会打印出“无能为力”。
- en: do。 Okay， all right， we'll return it， right？ Okay， so in web scraping。 the way
    that we'll use try and accept to protect ourselves from， errors is basically like
    this。 I'll have some kind of scraping function， I'll be passing in some kind of
    page， I will。 try to perform some parsing， some scraping and try to return that
    scraped value。
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，好吧，我们会返回它，对吗？好的，所以在网页抓取中，我们将如何使用try和accept来保护自己免受错误的影响，基本上是这样的。我会有某种抓取函数，我会传入某种页面，我会。尝试进行一些解析，进行一些抓取，并尝试返回抓取的值。
- en: If I ever have some kind of problem， instead of just completely shutting down，
    I will ask。 my scraper to return a value of none， as in that was missing from
    this page or I couldn't， find it。 So I'm going to rewrite my scraper that I had
    before， the one that was collecting the。 different information from the New York
    page。 I'm going to rewrite that function so that in case some information is missing
    from my。
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我遇到某种问题，而不是完全关闭，我会要求我的抓取器返回一个None值，也就是说这个页面缺失，或者我找不到它。因此，我将重写之前的抓取器，即从纽约页面收集不同信息的那个。我将重写这个函数，以防我的某些信息缺失。
- en: state page， instead of just completely shutting down， I just want Python to
    return none。 And for that one value。 Okay。 So here's my more robust scraper for
    these state pages。 I'm still going to put in the state URL。 I'm actually even
    going to be even more cautious。 If I some， for some reason， can't find a main
    table on the page， I actually want Python。
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 状态页面，而不是完全关闭，我只希望Python返回None。对于那个值。好的。那么这是我为这些状态页面准备的更健壮的抓取器。我仍然会输入状态的URL。实际上，我会更加小心。如果我出于某种原因在页面上找不到主表格，我实际上希望Python。
- en: and then I do want it to tell me that it could not parse a table for that page
    and return， none。 I can't find any of that information you're looking for。 The
    entire dictionary is null and void if I can't find that table。 But otherwise。
    it's going to try to parse the state page and find the main table。 Okay。
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我确实希望它告诉我无法解析该页面的表格并返回None。如果我找不到你正在寻找的任何信息，整个字典将是无效的，如果我找不到那个表格。但否则，它将尝试解析状态页面并找到主表格。好的。
- en: if it gets through that process and it did find the main table， I'll still initialize。
    my blank state in vote dictionary。 Then I'm going to try every single function
    that gathers the state。 the date admitted population， et cetera。 For all of those
    functions。 I am going to try that function。 And if it doesn't work， if I can't
    find it， if something happens。
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 如果它通过了那个过程并且确实找到了主要表格，我仍然会初始化我的空状态投票字典。然后我会尝试每一个收集状态的函数，入境日期、人口等。对于所有这些函数，我都会尝试那个函数。如果它不工作，如果我找不到它，如果发生了什么事情。
- en: just put none in place， of the state info value。 Okay， remember state info is
    my dictionary。 Value will be whatever value I'm looking for at that moment。 All
    right， so let's add that。 And now when I use this more robust scraper on my New
    York page， it still is going to。 return me the same information that I had before
    because it passed all of my checks and it。
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 只是将none放在状态信息值的位置。好吧，记住状态信息是我的字典。值将是我那一刻要寻找的任何值。好吧，让我们添加它。现在，当我在我的纽约页面上使用这个更强大的爬虫时，它仍然会返回我之前拥有的相同信息，因为它通过了我所有的检查。
- en: was able to find all five of these different values。 If there was any one value
    missing。 only that value would get none。 And so I'd still be able to collect some
    data and this scraper wouldn't completely fail。 It would just give me none back。
    Okay。 Let's try our robust scraper on some nonsensical pages。 just to see what
    it would do。 What are this Wikipedia page for the Python conference？ Funny enough。
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 能够找到这五个不同的值。如果有任何一个值缺失，只有那个值会返回none。因此，我仍然能够收集一些数据，这个爬虫不会完全失败。它只是会返回none。好吧，让我们在一些无意义的页面上尝试我们更强大的爬虫，只是看看它会怎么做。这个维基百科页面是关于Python会议的？有趣的是。
- en: a lot of these Wikipedia pages do have tables on them。 So if this page does
    have a table。 it passes our first check。 It creates that blank state info dictionary。
    but it can't find any of the other values， so， they all come back as none。 If
    I gave it a completely nonsensical website， this is not a real website， at least
    I hope， not。net。
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 许多维基百科页面上确实有表格。因此，如果这个页面上有表格，它就通过了我们的第一次检查。它创建了那个空的状态信息字典，但找不到其他任何值，所以它们都返回为none。如果我给它一个完全无意义的网站，这不是一个真实的网站，至少我希望不是，.net。
- en: Now we are going to say， hey， I can't even find a table on this webpage。 This
    is nonsensical。 so can't do it。 All right， so we've protected ourselves from a
    couple of different types of errors。 And that's something you do want to try to
    add to your web scraping。 As you scale up。 as you develop your skills， eventually
    being able to address the possibility。
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们要说，嘿，我甚至在这个网页上找不到表格。这是无意义的，因此不能执行。好吧，我们已经保护自己免受几种不同类型的错误。这是你在网络爬虫中希望尝试添加的内容。随着你技能的提升，最终能够处理这种可能性。
- en: of having missing values is going to be really critical to not losing a whole
    bunch of work。 All right。 One other thing， as you scale up， you will want to add
    pauses to your web scrapers。 So this is the point where I tell you， please do
    be a responsible web scraper。 Anytime you are requesting information， requesting
    that HTML， you are hitting the server of whoever。
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 缺失值的处理对不丢失大量工作将非常关键。好吧，还有一件事，随着你技能的提升，你会想要在你的网络爬虫中添加暂停。所以这是我告诉你，请做一个负责任的网络爬虫。当你请求信息、请求HTML时，你正在访问任何人的服务器。
- en: is hosting that webpage。 So if I'm hitting Wikipedia and asking it for thousands
    of different pages of information。 that is me using Wikipedia's resources。 So
    to be responsible in this。 because a robot like this web scraper could hit that
    server， many， many， many times。 sometimes we'll add artificial pauses to our web
    scrapers。
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 托管那个网页的。所以如果我正在访问维基百科并请求成千上万不同页面的信息，那就是我在使用维基百科的资源。因此，为了在这方面负责任，因为像这样的网络爬虫可能会多次访问该服务器。有时我们会在我们的网络爬虫中添加人工暂停。
- en: Just to make sure we don't overload anyone's servers and that we are web scraping
    responsibly。 So if you do want to add artificial pauses where you're just slowing
    your scraper down。 a little bit， that you can do with the time library。 So import
    time， then we're going to do time。sleep2。 That says I would like you to sleep
    for two seconds。 Now it's not going to compute anything。
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 只是为了确保我们不会给任何人的服务器带来负担，并且我们是负责任地进行网页抓取。因此，如果你想添加人工暂停，让你的爬虫稍微慢一点，你可以使用time库。导入time，然后我们要做time.sleep(2)。这意味着我希望你暂停两秒钟。现在它不会计算任何东西。
- en: it just slept for two seconds and then returned。 So if I had some kind of calculation
    here。 I have A， then I'm going to pause for five， seconds， then I'm going to compute
    B equals A plus one。 So let's see what that feels like。 Pauseing for five seconds，
    we're waiting。 we're waiting artificially long， and then we， get returned B equals
    six。
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 它只休眠了两秒钟，然后返回。所以如果我这里有某种计算。我有 A，然后我会暂停五秒钟，然后我会计算 B 等于 A 加一。那么我们来看看这感觉如何。暂停五秒钟，我们在等待。我们等得有点长，然后我们得到了
    B 等于六。
- en: So it just adds artificial pauses so that you don't overwhelm anyone's server。
    Sites will have what they're allowed usage or they're allowed scraping is typically
    on。 their web page。 There's often going to be this file called robots。txt。 I'm
    going to talk about that in a little bit in my conclusion。 But from Wikipedia's
    robots file。
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 所以它只是添加了人工暂停，以便你不会压垮任何人的服务器。网站通常会在他们的网页上列出他们允许的使用或抓取。通常会有一个叫做 robots.txt 的文件。我将在我的结论部分稍后谈到这个。但根据维基百科的机器人文件。
- en: paying attention to Wikipedia's rules， they request。 that people only ask for
    one request per second。 So one page request per second per computer that is asking
    for information。 So we're actually going to add a one second pause。 Every single
    time we try to request information about a state。
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 注意维基百科的规则，他们要求人们每秒只请求一次。因此，每台计算机每秒请求一个页面的信息。所以每次我们尝试请求有关某个州的信息时，我们实际上会添加一秒钟的暂停。
- en: And because I do have to add this artificial pause and I don't want this tutorial
    to take。 artificially long， I'm only going to collect information for ten states
    for now。 But as you'll see what comes up next， you could easily do this for all
    50 states using。 your own home internet IP。 By the way， if you don't respect a
    site's limits。
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我确实需要添加这个人工暂停，而我不想让这个教程花费太久，所以我现在只会收集十个州的信息。但正如你将看到的，接下来发生的事情，你完全可以使用你自己的家庭互联网
    IP 为所有50个州做这件事。顺便说一下，如果你不尊重网站的限制。
- en: you can get your IP blocked from that， site。 And it's not a fun thing。 It means
    that you wouldn't be able to access Wikipedia or whatever site you're scraping。
    So please do try to be as responsible as possible and follow those guidelines
    that are recommended。 by the site。 Try to figure out what the site's rules are
    before you start scraping information。
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会被那个网站封锁IP。这可不是一件有趣的事。这意味着你将无法访问维基百科或你正在抓取的任何网站。因此，请尽量负责任地遵循网站推荐的那些指南。在开始抓取信息之前，尝试弄清楚网站的规则是什么。
- en: Okay。 Okay。 So I'm going to eventually scrape information from all of these
    different state pages。 But just to reiterate， what I'm trying to do is get one
    dictionary of values for each， state。 Then I'll put those dictionaries into a
    list like this and I'll use pandas to convert that。 list to a data frame。 So you
    can see I'm starting to build out all of this nice information that I can eventually。
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 好的。好的。所以我最终会从所有这些不同的州页面抓取信息。但我想重申，我试图做的是为每个州获取一个值字典。然后我会将这些字典放入一个列表中，然后使用 pandas
    将这个列表转换为数据框。因此，你可以看到我开始构建所有这些漂亮的信息，最终可以使用。
- en: extract to a CSV。 And after the information is in a CSV format。 I can use any
    tool that I know about to analyze， it。 I could analyze it in Excel or with Tableau
    or anything like that。 Okay。 So this is my goal。 trying to get the state information
    in a data frame like this so that， I can extract a CSV。
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 提取到 CSV 中。在信息以 CSV 格式存储后，我可以使用我知道的任何工具来分析它。我可以在 Excel 中或使用 Tableau 或其他任何工具分析它。好的。所以这是我的目标。试图将州信息放入像这样的数据框中，以便我可以提取
    CSV。
- en: This is our full data pipeline。 Okay。 And this should be true for pretty much
    any kind of web scraping project you're taking。 on。 You will somehow want to gather
    a list of links to visit。 For us。 it's going to be the links of each state and
    we already did that。 Okay。 Now， for each state link。 we're going to gather the
    information about that state as a dictionary。
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们的完整数据管道。好的。这对于你正在进行的几乎任何类型的网页抓取项目都应该成立。你会以某种方式想要收集要访问的链接列表。对我们来说，它将是每个州的链接，而我们已经做到这一点。好的。现在，对于每个州链接，我们将收集该州作为字典的信息。
- en: We will append that state dictionary to a list and eventually we'll convert
    that list of dictionaries。 into a data frame so that we can save that data frame
    to a CSV or we can directly save。 it as an Excel file if we'd like。 So let's see
    that in action。 We already have our list of links。 Okay。 We did that before。 Now
    I'm going to say， all right。
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将把那个州字典添加到列表中，最终将这个字典列表转换为数据框，这样我们就可以将数据框保存为CSV文件，或者如果愿意，可以直接保存为Excel文件。让我们看看实际操作。我们已经有了链接列表。好的。我们之前做过。现在我将说，好吧。
- en: I'm actually going to create this blank list that will house， all of my dictionaries。
    Then for every link that I have in my state URLs， well， at least for the first
    10 because。 I don't want to， I don't want this part to take too long。 For those
    first 10 states。 get the state info with my robust scraper。 That's the one that
    has those tries and accepts。
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 我实际上会创建这个空列表来存放我的所有字典。然后对于我在州URL中的每一个链接，至少对于前10个，因为我不想让这个部分花太多时间。对于这前10个州，使用我强大的抓取器获取州信息。就是那个包含尝试和接受的。
- en: I'm passing the link in there。 This state info will now be a dictionary。 I will
    append that dictionary to my list and then I will make sure to pause for at least。
    one second before I go scrape the information for the next state。 Let's try it。
    Should take at least 10 seconds， right？ Because I have those 10 artificial pauses
    in there。 Okay。
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 我会把链接放在那里。这个州的信息现在将是一个字典。我会把这个字典添加到我的列表中，然后我会确保在抓取下一个州的信息之前暂停至少一秒。我们试试吧。应该至少需要10秒，对吗？因为我在其中设置了10个人工暂停。好的。
- en: Now my state info list contains all of those different dictionaries and I was
    able to。 scrape that information for the first 10 states that are alphabetically
    listed in that table。 I'm going to convert this list of dictionaries into a data
    frame。 And I'll now save this to a CSV。 The only thing extra I'm doing here is
    just removing the indices。 So this is zero through nine。
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我的州信息列表包含了所有不同的字典，我能够抓取该表中按字母顺序排列的前10个州的信息。我将把这个字典列表转换为数据框。然后我会将其保存为CSV文件。我在这里唯一额外做的就是删除索引。所以这是从零到九。
- en: I'm just going to leave that off。 You can keep it if you'd like and you can
    download that。 Yeah。 if you get a warning， do you want to download this file？
    Really？ Yeah。 you can say really I do want this file。 And now if I actually open
    this up。 Let's see。 I'm going to have to。 Great。 So you should be seeing my Excel
    file。 If I open up that CSV。
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 我就不再提这个了。如果你想可以保留，并且可以下载。是的。如果你收到警告，想要下载这个文件？真的吗？是的。你可以说我确实想要这个文件。现在如果我真的打开这个文件。让我们看看。我必须。太好了。你应该能看到我的Excel文件。如果我打开这个CSV。
- en: now I'm right back into an Excel file and I can do any kind of。 analysis that
    I know how to do with Excel or what have you。 Great。 So， yeah。 so that's really
    the pipeline。 That's the idea。 We'll visit lots of different pages。 We'll grab
    that information if we'd like and store it in some kind of flat file or， you，
    know。
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我又回到了Excel文件中，我可以进行任何我知道如何在Excel中进行的分析。太好了。是的。这就是管道。就是这个想法。我们会访问许多不同的页面。如果愿意，我们会抓取那些信息并存储在某种平面文件中。
- en: we could just continue on。 Continue on analyzing this data frame。 If we know
    about pandas。 we can keep rolling， keep the party going right here in Google，
    Colab。 start analyzing this data frame。 One thing to do。 So if you are doing this
    at home and you do collect everything for all 50 states。 you'll， find that there
    is one missing value。 So the try and accepts did do something here。
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以继续分析这个数据框。如果我们了解pandas，我们可以继续进行，保持在Google Colab中的派对继续。开始分析这个数据框。有一件事情需要注意。如果你在家里进行这个操作，并且为所有50个州收集了所有信息，你会发现有一个值缺失。因此，尝试和接受确实在这里做了一些事情。
- en: Kansas' date admitted is missing。 So visit the page and try to figure out why
    this happened and how could you fix the issue。 that our scraper had as a bonus？
    One final thing I wanted to tell you about before we wrap with this section。 So
    our strategy was to find a page that just had all the links that we want to visit，
    scrape。 that page and then visit each link systematically。
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 堪萨斯州的日期信息缺失了。所以请访问页面，试着找出这发生的原因，以及你如何解决我们抓取器出现的问题？最后一件我想告诉你的是，在我们结束这一部分之前。我们的策略是找到一个包含所有我们想要访问的链接的页面，抓取这个页面，然后系统地访问每个链接。
- en: But sometimes you'll find that the pages you want to visit to scrape are named
    systematically。 Okay。 Let me show you what that looks like。 Does everybody remember
    our Billboard Hot 100 singles from the very beginning of the。 tutorial？ I was
    showing you how we have the most popular 100 songs from 2019 according to Billboard。
    If I wanted to。 Let's show you that real quick to refresh remember。
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 但有时你会发现你想访问的抓取页面是系统命名的。好吧，让我给你看看那是什么样子。大家还记得我们在教程开始时提到的公告牌百强单曲吗？我向你展示了2019年根据公告牌评选出的最受欢迎的100首歌曲。如果我想，快速向你展示一下以便回忆。
- en: If I wanted to gather the top song from 2019 and every year since 2010， how
    could I do， that？ Well。 it turns out that instead of looking for another list
    of all the Billboard charts， etc。 all I really have to do is change this last
    number。 Instead of 2019。 I could look up 2010 or 2011 or etc。 So sometimes the
    pages you want to visit have a specific pattern and we can exploit that。
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我想收集2019年及自2010年以来的热门歌曲，我该怎么做呢？结果是，实际上我只需更改最后的数字，而不是寻找其他的公告牌排行榜列表。因此，我可以查找2010年或2011年等。有时，你想访问的页面有特定的模式，我们可以利用这个。
- en: I'm going to collect the top hits in this list and I'll be ranging from 2010
    up to 2019。 So the page I'm trying to scrape looks exactly the same except for
    the final year is changing。 as I do this scraping。 I won't go through all the
    different how I am finding that top song。 You're welcome to take a look at this
    code on your own time。
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 我将收集这份名单中的热门歌曲，范围从2010年到2019年。因此，我尝试抓取的页面除了最后的年份在变化之外，其他部分完全相同。在这个抓取过程中，我不会详细说明我是如何找到那首热门歌曲的。你可以在自己的时间查看这段代码。
- en: I just wanted to show you that you can visit each of these links systematically
    by just。 taking advantage of that pattern。 And now that I'm done in top hits，
    I have the top。 the year I have the song name and， the page that links to that
    song。 So I gathered all of that by just using this code right here， this scraping
    code。 Okay。
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 我只是想向你展示，你可以通过利用这种模式系统地访问每个链接。现在我在热门歌曲中完成了，我有了年度、歌曲名和链接到该歌曲的页面。因此，我通过使用这段抓取代码收集了所有这些信息。好的。
- en: so that's another strategy。 If you can't find a list of all the links you want。
    maybe you can just exploit the URL， and change it with some kind of pattern。 And
    that's just going to be a for loop over all the different years they want to collect。
    Okay。 so that wraps up our scraping pipeline。 If I were going to give this tutorial
    at PyCon。
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 这是另一种策略。如果你找不到所有想要链接的列表，也许你可以利用URL并通过某种模式更改它。这将是对你想要收集的不同年份进行循环的过程。好吧，这就是我们抓取流程的总结。如果我打算在PyCon上进行这个教程。
- en: the plan was to have all of the attendees， work on a mini project for roughly
    30 minutes or so。 So for the mini projects that I had planned， the idea was to
    be able to have you create。 your own scraping project。 Follow the pipeline steps
    that we did for the states。 but collect information from other， pages on Wikipedia。
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 计划是让所有参与者花大约30分钟进行一个迷你项目。因此我计划的迷你项目的想法是让你创建自己的抓取项目。遵循我们为州所做的流程步骤，但从其他维基百科页面收集信息。
- en: So I'll show you what the mini project ideas are in just a second。 But a couple
    of tips before you start any web scraping project。 Start small and scale up。 Make
    sure that your code works on just one page。 Make sure that you're grabbing all
    the information you want。 Then start requesting information for more links。 And
    you might even try， you know。
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 我马上会告诉你这些迷你项目的想法，但在你开始任何网络抓取项目之前，有几个提示。先从小做起，然后逐步扩大。确保你的代码在一页上有效，确保你抓取到你想要的所有信息。然后开始请求更多链接的信息。你甚至可以尝试。
- en: if it works on one page， now let's try 10。 Did it work on all 10 of those？
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 如果它在一页上有效，现在我们来尝试10页。它在这10页上都有效吗？
- en: Is there some other issue you forgot to account for？ So start small。 then scale
    up because you don't want to be hitting someone's server too often。 especially
    if you're hitting it so often that you might get blocked。 The second tip is to
    think through the data storage before you scale up。 So for us。
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 你是否还有其他问题忘记考虑了？所以先从小做起，然后逐步扩大，因为你不想太频繁地访问某人的服务器，尤其是如果你频繁到可能被封锁。第二个提示是在扩展之前考虑数据存储。因此，对于我们来说。
- en: we use the list of dictionaries that might not be the route that you want to，
    take。 Maybe you want to print everything out to a CSV with a CSU writer。 That's
    fine too。 Safeguard against missing values。 It is so heartbreaking to be on the
    999th page out of 1000 and you hit an error and lose。 all of the data you've had
    so far。 So that includes using the try and accepts that I talked to you about。
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用的字典列表可能不是你想要的路径。也许你想把所有内容打印到一个CSV文件中，用一个CSV写入器。这也可以。要防止丢失值。在第1000页的第999页时遇到错误而失去所有数据是令人心碎的。因此，这包括使用我之前提到的try和accept。
- en: as well as doing， things like every once in a while， write your data to a file。
    So if I'm trying to scrape a thousand pages， maybe every 50 pages， I write that
    information。 to a file and then I keep scraping。 That way I've collected whatever
    information I've collected up to a certain point does get。 saved and I don't lose
    everything。 So that's another strategy you might add。 And then of course。
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 以及像每隔一段时间就将数据写入文件的事情。因此，如果我试图抓取一千个页面，可能每抓取50个页面就将信息写入文件，然后继续抓取。这样，我收集到的信息在某一点是会保存的，不会失去一切。这是你可以添加的另一种策略。当然。
- en: tip number four I think is the most important one。 Please do pause for at least
    one second when we're scraping Wikipedia。 Other websites will have different rules。
    So try to figure those rules out before you begin scraping。 Otherwise you can
    get blocked from sites。 Okay， so let's look at these many project ideas。
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 第四个小贴士我认为是最重要的。在抓取维基百科时，请至少暂停一秒钟。其他网站会有不同的规则。因此，在开始抓取之前，尝试弄清楚这些规则。否则，你可能会被网站封锁。好的，让我们看看这些项目想法。
- en: This file is also hosted on my GitHub。 And here we go。 So it's a markdown file。
    It basically walks you through the directions。 The idea behind these directions
    is just let's try to scrape information from other Wikipedia。 pages and allow
    you to develop your own strategies for data collection and data storage。 Then
    once you have some data， go ahead and use any tool that you know how to use to
    conduct。
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 这个文件也托管在我的GitHub上。好了，这是一个Markdown文件。它基本上引导你了解方向。这些方向背后的想法是尝试从其他维基百科页面抓取信息，并让你发展自己的数据收集和存储策略。一旦你有了一些数据，尽管使用你会用到的任何工具来进行。
- en: some kind of analysis。 So maybe you do know more about pandas and you can continue
    working with your data frame。 there。 Or you know about Excel or Tableau。 Work
    with the data that you've collected。 This is your prize。 Okay。 So I have some
    project ideas and you're welcome to come up with your own to practice your。 web
    scraping skills。 But here are a couple of different ideas of things that you could
    try。
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 某种分析。因此，也许你更了解Pandas，可以继续处理你的数据框。或者你知道Excel或Tableau。处理你收集的数据。这是你的奖品。好吧。我有一些项目想法，你也可以提出自己的想法来练习你的网页抓取技能。但这里有几个不同的想法你可以尝试。
- en: I'll just walk you through the first one， but you're welcome to take a look
    at any of。 these eight ideas。 The first one is saying starting from this list
    of tea drinking countries。 what factors， if any， about a country correlate with
    its annual tea consumption per capita。 So let's look at this list。 If I look at
    this Wikipedia page。
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 我会带你走过第一个，但你可以查看这八个想法中的任何一个。第一个是从这份饮茶国家名单开始，是否有任何关于一个国家的因素与其人均年度茶消费相关。因此，让我们看看这个列表。如果我查看这个维基百科页面。
- en: what I see are the various countries ranked in order。 of those that drink the
    most tea per capita down to number 50， which drinks less tea per， capita。 So the
    idea here is that you will collect the pages for each of these countries and then。
    visit those to collect more information about the country。
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 我看到的是各国按人均饮茶量排名，从人均饮茶最多的国家到第50名，后者饮茶量较少。因此，这里的想法是你将收集这些国家的页面，然后访问它们以收集更多关于该国的信息。
- en: Maybe it's latitude longitude or you know， it's area or any of these kind of
    things that。 you think might be useful。 And you're going to try to see if any
    of the things that you can collect about that country。 correlate with its tea
    consumption per capita per year。 So a simple exercise， you know， fun。 just simple
    analysis， but getting the actual data， for this project is a full， you know。
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 也许是经纬度，或者你知道的面积，或者你认为可能有用的任何这些东西。你将尝试查看你能收集到的关于该国的任何东西是否与其每年的人均茶消费相关。因此，这是一个简单的练习，乐趣。简单的分析，但为这个项目获取实际数据是一个完整的，嗯。
- en: web scraping process。 So the idea behind these many projects is really to just
    to get you practicing your。 web scraping。 I said it at the， you know， a little
    while ago， but I will definitely reiterate it。 The way that you're really going
    to learn web scraping is by trying a project like this。 trying to develop strategies
    of how can I collect the information I want and then save。
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 网页抓取过程。所以这些项目背后的想法其实是让你练习网页抓取。我之前提到过这一点，但我会再次强调。你真正学习网页抓取的方法是尝试这样的项目，努力开发如何收集我想要的信息并保存。
- en: it so that I can use it in a downstream analysis。 So if you do happen to dedicate
    some time to one of these many projects and if you get。 any kind of results at
    all， I would love to know about that。 You can， you know。 let us know on Twitter
    or LinkedIn， contact me hashtag， type on 2020， any of those kind of things。 I
    would love to know if you do actually put some time toward one of these many projects。
    Okay。
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 这样我就可以在后续分析中使用它。所以如果你确实花了一些时间在这些项目上，并且得到了任何结果，我很想知道。你可以在Twitter或LinkedIn上告诉我们，或者联系我，使用#typeon2020之类的标签。如果你真的投入了一些时间到这些项目中，我会很高兴知道。好的。
- en: so that was it for our scraping pipeline。 So in the scraping pipeline section。
    you learned how to retrieve and prepare data。 We learned quite a few different
    steps that can help the web scraping pipeline for your。 own web scraping projects。
    You learned about that library called requests。 This is the library that we use
    to request HTML code from web pages。
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们的抓取管道部分。在抓取管道部分，你学会了如何检索和准备数据。我们学习了几种可以帮助你自己网页抓取项目的抓取管道的不同步骤。你了解了一个叫requests的库。这是我们用来请求网页HTML代码的库。
- en: You learned how to chain commands together。 So sometimes the most effective
    strategy is to look for larger elements and then drill。 down further。 And you
    can do that by chaining find and find commands one after another。 You learned
    how to locate information by position。 So that included locating information by
    text by looking for nearby text and then using。
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 你学会了如何将命令串联在一起。有时候，最有效的策略是寻找较大的元素，然后进一步深入。你可以通过一个接一个地链式使用find和find命令来做到这一点。你学会了如何按位置定位信息，包括通过寻找附近的文本来定位信息。
- en: the DOM to traverse to other elements by using 。next or 。parent or that child，
    etc。 You learned quite a bit about the data cleaning and storage process。 And
    so this may not be the most glamorous part about web scraping， but definitely
    is。 essential if you do want to take those strings and then do some meaningful
    analysis。
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 使用DOM遍历其他元素，使用.next或.parent或.child等。你学到了关于数据清理和存储过程的很多知识。虽然这可能不是网页抓取中最光鲜的部分，但如果你希望对这些字符串进行有意义的分析，这绝对是必要的。
- en: And we talked about how you could systematically visit multiple web pages。 So
    we had two different ways to do this and it's really going to depend on your unique，
    case。 Either you'll find a page that has a list of links that you can collect
    and then visit。 each of those links or maybe the links that you want to visit
    have some kind of systematic。
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 我们讨论了如何系统性地访问多个网页。我们有两种不同的方法，这确实取决于你的具体情况。要么你找到一个有链接列表的页面，然后访问每个链接，要么你想要访问的链接有某种系统化的命名方式。
- en: way that the pages are named。 And finally we talked about how you could do a
    full web scraping pipeline for a mini project。 So congratulations。 That's all
    three second sections of this tutorial from HTML basics to scraping basics and
    finally。 to the scraping pipeline。 I just want to wrap with a few final thoughts
    so that you can continue on in your web scraping。 journey。 The first thought and
    maybe this is the most important thing。
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 最后我们讨论了如何为一个小项目建立完整的网页抓取管道。恭喜你。这是本教程的三个部分，从HTML基础到抓取基础，最后到抓取管道。我想以几个最后的想法结束，以便你能继续你的网页抓取之旅。第一个想法，也许这是最重要的。
- en: please do be responsible when， you are web scraping。 So I talked about you could
    consult this file called robots。txt。 So where do you find this file？
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 请在进行网页抓取时务必负责。所以我提到你可以查看名为robots.txt的文件。那这个文件在哪里找到呢？
- en: For the various websites that you're going to be web scraping， if you visit
    the websites， address。 forward slash robots。txt， you should see a file like this。
    So this is the one from Wikipedia。 Wikipedia is robots。txt file is quite long。
    But what you can see here is that they have various rules for various different
    robots。 scrapers。 So you'll see user agent。 That means this rule applies to a
    specific type of scraper。
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 对于你将要进行网页抓取的各种网站，如果你访问这些网站的 `robots.txt` 文件，你应该能看到类似这样的文件。这是来自维基百科的文件。维基百科的
    `robots.txt` 文件相当长。但你可以看到，他们为不同的爬虫设定了各种规则。所以你会看到用户代理，这意味着此规则适用于特定类型的爬虫。
- en: Eventually if you kind of scroll down relatively far on this robots。txt file，
    you'll see user。 agent star。 This means that whatever comes next applies to you。
    So you are any other kind of user agent that wasn't mentioned explicitly by name。
    So there's a couple of places that Wikipedia would like you to not scrape。
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，如果你向下滚动到这个 `robots.txt` 文件的相对较远的地方，你会看到 `user-agent *`。这意味着接下来所说的内容适用于你。也就是说，适用于任何没有明确提及名称的其他用户代理。所以维基百科有几个地方希望你不要进行抓取。
- en: So you should do your best to avoid those things。 So you can check this robots。txt
    file for any site that you'd like to scrape to see what， their rules are for scrapers。
    The other thing that you should definitely do is add regular pauses to your scraper。
    Python and requests in beautiful soup， they all work really quickly， but sometimes
    too， quickly。
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，你应该尽力避免这些内容。你可以检查任何想要抓取的网站的 `robots.txt` 文件，以了解他们对爬虫的规则。你绝对应该做的另一件事是为你的爬虫添加定期的暂停。Python
    和 requests 以及 Beautiful Soup 都运行得非常快，但有时也太快了。
- en: So especially requests you don't necessarily want to be requesting lots and
    lots of HTML。 code from one particular website。 You could overwhelm that website's
    servers and you just want to avoid that kind of irresponsibility。 So add regular
    pauses as you are scraping through。 And that might look like those systematic
    two second pauses or one second pauses per， request。
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 所以特别是请求时，你不一定希望从某个特定网站请求大量的 HTML 代码。你可能会让该网站的服务器不堪重负，而且你要避免这种不负责任的行为。因此，在抓取时添加定期的暂停。可能看起来像是每次请求的系统性两秒或一秒的暂停。
- en: Or if you want to make your requests look a little bit more human-like， you
    could add。 some kind of random component to that pause。 So the second recommendation
    about making the random pause is more about trying not to get。 detected as being
    a robot。 So that is one strategy is to make your requests look a little bit more
    human-like with some。 kind of random pausing。 And definitely try to avoid losing
    work。 It's really。
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 或者如果你想让你的请求看起来更像人类，你可以添加某种随机的暂停成分。因此，关于添加随机暂停的第二个建议主要是为了避免被检测为机器人。这是让你的请求看起来更像人类的一种策略，增加一些随机暂停。同时，一定要尽量避免丢失工作。这真的很悲伤。
- en: really sad when you've gotten through 900 pages and then on that next page。
    you have some kind of error。 So you should always assume that your connection
    is going to fail。 You should always assume that there might be some kind of missing
    data on that next web， page。 So having to try and accept will definitely help
    with missing information。
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 当你翻阅了 900 页后，下一页出现某种错误时真的很令人沮丧。因此，你应该始终假设你的连接会失败。你应该始终假设在下一网页上可能会有一些缺失的数据。所以尝试接受这些情况将有助于处理缺失信息。
- en: And also I would recommend saving data frequently。 So maybe after every 50th
    page or so you could either write what data you have so far to a。 CSV or you can
    use something called a pickle。 I won't go into the full details about this but
    basically the idea is that you can serialize。 any kind of Python object and save
    it to a file。 So that later you can reload that object in Python and continue
    to work with it。
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 我还建议你定期保存数据。因此，可能在每 50 页后，你可以将迄今为止拥有的数据写入一个 CSV 文件，或者使用一种叫做 pickle 的方法。我不会详细讲解这个，但基本思想是你可以将任何
    Python 对象序列化并保存到文件中。这样你可以稍后在 Python 中重新加载该对象并继续使用它。
- en: So here's some example code。 It's just with a library called pickle and you
    can basically save a Python object to a。 file and then later load that information。
    So this is a good strategy。 If you do have let's say a list of dictionaries， if
    you say that is a pickle and then reload， it。 it will still appear as a list of
    dictionaries within Python。
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一些示例代码。它使用一个叫做 pickle 的库，你基本上可以将 Python 对象保存到文件中，然后稍后加载该信息。所以如果你有一个字典列表，比如说这就是一个
    pickle，然后重新加载，它在 Python 中仍然会作为字典列表出现。
- en: So let's review what did you learn today quite a bit actually。 We started all
    the way at the beginning by deciphering a basic HTML so you've come quite。 a long
    way。 We talked about how to retrieve information from the internet with requests
    and how to。 parse that data with beautiful soup。 Finally， we took a pretty long
    look at how you could build out a web scraping pipeline。
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 那么让我们回顾一下今天你学到了什么，实际上还挺多的。我们从解码基本的HTML开始，所以你已经走了很长一段路。我们讨论了如何使用请求从互联网检索信息，以及如何使用美丽汤解析这些数据。最后，我们详细查看了如何构建网页抓取管道。
- en: and how to gather data and prepare it in a systematic way。 Where do you go from
    here？
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 以及如何系统地收集数据并准备数据。接下来你要去哪里呢？
- en: We'll certainly working on those mini projects would be a good idea。 But I want
    to tell you about one more thing。 So sometimes when you are scraping with requests
    and beautiful soup。 you'll find that the， information you thought you saw on the
    web page is not there。 And sometimes information is what's called dynamically
    loaded or dynamically generated。
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 我们肯定会认为这些小项目是个好主意。但我还想告诉你一件事。有时当你使用请求和美丽汤进行抓取时，你会发现你以为在网页上看到的信息并不存在。有时信息是所谓的动态加载或动态生成的。
- en: This basically means that the website needs to pull from some kind of database
    before。 it can fill in various values。 And if that's the case。 if it's using JavaScript
    and using some kind of dynamic element， then。 just using the basic request followed
    with beautiful soup will not allow you to access。
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 这基本上意味着网站需要从某种数据库中提取数据，然后才能填入各种值。如果是这样的话，如果它使用JavaScript并使用某种动态元素，那么仅仅使用基本请求加上美丽汤将无法让你访问该信息。
- en: that information。 So you need an extra tool to address this。 The tool that most
    people use to get around to this sort of dynamic generation of content。 is called
    Selenium。 I certainly won't go into the Selenium details at this time。 but if
    you want a quick demo， on what that would look like。
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 所以你需要一个额外的工具来解决这个问题。大多数人用来应对这种动态内容生成的工具叫做Selenium。我当然不会在这个时候深入Selenium的细节，但如果你想快速演示一下，它会是什么样子。
- en: you could still use Python code except for now we're actually。 going to use
    something called Selenium。 Here I'm doing a YouTube query but really it could
    be anything with a dynamic component。 I'm using Selenium to actually control my
    browser。 So there I launched the window。 I can do things like scrolling within
    the window。 And I can do things like editing text in certain input fields。
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 你仍然可以使用Python代码，不过现在我们将使用一种叫做Selenium的工具。在这里我正在做一个YouTube查询，但实际上它可以是任何具有动态组件的东西。我正在使用Selenium来控制我的浏览器。所以我启动了窗口。我可以在窗口内滚动，也可以在某些输入字段中编辑文本。
- en: Now we're going to search for web scraping。 We can even do things like clicking
    on various buttons like here I'm going to filter down。 to this week's videos。
    And eventually once you've loaded the content that you actually want。 then you
    could parse， the page source of that driver with beautiful soup。 And you could
    actually get the information that has been loaded into that page。
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们要搜索网页抓取。我们甚至可以做一些事情，比如点击各种按钮，比如我这里要筛选出本周的视频。一旦你加载了实际想要的内容，就可以使用美丽汤解析该驱动的页面源代码。你实际上可以获取已加载到该页面的信息。
- en: It's just a matter of you would need to use Selenium to first load that content
    and then。 parse the page with beautiful soup。 So that's something to look into
    if you do continue with your web scraping journey。 Of course working on the many
    projects， I do think like practicing your web scraping。 will go a long way into
    understanding this sort of content。
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要使用Selenium首先加载内容，然后用美丽汤解析页面。所以如果你继续你的网页抓取之旅，这值得深入了解。当然，进行多个项目，我确实认为练习你的网页抓取技能将对理解这类内容大有裨益。
- en: There's a lot of interesting things that can happen depending on what page you're
    scraping。 and you really won't know that until you try it out yourself。 Eventually，
    as the skill is honed。 you can fuel larger projects with scraped data。 So I just
    wanted to give you a couple of examples of projects that meta students have done
    using。 scraped data。 Here's one on obtaining insights from NBA data。
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 根据你抓取的页面，可能会发生很多有趣的事情，而你只有自己尝试后才能真正了解。最终，随着技能的提升，你可以利用抓取的数据推动更大的项目。所以我想给你一些关于元学生使用抓取数据完成的项目的例子。这里有一个关于获取NBA数据洞察的项目。
- en: And you'll see within this blog that one of our students wrote， he says that
    the box。 scores I obtain were scraped from basketballreference。com。 Here's one
    where someone scraped information from SF gate。com or the SCC's submission system。
    So the thing that all of these have in common is that web scraping is just part
    of the whole。
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 你会看到在这个博客中，我们的一位学生写道，他说我获得的比赛数据是从basketballreference.com抓取的。这里有一个从SF gate.com或SCC的提交系统抓取的信息。所以这些都有一个共同点，那就是网络抓取只是整个过程的一部分。
- en: analysis。 Once you have that data， you're able to do things like machine learning
    or visualizations。 or et cetera。 So I hope that you do find all of this information
    super empowering because this is where you。 can head is to be really large-scale，
    really cool projects。 So thank you so much for joining me today。 I would definitely
    be happy to chat further if you have any further questions。
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 分析。一旦你有了这些数据，你就能够进行机器学习或可视化等操作。所以我希望你能发现这些信息非常赋能，因为这就是你可以迈向的方向——真正的大规模、非常酷的项目。非常感谢你今天加入我。如果你还有其他问题，我会很乐意进一步交谈。
- en: Definitely reach out。 Happy to answer those。 I'm also happy to connect。 Just
    let me know that you saw this PyCon 2020 tutorial。 And thanks so much for stopping
    in。
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 当然可以联系我。很高兴回答这些问题。我也乐于建立联系。只需告诉我您看到这个PyCon 2020教程。非常感谢您过来。
- en: '![](img/021740aa4daa35fb40acaf4dbe65ca57_4.png)'
  id: totrans-390
  prefs: []
  type: TYPE_IMG
  zh: '![](img/021740aa4daa35fb40acaf4dbe65ca57_4.png)'
- en: '[BLANK_AUDIO]。'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: '[空白音频]。'
- en: '![](img/021740aa4daa35fb40acaf4dbe65ca57_6.png)'
  id: totrans-392
  prefs: []
  type: TYPE_IMG
  zh: '![](img/021740aa4daa35fb40acaf4dbe65ca57_6.png)'
