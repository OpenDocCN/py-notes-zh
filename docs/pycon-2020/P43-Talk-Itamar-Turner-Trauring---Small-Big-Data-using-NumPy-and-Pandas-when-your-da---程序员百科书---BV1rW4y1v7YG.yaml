- en: P43：Talk Itamar Turner-Trauring - Small Big Data using NumPy and Pandas when
    your da - 程序员百科书 - BV1rW4y1v7YG
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](img/a073c113a15db82d1183adf308586723_0.png)'
  id: totrans-1
  prefs: []
  type: TYPE_IMG
- en: 嗨，我叫埃玛拉特纳，今天我要讲的是小的大数据，当您的数据不适合内存时 该怎么办，在我们开始之前，我想谈谈任何软件项目中最重要的问题，到底该不该写，如果你的代码伤害了人们，如果您的代码正在破坏环境。你就越好，你越有效，你的代码越快，效率越高，你造成的伤害就越大，所以在你开始你的项目之前，在你开始写代码之前，确保这实际上是值得编写的代码，在我接下来的演讲中，我假设你已经做出了决定。所以我们今天要讨论的问题是，当你有太多的数据时，假设有一些数据的
    csv，可能是一个阵列，你用一些少量的数据来尝试它，一切都很好，但是当你用真实的数据加载数据时，是2G，10G，2G，你的程序崩溃了。问题是如果你只有16G的内存，这是我们想要解决的问题，所以一个解决方案是大数据集群，你需要一组计算机，你得安排一下，你得重写所有的代码，这是夸大其词，我不公平，你可以得到星团云，云中的计算机集群等。
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: 但是从你的正常代码，在计算机上运行到一个大集群 有点不和谐，它需要哭泣者的努力，理想情况下，我们希望避免它，尤其是，我要把重点放在一些假设上，你只有一台安装最少的电脑，理想情况下。可以使用现有的api现有代码，基本上你只是用普通的电脑处理大量数据，它的术语是小的大数据，因为艾利克斯·沃斯，伊利沃夫和约翰刘易斯，他们有一个小的大数据宣言链接在这里，当你读到你能听到的幻灯片时。所以在我们继续解决数据过多的问题之前，你可能会想，为什么我们要把数据加载到内存中，我们有一个硬盘，我们的数据在硬盘上，你可以在硬盘上读写，所以为什么不直接对磁盘进行读写呢，以及旁路活塞。原因是你的硬盘即使是更现代的，快得多，SSDS仍然比你的内存慢得多，所以从
    ssd读取数据可能需要16000纳秒，从记忆中读取100纳秒，所以如果你只处理磁盘上的数据，你可以做到这一点。
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: 你的代码会慢一百六十倍，如果你不介意的话，但大多数时候运行我们的代码要慢一百六十倍，不是我们想要的，所以如果你想让你的计算，快点，你得把数据放进内存，你得把一些数据加载到内存里。所以一个解决办法是花更多的钱，你可以花更多的钱，多拿点公羊来，所以你可以去买你的电脑，我随便查了一下价格，价格可能会不一样，你可能会花比这更少的钱，这只是例子，但是你可以买一个工作站
    66个核心。64G RAM售价千元，你还可以租一台电脑，在云中得到一台虚拟机，例如，你可以得到一台有64个内核的机器，四百三十二千兆字节的内存大约三美元，每小时六角，再一次，你实际上可能会支付比这更少的钱。所以在很多情况下，因为你的时间，要花钱，你可以在这个问题上扔钱，通过买一台电脑，租一台电脑，运行您的代码，多拿点公羊来，一切都很好，你不需要更改我们的代码，所以值得记住的是，确实存在一个简单的解决方案。
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: 问题，虽然，这种花钱的方法并不总是适用，例如，我有一次在某个工作中，你在做大量的批量工作，我计算了一下 你在计算成本上花了多少钱，关于租用虚拟机，你花在计算机上的钱，基本上我们的预期收入。这样我们就没有钱去做任何事情了，例如，我的薪水，我非常关心，我需要降低我们使用的虚拟机的成本，这意味着我需要减少
    cpu的使用，但我也不得不减少相当多的内存使用量，所以在很多情况下。花更多的钱不是你想采取的解决办法，你想改变你的软件来使用更少的内存，在接下来的演讲中 我将重点讨论，特别是我将重点介绍三种技巧，压缩数据，分块处理数据，所以加载一点点处理，再装一点，处理得更多。最后索引你的数据，所以只加载实际需要和不需要的数据，不会占用任何记忆，所以在接下来的谈话中，我要把重点放在这些技巧上，特别是我要讲的两个图书馆，这些技术的实现并非详尽无遗，在演讲的最后。
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: 我要链接到我的网站，我在那里有一些文章对这次演讲进行了扩展，我讲了一些我在这里没有提到的技巧，只是时间有限，值得记住的是，基本技术适用于所有的领域，因为它们来自于你如何处理，数据，关于硬件的性质。算法是如何工作的，所以其他的图书馆，其他系统将使用相同的技术，所以即使你不使用它们，这些技术将是有用的，最后，我说的技术是相当通用的，但如果你有具体的数据，你可能会想出你自己的定制变体，所以嗯。如果你有某种类型的数据可以很好地压缩，还有你，因为最了解数据的人将能够提出这些变化，我们将从压缩开始，压缩的原理是把一些数据，你用一种不同于通常的方式来表达它，所以它使用更少的内存。你可以用一种无损的方式做到这一点，其中压缩的数据与原始数据相同，或者你可以在任何你失去一些细节的地方做，如果你失去了一些细节，你会尽量忽略那些无关紧要的细节，我要说的是失去，压缩更少。
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: 但如果你去我的网站，有篇文章说诉讼压缩，我想澄清一下 当我谈到压缩时，你可能会想到 zip文件或者 gzip文件，它被压缩在磁盘上，zip文件的问题是为了处理数据，你得解压数据，一旦它进入记忆。不再压缩，所以我不是在说那种事情，对磁盘进行压缩，我说的是内存压缩，所以让我们从麻木开始，麻木，如果你不熟悉，它是一个库，允许您存储多维数组，一维，二维，三维，每个数组都有一个类型，数据类型。你可以存储整数，你可以漂浮星星浮子，你可以，当你存储一个整数，有各种各样的整数类型，你可以存储一个16位的无符号整数，它允许你存储0到600之间的值，五万五千，etc。或者你可以有一个64位的无符号整数，然后它可以存储零到一个非常大的数字之间的数据，是64减去1的2次方
    也就是64位整数，让你代表更大的数字，但正如你所料，它们使用的内存也是16位整数的四倍。
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: 让我们看一个例子，呃，我们要创建一个数组，我们要创建两个数组，会有很多人，它是一个二维阵列，一千二十四乘一千二十四，第一个数组使用64位整数，第二个数组使用16位整数。我们可以看看每个数组使用多少字节的内存，第一个使用了大约八兆字节的内存，第二个使用了2兆字节的内存，如你所料，使用64位整数所消耗的内存是16位整数的四倍，所以如果你的数据符合16位数组。如果你们的人数少于六万五千，你应该用一个没有符号的16位
    d型，你只会减少你的记忆，在这种情况下是四倍，另一种压缩方法，它只适用于某些类型的数据，是稀疏数组，所以假设你有一个大部分是零的数组。如果你的大部分数据都是零，在内存中存储所有这些零
    有点浪费时间，你可以存储那些不是零的值，然后说，如果我们没有这方面的信息，假设它是零，这个特别的图书馆，圆周率数据稀疏库，实现稀疏数组。
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: 它的实现方式是在麻木的数组中进行交互操作，它支持同一个 api的子集，你可以把它们相乘，它支持不同的表示，在这种情况下，我将向你们展示一个使用坐标的例子，稀疏坐标，我们的想法是让我们假设一个大的阵列。假设它显示了天空中恒星的位置，所以在天空的大部分地方你会看到黑暗，不是很好的望远镜，在一些地方，到处都有星星，所以你真的不需要开始，您不必存储整个数组值，你只需要储存几个有明星的地方。基本上你要储存在这个和弦上，x
    y坐标，有一个像素或数组值，你想到的事情，这个值用来模拟，我们创建了一个由一千零二四乘一千零二四组成的随机数组，基本上随机值在0和1之间，任何低于零点九的值。我要设置为零，所以基本上90%的像素都是黑色的，在我们的二维射线二维图像中，然后我们就可以让我们的牧马人麻木，数组，并从中创建一个基于稀疏坐标的数组，如果我们比较一下。
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 它的实现方式是在麻木的数组中进行交互操作，它支持同一个 API 的子集。你可以把它们相乘，它支持不同的表示，在这种情况下，我将向你们展示一个使用坐标的例子，稀疏坐标。我们的想法是让我们假设一个大的阵列。假设它显示了天空中恒星的位置，所以在天空的大部分地方你会看到黑暗，不是很好的望远镜，在一些地方，到处都有星星，所以你真的不需要存储整个数组值，你只需要储存几个有星星的地方。基本上，你要储存在这个和弦上，x
    y 坐标，有一个像素或数组值，这个值用来模拟。我们创建了一个由 1024 乘 1024 组成的随机数组，基本上随机值在 0 和 1 之间，任何低于 0.9
    的值我都要设置为零。所以基本上 90% 的像素都是黑色的，在我们的二维图像中，然后我们就可以让我们的麻木数组，并从中创建一个基于稀疏坐标的数组。
- en: 稀疏数组使用的内存大约是常规数组的三分之一，不仅仅是，它不能仅仅因为它必须存储所有的 x，y坐标，有一些开销，所以这就是为什么它对很多人都不起作用，如果你的阵列里有很多很多的点，但只要点的数量足够少。号码，数组的内存使用量会很低，它可以很好地与普通的，Numpy数组，做同样的手术，接下来，类似于电子表格或
    csv中的数据，基本上我们有数据列，每列都有一个名字和一个叫法，每列都有一个类型。所以你可能有一个地址，那是个专栏，是一根绳子，然后说一个全球定位系统，坐标
    x，Y，然后那两根柱子，数组类型，列类型，是整数还是浮点数，就像在麻木，您可以为每个列指定 d类型，默认情况下。它会加载数据并猜测，它是什么样的数据类型，数字也是如此，如果它搞不清楚，如果它决定是一个数字，它将使用64位的每一个条目，但如果我们知道每一个特定列中的所有值，小于，假设一万六千。
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 稀疏数组使用的内存大约是常规数组的三分之一，不仅仅是，它不能仅仅因为它必须存储所有的 x，y 坐标，有一些开销。这就是为什么它对很多人都不起作用，如果你的阵列里有很多很多的点，但只要点的数量足够少，数组的内存使用量会很低。它可以很好地与普通的
    Numpy 数组，做同样的手术。接下来，类似于电子表格或 csv 中的数据，基本上我们有数据列，每列都有一个名字和一个类型。每列都有一个类型，所以你可能有一个地址，那是个专栏，是一根绳子，然后说一个全球定位系统，坐标
    x，y，然后那两根柱子，数组类型，列类型，是整数还是浮点数，就像在麻木，你可以为每个列指定 d 类型。默认情况下，它会加载数据并猜测它是什么样的数据类型，数字也是如此。如果它搞不清楚，如果它决定是一个数字，它将使用
    64 位的每一个条目，但如果我们知道每一个特定列中的所有值，小于，假设一万六千。
- en: 我们可以用一个16位的整数，所以不用64位的整数，对于列中的每个条目，内存为64位，对于列中的每个条目，我们使用了16位内存，因此，我们再次将这篇专栏文章的内存使用量减少了4个，它存储着完全相同的数据。所以它失去了更少的压力，我们没有失去任何东西，所以这就是压缩，接下来我们要继续切块，和大块，是一种让你，减少内存使用，而无需将所有内容同时加载到内存中，在压缩中，我们确实在所有东西上都装了块。我们是说也许我们不需要把所有的东西都装上，考虑这个问题的方法是考虑一个数组，它上面有一堆值，我们要找到最大值，所以我们可以做的一件事就是把整个事情加载到内存中，遍历所有的值，找到最大值，上下半场。下半场拿到最大值，整个数组的最大值就是这两个值的最大值，所以我们得到了相同的答案，整个阵列的最大值，我们只需要一次加载一半的数据，我们可以对季度数据做同样的事情，所以基本上我们只需要把，对大块进行操作。
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你看一个沙皇的物体，我们可以用一个 16 位的整数，所以不用 64 位的整数。对于列中的每个条目，内存为 64 位，而对于列中的每个条目，我们使用了
    16 位内存。因此，我们再次将这篇专栏文章的内存使用量减少了 4 倍，它存储着完全相同的数据。所以它失去了更少的压力，我们没有失去任何东西，这就是压缩。接下来我们要继续切块和大块，是一种让你减少内存使用，而无需将所有内容同时加载到内存中。在压缩中，我们确实在所有东西上都装了块。我们是说也许我们不需要把所有的东西都装上，考虑这个问题的方法是考虑一个数组，它上面有一堆值。我们要找到最大值，所以我们可以做的一件事就是把整个事情加载到内存中，遍历所有的值，找到最大值。上下半场拿到最大值，整个数组的最大值就是这两个值的最大值。所以我们得到了相同的答案，整个阵列的最大值，我们只需要一次加载一半的数据，我们可以对季度数据做同样的事情，所以基本上我们只需要对大块进行操作。
- en: 然后我们把结果结合起来，说到麻木，您需要一种将数据存储在磁盘上的方法，因此每次只能加载数据的子集，您只需要能够加载一个数据块，不是整个阵列，一种方法是用沙皇，呃，TZAR是一种将数据存储在磁盘上的方法。它允许您，呃，设置块大小和存储在磁盘上，然后当你把一块或两块一起加载到内存中，或者不管有多少，它变成了一个麻木的阵列，当你把这个数组写回沙皇的时候，它被写回适当的块。但是您不必仅仅为了处理其中的一部分而加载整个数组，所以在这个例子中，我们要打开一个沙皇，我们基本上是从这个一维数组中加载的，它有一百万个条目，我们一次只装一千件物品，我们在每一块中分别运行最大值。一旦我们有了那块，所有这些块的最大值，我们可以做整体最大值，所有那些大块，这是整个阵列的最大值，所以我们可以计算出整个阵列的最大值，而不需要加载整个阵列，如果你看一个沙皇的物体。
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们把结果结合起来。说到麻木，你需要一种将数据存储在磁盘上的方法，因此每次只能加载数据的子集。你只需要能够加载一个数据块，而不是整个阵列。一种方法是用沙皇，呃，TZAR
    是一种将数据存储在磁盘上的方法。它允许你设置块大小并存储在磁盘上，然后当你把一块或两块一起加载到内存中，或者不管有多少，它变成了一个麻木的阵列。当你把这个数组写回沙皇的时候，它被写回适当的块。但是你不必仅仅为了处理其中的一部分而加载整个数组，所以在这个例子中，我们要打开一个沙皇，我们基本上是从这个一维数组中加载的，它有一百万个条目，我们一次只装一千件物品。我们在每一块中分别运行最大值。一旦我们有了那块，所有这些块的最大值，我们可以做整体最大值，所有那些大块，这是整个阵列的最大值，所以我们可以计算出整个阵列的最大值，而不需要加载整个阵列。
- en: 我们已经打开了 它的类型只是一个沙皇的物体，当你打开沙皇的时候，它实际上并没有装载任何东西，只是指向磁盘上某个地方的指针，只有当你把它切成薄片的时候，只有当你得到它的潜艇时，它会被载入内存吗。你开始使用记忆，我们不必使用第三方图书馆，它实际上有一个内置的功能，例如，如果我们想提前加载一个
    csv，我们立刻把整个 csv重新加载到内存中，我们能做的是，而不是装载整个 csv。我想加载 ccn块，在这种情况下，我们要加载100行，当我们这样做的时候，一次100张唱片，当我们得到一个数据帧迭代器时，每个数据帧都是csv的100行，等等，如果你想计算最大值。我们只要计算出这一块的最大值，然后将其与整体最大值进行比较，我们计算了这一列的最大值，无需将整个
    csv加载到内存中，一次只加载了100行，内存使用量非常低，最后我要讲的技术是索引，所以当你考虑索引时。
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经打开了它的类型，这只是一个沙皇的物体。当你打开沙皇的时候，它实际上并没有装载任何东西，只是指向磁盘上某个地方的指针。只有当你把它切成薄片，或者当你得到它的潜艇时，它才会被载入内存。你开始使用记忆，我们不必使用第三方图书馆，它实际上有一个内置的功能。例如，如果我们想提前加载一个
    csv，我们可以立刻将整个 csv 重新加载到内存中。我们能做的是，不是装载整个 csv。我想加载 ccn 块，在这种情况下，我们要加载 100 行。当我们这样做的时候，一次
    100 张唱片，当我们得到一个数据帧迭代器时，每个数据帧都是 csv 的 100 行。如果你想计算最大值，我们只需计算这一块的最大值，然后将其与整体最大值进行比较。我们计算了这一列的最大值，无需将整个
    csv 加载到内存中，一次只加载 100 行，内存使用量非常低。最后我要讲的技术是索引，当你考虑索引时。
- en: 想想书的背面，就像你有一本参考书，一本历史书，你想知道发生的一切与书中的一切有关，与滑铁卢战役有关，所以你去书的后面，而且是按字母顺序排列的，所以你很容易就能找到更好的滑铁卢。它告诉你在两百页上提到了滑铁卢的面糊，所以你翻了一页两百，你看了那一页，现在你知道了，这本书要告诉你的一切，滑铁卢战役，我们还没读完整本书，我们只需要看一下索引，它比这本书小得多
    只是一个总结。书中关于数据的部分，我们关心那是一个索引，索引可以让我们得到数据的子集，这和你喜欢的分块是不一样的，他们在某些方面设置了分块和索引，他们可能看起来有点相似，因为在这两种情况下，不同的是。分块是你想用的时候用的，你需要阅读所有的数据，你无法避免，你无法避免，你只是不想一次装上所有的东西，所以如果你的问题是这本书里最长的单词是什么，你知道的唯一方法就是阅读书中的每一页，你不能只看两百页。
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 想想书的背面，就像你有一本参考书，一本历史书。你想知道发生的一切与书中的一切有关，与滑铁卢战役有关，所以你去书的后面，而它是按字母顺序排列的，这样你就能容易找到滑铁卢。它告诉你在两百页上提到了滑铁卢的面糊，所以你翻了一页两百，你看了那一页，现在你知道了，这本书要告诉你的一切，滑铁卢战役。我们还没读完整本书，我们只需要看一下索引，它比这本书小得多，只是一个总结。书中关于数据的部分，我们关心的是一个索引，索引可以让我们得到数据的子集。这和你喜欢的分块是不一样的，他们在某些方面设置了分块和索引，可能看起来有点相似，因为在这两种情况下，不同的是，分块是你想用的时候用的。你需要阅读所有的数据，你无法避免，你只是不想一次装上所有的东西。所以如果你的问题是这本书里最长的单词是什么，你知道的唯一方法就是阅读书中的每一页，你不能只看两百页。
- en: 找到书中最长的单词，你只会找到最长的书，两百页中最长的单词，你得读完整本书，但你可以一次加载一页，找到，那页最长的字，忘了那一页，这样你就不用把整本书都记在记忆里了，索引是有用的。我们只需要加载一些数据，不是全部，但有一些，所以，例如，你们有一套会计制度，你想知道我们七月份花了多少钱，你不关心琼，你不在乎，愿你不在乎九月，你只关心七月，所以你只想加载7月份的数据。你不必在实践中加载所有的东西，你最终可能会同时使用这两种技术，例如，如果你的问题是七月份我们花了多少钱，可能7月份的数据也相当大，所以是的，你可以忽略所有其他月份，但你还有很多数据。所以一旦你找到了7月份的数据，而不是把所有这些都加载到内存中，你可以把它分块装到垃圾堆里，把大块总结出来，得到答案，你实际上可以把这些新技术一起使用，做分块最简单的方法，对不起，做索引最简单的方法是。
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 找到书中最长的单词，你只会找到最长的书。在两百页中，最长的单词，你得读完整本书，但你可以一次加载一页，找到那页最长的字，忘了那一页，这样你就不用把整本书都记在记忆里了，索引是有用的。我们只需要加载一些数据，而不是全部，但有一些，所以，例如，你们有一套会计制度，你想知道我们七月份花了多少钱。你不关心琼，你不在乎，愿你不在乎九月，你只关心七月，所以你只想加载七月份的数据。你不必在实践中加载所有的东西，你最终可能会同时使用这两种技术。例如，如果你的问题是七月份我们花了多少钱，可能七月份的数据也相当大，所以是的，你可以忽略所有其他月份，但你还有很多数据。所以一旦你找到了七月份的数据，而不是把所有这些都加载到内存中，你可以把它分块装到垃圾堆里，把大块总结出来，得到答案。你实际上可以把这些新技术一起使用，做分块最简单的方法，做索引最简单的方法是。
- en: 有一个目录和每个数据文件的文件名，告诉你文件里有什么数据，所以在这个例子中，我们每天都有一个不同的文件，一年中的每个月，所以我们有一月份的文件，二月和三月，等等，所以如果你想加载7月份的数据。我们只需要在
    csv装载218 dash宝石，我们不需要加载其他文件，索引基本上就是目录列表，目录，我们将从一段不使用索引的简单代码开始，然后我们会修改它添加索引，所以我们假设。对于住在你们城市的人来说，所以如果你要用分块来做这个，因为你有一个很大的文件，它不适合你的记忆，你会说每一块低，假设每一块都有一千行，按街道名称过滤，确保街道名称只符合我们想要的，对于特定海峡。为了那条街，我们把所有的小数据帧连成一个大数据帧，如果我们只做一次，这样就好了，我们可能想一遍又一遍地运行这个函数，在不同的街道上，例如你在管理，拉票努力，有一天你要去这些街道，有一天你会去不同的街道。
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: 所以你要一遍又一遍地调用这个函数，如果这个函数很慢，所有这些加起来，这就是索引有帮助的地方，我们索引数据的方法是，把它储存在立方体里，SQLite是一个内置到
    python中的 SQL数据库关系数据库。这是一个库，你不必运行服务器，像 Postgres或 mysql，你只是在磁盘上有个文件，你把它装上这个内置在蟒蛇中的库，给它读和写，所以使用
    sql lite文件非常方便。和处理 csv文件没什么不同，我们要做的是，我们会把所有的数据，分块读，把它转储到一个 sqlite数据库中，会自动创建续集表，您不必担心列和列类型，一旦我们把所有的数据都加载到
    sql表中。我们要让数据库创建一个索引，SQL数据库具有为快速查询创建索引的内置功能，我们可以重新使用街道栏的索引功能，因为它是，续集表，我们想要多少索引就有多少索引，我们可以在其他列中有索引。
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: 我们可以在多个列上设置索引，我们甚至可以有跨越表的索引，尽管那是一个更复杂的，利用男人，所以现在我们有了这个文件，磁盘上的 sqlite文件，它有所有的数据和快速航线的索引，我们会把它从数据库里。所以我们打开与数据库的连接，我的意思是我们运行一个查询说，从表中选择所有行，在那里，街道等于我们通过的任何价值，一旦你调用了这个函数，我们不知道数据来自一个
    SQL数据库，就像我们不知道，来自 csv。我们只是有一个正常的数据帧，因为续集有一个索引，它就能找到，与这条街道非常吻合，只加载那些行，它不需要读取所有的数据，它只需要读取我们关心的数据，让我们看看这两种技术的比较。分块技术我们只是从
    csv加载，和索引技术，其中你有续集精简运行一个索引，纽约市将有数百万人，想想风险投资案件中的内存使用情况，我们一次只装了一千块，我们把匹配的行，所以它真的不需要太多的内存。
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: 在 sqlite案件中，它必须将索引加载到内存中，但同样，索引是一个总结，所以应该很小，同样，我们有匹配行的内存使用，所以在这两种情况下 内存使用量都会很低，因此，我们已经解决了我们的基本目标。即能够处理大量的数据，只有一点点记忆，但它们的主要区别在于执行时间，加载
    csv需要574毫秒，因为我们必须从 csv中加载每一行并解析它，把它和我们的街道比对，过滤续集精简。我们只需要加载那些我们需要的记录，因为我们在桌子上加了一个索引
    所以它的速度又快了50倍，同时仍然有很低的内存使用量，这是我们最后的索引技术，如果你在使用其他图书馆。你也会遇到同样的问题 数据太多 内存不够，基本上相同的根源是问题，记忆快，但它也很贵，磁盘又便宜又慢，所以同样的基本解决方案，压缩，会提供给你，事实上，如果你想到西格利特，一直解决这个问题。
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: 他们用类似的方法通过压缩来解决这个问题，分块和索引，所以现在你知道了这些技巧，您只需阅读您正在使用的任何工具的文档，找找那些技术，以及如何在你的特殊情况下应用它们。虽然这次演讲主要集中在这三种软件技术上，别忘了你可以选择花更多的钱，买更多的内存
    或者租一台有更多内存的电脑，获得虚拟机，有时这将是最便宜的，最便宜的，最简单的事，终于，呃，如果你想拿到股票的幻灯片。这是网址，在我的网站上还有更多的技巧和策略，斜杠数据科学，这是我的电子邮件和推特手柄，呃，如果你找到这只狗，呃，只是，你知道吗，我最近在训练，将是线上培训，所以如果这是你感兴趣的事。如果你想提升你的团队能力，然后更广泛地伸出援手，如果你对技术有任何疑问，我将介绍如何在内存有限的情况下处理大量数据，请联系，给我发邮件，给我发推特，我很乐意回答。
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a073c113a15db82d1183adf308586723_2.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
