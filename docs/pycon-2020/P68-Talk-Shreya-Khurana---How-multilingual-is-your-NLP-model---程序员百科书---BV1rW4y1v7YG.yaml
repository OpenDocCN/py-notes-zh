- en: P68：Talk Shreya Khurana - How multilingual is your NLP model - 程序员百科书 - BV1rW4y1v7YG
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: P68：谈论 Shreya Khurana - 你的 NLP 模型多语言能力如何 - 程序员百科书 - BV1rW4y1v7YG
- en: Hello everyone and welcome to PyCon 2020。 This is my talk on how multilingual
    is your。
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 大家好，欢迎来到 PyCon 2020。这是我关于你的多语言能力的演讲。
- en: '![](img/655df6621bc0adf7ffe519682da6a324_1.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](img/655df6621bc0adf7ffe519682da6a324_1.png)'
- en: NLP model。 First of all， I would just like to say that I would have very much
    liked to。 meet you guys in person and Pittsburgh not only because I would have
    enjoyed it more but。 because you would have been a way better audience than my
    laundry bag and you clearly。 right in front of me。 Alright， so first a little
    bit about myself。 I work as a data scientist。
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: NLP 模型。首先，我想说，我很希望能在匹兹堡与大家见面，不仅是因为我会更享受，更因为你们将是比我的洗衣袋更好的观众，而且你们清晰地就在我面前。好了，先说说我自己。我是一名数据科学家。
- en: at GoDaddy。 I've been working there since July last year。 Before that I did
    my master's。 and statistics from the University of Illinois at Obama-Champaign。
    I work with unstructured。 language data and deep learning models。 Before that
    I also worked on patient hierarchical， modeling。 If you're ever interested you
    can just give me a shout out。 I'll be giving out。
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在 GoDaddy，我自去年七月以来一直在这里工作。在此之前，我在伊利诺伊大学香槟分校完成了我的硕士学位，主修统计学。我处理非结构化语言数据和深度学习模型。在此之前，我还从事过患者层次建模。如果你有兴趣，可以随时联系我。我会分享。
- en: my LinkedIn contact and my GitHub as well at the end。 Now this presentation
    has a couple。 of notebooks so if you want to follow them you can just go to my
    GitHub link right here。 and you can find everything you need to install and run
    the notebooks over there。 Alright。 so here's how this talk is going to go。 I will
    be introducing what multilingual data， is。
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 我的 LinkedIn 联系方式和我的 GitHub 链接在最后。这个演示包含几个笔记本，如果你想跟随它们，可以直接访问我的 GitHub 链接，你可以找到所有需要安装和运行这些笔记本的内容。好的，那么这次演讲将如何进行。我将介绍什么是多语言数据。
- en: The challenges we see with code switch data and transliteration。 What frameworks
    we。 have available for language identification and how do we go about combating
    those challenges。 Then we will be moving to the deep learning model which is BERT
    and we'll be looking at。 a few examples of how BERT Multilingual performs on code
    switch transliterated data。 We'll also。
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在代码切换数据和音译方面所面临的挑战。我们有什么框架可用于语言识别，以及我们如何应对这些挑战。接下来我们将转向深度学习模型 BERT，看看 BERT
    多语言在代码切换音译数据上的表现示例。
- en: be looking at how we can evaluate the BERT Multilingual model。 So why do we
    even care。 about multilingual data？ So right now on this earth we are about 7
    billion people speaking。 6800 languages and in 93 countries。 That is a huge statistic。
    Around 1。6 billion people。 speak Mandarin and 500 million people each speak English，
    Spanish and Hindi which is my。
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将研究如何评估 BERT 多语言模型。那么我们为什么在乎多语言数据呢？目前地球上大约有70亿人说6800种语言，遍布93个国家。这是一个巨大的统计数据。大约16亿人说普通话，5亿人分别说英语、西班牙语和印地语，而印地语也是我的母语。
- en: native language as well。 So you see with this huge statistic there is a need
    to actually。 evolve our natural language system so we can incorporate all of the
    customers， all of the。 people across the world and remove language barriers。 Alright
    so whenever we work with language systems we have a few tasks associated with
    them。 The。
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 由此可见，这个巨大的统计数字显示我们需要进化我们的自然语言系统，以便能整合所有客户和全世界的人，消除语言障碍。好吧，每当我们处理语言系统时，我们都有一些相关任务。
- en: first and most basic is the tokenization where what we're trying to do is let's
    say。 we have an input sentence we're trying to break it down into words that are
    already present。 in our vocabulary。 P。O。S。 tagging means that you have to assign
    all of those words a certain。 tags like noun， adjective， adverb， so on。 Named
    entity recognition where what you're trying。
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 首先最基本的是标记化，我们尝试做的是，假设我们有一个输入句子，试图将其分解成已经存在于我们词汇中的单词。词性标注意味着你必须给这些单词分配某种标签，比如名词、形容词、副词等等。命名实体识别是你所要做的事情。
- en: to do is figure out which words in your input sequence are named entities like
    locations。 names of persons and so on。 Language modeling。 This is by far the most
    interesting job according。 to me in a language system。 So what you're trying to
    do is you have an input sentence。 you basically try to predict what is the probability
    of that sentence， what is the probability。
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 要做的是找出输入序列中哪些词是命名实体，比如地点、人物名字等。语言建模。在我看来，这是语言系统中最有趣的工作。因此，你要做的是输入一个句子，基本上尝试预测该句子的概率，即该句子的概率。
- en: of that corpus， what is the probability that this word will occur next given
    the first， few words。 Machine translation is as the name suggests it's translating
    from one language。 to the other and right now with the boom of neural networks
    being used in NLP there's。 a huge research going on in this area。 Sequence classification
    could be something like sentiment。
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在该语料库中，给定前几个单词，接下来这个词出现的概率是多少。机器翻译顾名思义，就是从一种语言翻译成另一种语言，而现在随着神经网络在自然语言处理（NLP）中的蓬勃发展，这一领域正在进行大量研究。序列分类可能与情感分析有关。
- en: analysis where what you're trying to do is you have an input sequence and you're
    just trying。 to assign it to one or the other class。 Now let's see some of the
    examples of this new。
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 分析，你要做的是输入一个序列，然后尝试将其分配给一个或另一个类别。现在让我们看看一些这种新方法的例子。
- en: '![](img/655df6621bc0adf7ffe519682da6a324_3.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](img/655df6621bc0adf7ffe519682da6a324_3.png)'
- en: sort of language data that we're seeing。 So which means that you're trying to
    mix between。 syntax or grammar of one language with the other and this often happens
    when we have multilingual。 speakers and they sometimes miss elements of multiple
    languages together and you see。 a variety of phonology syntax mixing up。 So like
    here on the right side you see this example。
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们所看到的这种语言数据意味着你正在尝试将一种语言的句法或语法与另一种语言混合，这种情况通常发生在我们有多语言使用者的时候，他们有时会将多种语言的元素混合在一起，因此你会看到不同的音位和句法混合在一起。在右侧，你可以看到这个例子。
- en: where the first two words are of English and the rest of the sentence is in
    Malay and you。 can often see them in conversations。 This is an example in Spanglish
    so it's a mixture。 of English the first few words and the rest ending is of Spanish。
    Okay so out of the liberty。 of actually going to my own Facebook timeline and
    finding out examples of code switched and。
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 其中前两个单词是英语，其余句子是马来语，你经常可以在对话中看到它们。这是一个西班牙英语的例子，因此它是英语和西班牙语的混合，前几个单词是英语，其余部分是西班牙语。好吧，实际上是利用我的Facebook时间线去寻找代码切换的例子。
- en: translated data。 So this first example here is code switched because the first
    few words。 in the first example are in the translated text which means it's Hindi
    but it's actually。 converted to the English alphabet and the last three words
    let me celebrate you will be able。 to recognize they are English。 This example
    is unique because the first few words are。
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 翻译的数据。第一个例子是代码切换，因为在第一个例子中的前几个单词是翻译文本，这意味着它是印地语，但实际上是转换为英语字母，最后三个单词“让我庆祝”是英语。这个例子是独特的，因为前几个单词是。
- en: in the Hindi they've not great script which means it's in the native script。
    The last of。 the few words are in English and there's also a couple of words which
    are in transliterated。 English text。 This is the example of a tweet where you
    will be able to recognize the first。 four words when your mom says they are in
    English but the rest of the words are in transliterated。
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在印地语中，他们没有很好的书写方式，这意味着它使用的是本土书写系统。最后几个单词是英语，还有几个词是转写的英语文本。这是一个推文的例子，你可以识别出前四个单词“当你妈妈说”是英语，但其余的单词是转写的。
- en: English。 So these examples are pretty common if you look at YouTube comments
    or comments。 on Amazon tweets or on Facebook and this is because whenever multilingual
    speak they tend。 to mix languages and grammar of different types together。 So
    as part of this Facebook。 initiative of actually breaking down barriers and allowing
    everyone to engage in their own。
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 英语。这些例子在观看YouTube评论或亚马逊推文，或在Facebook上时是相当常见的，这是因为每当多语言者说话时，他们往往会混合不同类型的语言和语法。因此，作为Facebook这一倡议的一部分，实际上是打破障碍，允许每个人以自己的方式进行互动。
- en: content in their own language what they did was they started working with transliterated。
    text and translating it into either the native script or the English script。 So
    the official。 definition of transliteration is that you convert words written
    in one alphabet to another。 alphabet and that often happens because you don't
    have support for different scripts either。
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在自己的语言中，他们开始使用转写文本，并将其翻译成本地文字或英语文字。因此，转写的官方定义是将用一种字母书写的词转换为另一种字母，这通常发生在你没有对不同文字的支持时。
- en: in your keyboard or if you're working on a phone so basically it's just easier
    to work。 with a single set of alphabet。 So on the left hand side of the screenshot
    you would be able。 to recognize there are different scripts， there are words in
    English that you wouldn't。 be able to recognize probably there are words in Spanish
    and so what Facebook did was they。
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在你的键盘上或者如果你在手机上工作，基本上用一个字母集工作会更简单。因此，在屏幕截图的左侧，你可以识别出不同的文字，有些英语单词你可能无法识别，可能还有西班牙语单词，而Facebook所做的就是他们。
- en: basically translated everything to English and so if you are a speaker who does
    not speak。 Spanish or other languages that are shown in the text then you would
    be able to recognize。 the screenshot on the right and read stuff in English。 Alright，
    now we switch over to a。 language identification。 Now language identification
    is a very hard problem because there are certain。
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上将所有内容翻译成英语，因此如果你是一个不会说西班牙语或其他文本中展示的语言的说话者，那么你将能够识别右侧的屏幕截图，并以英语阅读内容。好了，现在我们转到语言识别。语言识别是一个非常困难的问题，因为存在某些。
- en: vocabularies in many languages which are shared and the first framework we're
    going。 to be covering right now is CLD3。 Now CLD3 was this Chrome extension that
    was released by。 Google and if you ever use Chrome you know that it detects language
    right。 So how it。 was built was using N-grams。 So what they did was so let's say
    you have an input sequence。
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多语言中共享的词汇，而我们现在要覆盖的第一个框架是CLD3。CLD3是谷歌发布的一个Chrome扩展，如果你使用过Chrome，你就知道它可以检测语言。那么它是如何构建的呢？是使用N-grams。所以他们做的是假设你有一个输入序列。
- en: they would extract the unicroms and they would calculate the probability of
    each。 Instagram so a unigram in this case is just a simple character。 Then they
    would go to。 buy grams and they would look at pairs of characters and what is
    the probability that。 they're seeing them in the input sequence and similarly
    for trigrams。 Then basically。
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 他们会提取单元，并计算每个Instagram的概率，因此在这种情况下，单元仅仅是一个简单的字符。然后他们会转向双元，查看字符对以及它们在输入序列中出现的概率，对于三元也是如此。然后基本上。
- en: they would embed all of these unigram probabilities in an embedding layer， feed
    that to a hidden。 layer of certain neurons and basically feed that to a softmax
    layer。 During the inference。 they would do something like extract these N-grams
    from the input text， calculate the。 probability then they would just go through
    this one forward pass of the whole neural network。
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 他们会将所有这些单元概率嵌入到一个嵌入层，然后将其输入到某些神经元的隐藏层，基本上再送到一个softmax层。在推理过程中，他们会提取输入文本中的这些N-grams，计算概率，然后只需通过整个神经网络进行一次前向传递。
- en: So let's see CLD3 in practice。 It supports these set of languages that are shown
    here。 and it also supports the encircled ones which are the transliterated ones。
    So you can see。 that it supports Chinese transliterated Russian， Bulgarian， Greek
    which is EL， JA which is。 Japanese， H。I which is Hindi and the Latin prefix suffix
    just means that it's transliterated。
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 所以让我们看看CLD3的实际应用。它支持这里展示的一组语言，也支持圈出的那些转写语言。因此你可以看到，它支持中文、转写的俄语、保加利亚语、希腊语（EL）、日语（JA）、印地语（H.I），而拉丁前缀和后缀则表示它是转写的。
- en: So CLD3 has this function called get language where if you just give it the
    input sentence。 or just a sequence of characters it would recognize what language
    it's in。 So it gives。 the probability with which it's predicting that language
    then it will also give the proportion。 of the text that it thinks it's in that
    language。 So over here in the first example we see we。
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: CLD3有一个叫做get language的功能，如果你输入一句话或一串字符，它会识别出是什么语言。因此，它会给出预测该语言的概率，并且还会提供它认为文本中属于该语言的比例。在这里，第一个例子中我们看到。
- en: give it a mixture of French which is the Jürburgr， the first three words and
    we give。 it a URL but it's unable to recognize that French part of the sentence
    and that is because。 it gets confused to the URL which is in English。 Now CLD3
    also has this another function where。 you can get more than one language as prediction
    and that is just get frequent languages。 So。
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我给它混合了法语的几句话，前面三个单词是Jürburgr，然后我们给了它一个网址，但它无法识别出句子中的法语部分，这是因为它对网址产生了困惑，网址是英语。现在CLD3还有另一个功能，即你可以获取多种语言的预测，这只是获取频繁的语言。
- en: I try to do this。 I try to give it the same input sequence and what I would
    have expected。 was that it would be able to recognize that some proportion of
    it is in French some proportion。 of it is in English but it was unable to do that。
    Okay let's look at some other examples。 Now this input text right here in this
    example is of Russian transliterated。 Now as you can。
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我尝试这样做。我尝试给它相同的输入序列，而我所期望的是，它能够识别出其中有一定比例是法语，有一定比例是英语，但它无法做到这一点。好吧，让我们看看其他一些例子。现在这个输入文本在这个例子中是俄语音译。正如你所看到的。
- en: see it's able to recognize it periodically well and it has a really high probability。
    It says it's reliable and the proportion of text that is in Russian Latin is one
    which。 is completely correct and that's because these are the languages that are
    supported。 Oftentimes you will see that you will need to experiment with it to
    figure out how accurate。
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到它能够定期很好地识别，并且具有很高的概率。它说这是可靠的，而文本中拉丁语的比例为1，这是完全正确的，因为这些是支持的语言。通常情况下，你会发现你需要进行实验，以找出它的准确性。
- en: this is for your own language。 Now I give it a piece of Russian in its own native
    script。 and it's able to recognize that with a pretty high probability as well。
    Let's see how it。 works on transliterated data。 So this is a piece of transliterated
    Hindi and as you can。 see there are some prop amounts which is being depicted
    by the capitalization here and all。
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这是为了你自己的语言。现在我给它一段用其本土文字书写的俄语，它能够以相当高的概率识别出来。让我们看看它在音译数据上的表现。这是一段音译的印地语，正如你所看到的，有一些比例是通过大写字母表示的。
- en: I wanted to see was whether it was able to predict because it does seem to be
    trained。 on Hindi transliterated data but here it's predicting finish and that's
    because it might。 have some common vocabulary with the Finnish language。 I give
    it another example over here。 which is just Hindi for how are you？ I'm good it's
    a pretty common phrase again this is。
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我想看看它是否能够进行预测，因为它似乎确实是基于印地语音译数据进行训练的，但在这里它预测为芬兰语，这是因为它可能与芬兰语言有一些共同词汇。我在这里给了另一个例子，就是印地语的“你好吗？我很好”，这是一句非常常见的短语，再次这是。
- en: transliterated as you can see but it does not predict Hindi transliteration
    over here it predicts。 that it's Gaelic。 Let's see how it does on code switch
    data right code switch data if you remember。 is just mixing two languages together
    in the single sequence。 So this is Spanish and English。 so as cool kids are calling
    it these days it's like Spanglish now but when you feed it to。
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，这里是音译的，但它并没有预测出印地语音译，它预测为盖尔语。让我们看看它在代码切换数据上的表现，对吧？代码切换数据，如果你记得，就是在单一序列中混合两种语言。所以这是西班牙语和英语，正如现在的酷孩子们所称之的，这就像西班牙英语混合体，但当你把它喂给。
- en: CLD3 it does not predict it to be Spanish or English it predicts to be Maori。
    And I give it some other phrases also this is again Spanglish this is Frankly
    which is French。 English it's not able to predict it really well again right it
    predicts something like。 Catalan which is a western romance language which was
    derived from Latin and it says the proportion is one。
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: CLD3并没有预测它是西班牙语或英语，而是预测它是毛利语。我还给了它其他一些短语，这又是西班牙英语混合，这是法语和英语的结合。它不能很好地预测，确实是，它预测出一些像加泰罗尼亚语这样的结果，这是一种源自拉丁语的西方罗曼语言，比例为1。
- en: Now in the last slides you saw that CLD3 has its own challenges right it's not
    able to predict all。 languages with the very high probability。 We move over to
    a different framework which is， LANGID。 Now LANGID was the standalone LANGID with
    that identification tool it's pre-trained over。 97 languages and it can be deployed
    as a web service but basically at its pace it's just a。
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在最后几张幻灯片中，你看到CLD3有自己的挑战，它无法以很高的概率预测所有语言。我们转向另一个框架，即LANGID。LANGID是一个独立的语言识别工具，它预训练了97种语言，可以作为网络服务进行部署，但基本上在其基础上，它只是一个。
- en: naive base classifier with a multinomial event model and what it does is it
    just looks at a mixture。 of byte and grams。 Let's look at it in practice so right
    now what I did was you have you just load。 the model over here you normalize the
    probabilities which means that you have the score and it will。 just normalize
    the probabilities for all the different languages。
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 朴素贝叶斯分类器和多项事件模型的组合，它只查看字节和n-gram的混合物。现在我们来看看实践中如何运作，所以我所做的是加载模型，您在这里标准化概率，这意味着您有评分，并将为所有不同语言标准化概率。
- en: So I set the language to be a particular， set over here this is one very useful
    advantage of using this module that you can actually set it to。 be a particular
    language set。 So if you're sure that your training set only has about four languages。
    you can give that to the identifier here and it will predict one of these four
    languages。 So let's see now that we've said our languages about Russian English
    Italian and Slovakian。
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我将语言设置为特定语言集，这里是使用该模块的一个非常有用的优点，您可以将其实际设置为特定语言集。因此，如果您确定训练集只有四种语言，您可以将其提供给识别器，它将预测这四种语言之一。现在让我们看看我们所说的语言，包括俄语、英语、意大利语和斯洛伐克语。
- en: we see that this is the piece of transliterated Russian that we gave it earlier
    also in CLD3。 it predicts it to be a slow vakian with a probability of 0。67。 Another
    example is of this Hindi transliteration where I give it a set of Hindi and English。
    but because it does not support transliterated text at all it just predicts it
    as English because。
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到这是之前在CLD3中给出的转写俄语，它预测为慢瓦基安语，概率为0.67。另一个例子是这个印地语转写，我给了一组印地语和英语文本，但由于它根本不支持转写文本，因此只预测为英语。
- en: it sees English characters over here。 Another identification framework we have
    is Langdetect。 However one thing that you should know about Langdetect is that
    it is。 undeterministic which means that if you try and run it on a text which
    is either too short or。 too ambiguous you might get different results every time。
    Again it's based on a night based。
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 因为它在这里看到的是英语字符。我们还有另一个识别框架Langdetect。然而，您应该知道Langdetect有一个特点，即它是不可确定的，这意味着如果您在文本过短或过于模糊的情况下运行它，可能每次都会得到不同的结果。
- en: classifier and works with character based engrams。 However it supports only
    about 49 languages and。 those also with about 99。8% precision。 It does not support
    any of the transliterated languages though。 and how it works is that basically
    it consists of computing features based on the language。 So like if you look at
    the accented E you can only see it in Spanish， Italian but not so much in。
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 分类器基于字符的n-gram工作。然而，它仅支持大约49种语言，并且精确度约为99.8%。不过，它不支持任何转写语言，其工作原理基本上是根据语言计算特征。如果您查看带重音的E，您只能在西班牙语和意大利语中看到，而在其他语言中则不太常见。
- en: English right and it will look at features like how often the words that are
    starting with Z。 are used in German versus how often they're used in English and
    then it will compute the。 probability of those features given the input sequence。
    In Python you can just import it as Langdetect and this has it has a function
    which is detect。
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 它是基于英语的，并会查看像Z开头的单词在德语中使用的频率与在英语中使用的频率的差异，然后计算给定输入序列的这些特征的概率。在Python中，您可以将其导入为Langdetect，它有一个检测函数。
- en: Langs。 Again you can just go through the notebooks at your own pace， play with
    it， give it more。 examples， get a feel of how it's performing on your preferred
    language。 Now over here as you can see again because it does not support transliterated
    data it just。 predicts it as low wakian。 If you run it again because it's non-deterministic
    it will give you。
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以根据自己的节奏浏览笔记本，进行尝试，给它更多例子，感受它在您偏好的语言上的表现。正如您在这里所见，由于它不支持转写数据，它只预测为慢瓦基安语。如果您再次运行它，由于其非确定性，它会给您不同的结果。
- en: four different languages with four different probabilities like over here is
    predicting。 slow wakian， albanian， huern， hung， gary， and but none of them is
    Russian。 So in the past few slides we saw how all of these frameworks have their
    own limitations right。 We gave them texts with Romanized scripts in a different
    language but they had difficulty。
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 四种不同语言和四种不同概率的预测，如这里所示。慢瓦基安语、阿尔巴尼亚语、赫尔语、匈牙利语、加里语，但其中没有俄语。在之前的几张幻灯片中，我们看到所有这些框架都有自己的局限性。
- en: recognizing them。 If you have a very small text length that means you're just
    giving very。 less information to the model or any of these models that these frameworks
    have been trained on。 There are often borrowed words from different languages
    that you can see in any languages。 vocabulary right and that is why certain frameworks
    get confused between predicting either of them。
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 识别它们。如果你的文本长度非常小，那意味着你只是在给模型或这些框架中训练的任何模型提供非常少的信息。不同语言中的借用词通常可以在任何语言的词汇中看到，这也是某些框架在预测时相互混淆的原因。
- en: There are very different transliteration schemes so like how I write in Hindi
    transliterated is very。 different from my friend who lives in a different part
    of India and who writes in a different。 English script。 There are overlapping
    vocabulary items， certain。 vocabularies of different languages they have the same
    words but with different meanings which。
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 有非常不同的音译方案，比如我在印地语中的音译写法与我在印度不同地区的朋友的写法非常不同，而他使用的是不同的英语脚本。不同语言中有重叠的词汇项，某些语言的词汇有相同的词但有不同的含义，这常常会让这些模型感到困惑。
- en: often confuses these models as well。 And as you can see with limited data which
    means that with。 all of these code switched language examples and with these translated
    examples there's very little。 data to actually train your own model right。 So
    if you want your model to be able to recognize this。 you have to have something
    giving you as a very high probability that it is in fact a piece of。
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 而且你会看到，有限的数据意味着所有这些代码切换语言的示例和这些翻译的示例，实际上很少的数据来训练你自己的模型。所以如果你希望你的模型能够识别这一点，你必须有一些东西来给你一个非常高的概率，表明它实际上是一段。
- en: transliterated text or or if it's in fact a piece of code switched text but
    the reason that these。 frameworks are not giving it is because they've been trained
    on very small data for these certain。 special cases。 So how do we go about solving
    this？
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 音译文本，或者如果它实际上是一段代码切换文本，但这些框架没有给出的原因是因为它们在这些特定特殊案例上训练的数据非常少。那么我们该如何解决这个问题呢？
- en: One of the approaches is that you actually augment。 your datasets with the dataset
    that you build yourself。 So often you will see that there is not。 enough data
    that is available in your language。 For example within the ISO that in transliterated。
    text I did not have a particular parallel corpus to work with。 So if I wanted
    to detect something。
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 一种方法是你实际增强你的数据集，使用你自己构建的数据集。因此你会发现，你的语言中没有足够的数据。例如在ISO中，在音译文本中，我没有一个特定的平行语料库可以使用。所以如果我想检测某些内容。
- en: as it was transliterated or if it was English there wasn't any model that was
    trained on a very。 large piece of data to actually give me this with a very high
    probability。 Often you will see。 these off the shelf models also which are trained
    on multiple languages right but if they are trained。 on multiple languages just
    like we'll see with bird you see that they have vocabularies spanning。
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 如果它是音译过来的或者如果是英语，那么没有任何模型是在非常大数据集上训练的，实际上给我这个结果的概率非常高。你经常会看到这些现成的模型，它们也在多种语言上进行训练，但如果它们是在多种语言上训练的，就像我们会看到的BERT，你会发现它们的词汇是广泛的。
- en: a lot other languages that what you want to be working with right and that just
    brings a noise。 from other languages to your model。 So how do we do this？ So the
    first thing is that you have to。 identify one data source or multiple data sources
    in the language in any language that can be translated。 to yours either by rules
    or by machine translation。 And then you use this machine generated data to。
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 还有许多其他语言，你希望处理的语言，这只会给你的模型带来来自其他语言的噪音。那么我们该怎么做呢？第一件事是你必须识别一个数据源或多个数据源，这些语言可以通过规则或机器翻译转换为你的语言。然后，你使用这些机器生成的数据来。
- en: augment。 For example right now let's say I was working with Hindi and I wanted
    to get much more。 data and transliterated form right。 So I have this dump of Hindi
    Wikipedia articles which is。 available readily online and I built my own transliterator
    and this transliterator is based on rules so I。 define certain rules。 I give it
    to my transliterator class。
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，现在假设我正在处理印地语，我想获取更多的数据和音译形式。所以我有这批印地语维基百科文章的转储，这些文章在线上可以轻松获取，我构建了自己的音译器，这个音译器是基于规则的，因此我定义了一些规则。我把它们给我的音译器类。
- en: It just basically converts all of this dump， of Hindi Wikipedia articles into
    a new data set。 And once I have this new data set I can basically。 augment it
    with very small data set that I already had。 So this data set right here this
    data set is， the addition that we are providing to our base data set to actually
    help it give more confidence。
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 它基本上将这些印地语维基百科文章的所有内容转换为一个新数据集。一旦我拥有这个新数据集，我基本上可以用我已有的非常小的数据集进行增强。因此，这个数据集是我们提供给基础数据集的附加内容，实际上有助于提高它的可信度。
- en: to recognize that it's actually transliterated or not。 Okay so let's look at
    how we go about。 solving this small example。 Now this is a very simple transliterator
    in which what I've done is。 I've just defined my mappings of my Hindi alphabet
    to my English alphabet and basically I just。 see it in a piece of text which might
    be in Hindi。
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 要认识到它实际上是音译的。好的，那么让我们看看如何解决这个小例子。现在这是一个非常简单的音译器，我做的只是定义了我的印地语字母与英语字母的映射，并基本上在一段可能是印地语的文本中查看它。
- en: So if I have an article or Wikipedia article basically， I just read all sentences。
    I use these mappings to convert them into English alphabet。 I define certain。
    other very simple rules as to what character can come before the other and then
    I just converted。 to the English alphabet。 So like right here I just convert this
    one single sentence which is from。
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 所以如果我有一篇文章或维基百科文章，基本上我会读取所有句子。我使用这些映射将它们转换为英语字母。我定义了一些其他非常简单的规则，例如什么字符可以在其他字符之前出现，然后我将其转换为英语字母。因此，就像这里我转换了这一句来自于某处的单句。
- en: a Hindi Wikipedia page for Python and it just converts it to English alphabet。
    If you're familiar。 with Hindi alphabet you will see that this is not a very good
    transliteration and that's because。 we're working with a very simple example right
    here and I just wanted to show that you can actually。 just build more sophisticated，
    more complex transliterated cases based on your own custom task。
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 一个关于Python的印地语维基百科页面，它将其转换为英语字母。如果你熟悉印地语字母，你会看到这并不是一个很好的音译，因为我们在这里处理的是一个非常简单的例子，我只是想展示你实际上可以基于自己的自定义任务构建更复杂的音译案例。
- en: Now we can either build our own transliterated systems or we can actually look
    at certain other。 examples that might exist in the literature already。 So for
    example we， I was working with。 the Indic languages and I found the Sealycrate
    library which is the CS and LI and what it does is。 it already has a pre-trained
    neural machine translation model that converts the Roman to Hindi。
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以构建自己的音译系统，或者我们可以查看文献中可能已经存在的其他示例。例如，我在处理印地语时发现了Sealycrate库，它是CS和LI，它的功能是已经有一个预训练的神经机器翻译模型，将罗马字母转换为印地语。
- en: and basically it would give you something like this。 So if you give it a piece
    of text it would。 convert it to the Hindi script and basically if the probability
    of that conversion is really high。 it would assign it a Hindi tag。 So what I get
    with this is I get language identification that is not。 only sent tense right
    but that is token right。 So that really helps in examples which might。
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上，它会给你这样的结果。所以如果你给它一段文本，它会将其转换为印地语脚本，如果该转换的概率非常高，它会分配给它一个印地语标签。所以我得到的是语言识别，不仅是句子级别的，而是词元级别的。因此，这在可能的例子中非常有帮助。
- en: have code switch data。 Now this is only for Indic languages but there are tons
    of other open source。 libraries that are available for other languages as well。
    Okay。 now that we've looked at all of these， language identification frameworks
    and the challenges associated with them。 how do we go about solving， them。 Let's
    move on to certain off the shelf models that are readily available right。
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 有代码切换数据。现在这仅适用于印地语，但还有许多其他开源库可用于其他语言。好的，现在我们看过所有这些语言识别框架及其相关挑战后，我们该如何解决它们呢？让我们继续看看一些现成的模型，这些模型是现成可用的。
- en: So we have this， really state-of-the-art model called the transformer and the
    transformer was actually very famous not。 because of what it brought to the table
    which was state-of-the-art at that time but because it。 led to so much of research
    being happening in the neural language community which meant that。 there were
    so many models， so many research papers being published after this。 Now the。
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们有这个非常先进的模型叫做transformer，而transformer之所以出名并不是因为它当时所带来的前沿技术，而是因为它促使了神经语言学社区进行大量研究，这意味着在此之后有许多模型和研究论文被发表。
- en: transformer was introduced in Attention is All You Need。 This was the paper
    by。 Waspani and others and basically it has a stack of encoders。 So this right
    here is an encoder。 Each encoder has two layers which is the multi-head attention
    and of normal feed forward layer。 We'll talk about multi-head attention in the
    coming slide and it had a stack of six encoders。
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: transformer是在《Attention is All You Need》中引入的。这是Waspani等人所写的论文，基本上它有一堆编码器。所以这里就是一个编码器。每个编码器有两层，分别是多头注意力和普通前馈层。我们将在接下来的幻灯片中讨论多头注意力，它有六个编码器堆叠。
- en: and similarly a stack of 60 coders and each decoder had about three layers。
    So again one was multi-head， attention， again a multi-head attention and then
    a feed forward neural network layer。 Okay so we've been talking about multi-head
    attention right。 Multi-headed attention was this。 concept introduced by this paper
    which is why it was becoming very famous。
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，有60个编码器堆叠，每个解码器大约有三层。所以再次强调，一个是多头注意力，另一个是前馈神经网络层。好吧，我们一直在谈论多头注意力，对吧？多头注意力是这个概念，是由这篇论文引入的，这也是它变得非常著名的原因。
- en: So attention was actually， not introduced in this paper。 Attention was introduced
    way back in 2014 and attention is when you're。 actually trying to figure out which
    word in the given sequence places how much of attention on。 the other words in
    the sequence and this could basically just mean that how many weights or how。
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，注意力并不是在这篇论文中引入的。注意力早在2014年就被引入了，注意力是当你试图弄清楚在给定的序列中，哪个词对其他词的关注程度有多高，这基本上意味着你需要计算多少权重或多少关注。
- en: much of weight are you giving on each of these words in the sequence。 Now multi-headed
    attention。 meant that you're not even looking at just one sort of weights but
    you're looking at two different。 sets of weights or three different sets of weights。
    In the paper they're using eight different matrices。
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 你在这个序列中的每个词上给了多少权重。现在，多头注意力意味着你不仅仅看一种权重，而是查看两种或三种不同的权重集。在论文中，他们使用了八个不同的矩阵。
- en: which means that they're looking at eight different attention heads。 So as your
    sentence gets more。 complicated it needs to figure out different relations between
    the given word and the other。 words in the sequence。 So like right now if I'm
    given no other context of all of these words before。 this word it I might just
    want to figure out what it is referring to。 So let's say if I'm。
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着他们在查看八个不同的注意力头。因此，随着句子的复杂性增加，它需要弄清楚给定词与序列中其他词之间的不同关系。所以现在如果在没有其他上下文的情况下看到这些词，我可能只是想弄清楚它指的是什么。假设我。
- en: not given any of this and I just have this sentence which is it was too tired。
    My question might be。 who is it？ Who does it refer to right？ Who isn't too tired？
    Is it the street？ Is it the animal？
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在没有提供任何信息的情况下，我只有这一句，它太累了。我的问题可能是：它指的是谁？它是指谁对吧？谁不太累？是街道吗？还是动物？
- en: And basically to answer this question is where attention is required。 So this
    was just one question that I'm answering but what if the other question that I'm
    answering。 is that why didn't the animal cross the street then the answer becomes
    because it was too tired。 So then that places a different sort of weight on different
    words and basically this is the。
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上，回答这个问题需要关注点。所以这只是我在回答的一个问题，但如果我回答的另一个问题是，为什么动物没有过马路，那么答案就是因为它太累了。因此，这会对不同的词赋予不同的权重，基本上就是这样。
- en: problem that this model is trying to solve it's basically trying to see how
    you can answer。 various questions that are coming how you can assign different
    weights to different clauses。 based on just a single given word。 Now what was
    bird？ Now bird again state of the art soda。 everything now that you will see in
    this NLP community is that it beats soda right。 But bird。
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型试图解决的问题基本上是如何回答不同的问题，以及如何根据一个给定的单词给不同的子句分配不同的权重。现在，bird是什么？再次说，鸟是最先进的soda。在这个NLP社区中你将看到一切都是超越soda的，对吧？但bird。
- en: again was a transformer at its base and when it was built literally every other
    model on that was just。 based on bird。 So like we have Robert we have Disturb
    but we have two of bird we have Senti， bird。 I mean most of the models that we
    see right now or most of the natural language。 community that you see right now
    is working on bird working on its representations trying to。
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 它的基础是变换器，当它被构建时，实际上所有其他模型都是基于鸟类的。例如，我们有罗伯特，我们有Disturb，还有两种鸟类，我们有Senti，bird。我的意思是，我们现在看到的大多数模型，或者说大多数自然语言社区，都是在使用鸟类，研究它的表示，试图去改进。
- en: improve it trying to make it faster and so on。 So what is it？ Why is it special？
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 改进它，试图使其更快等等。那么这是什么？它有什么特别之处？
- en: And bird is special because of the way it uses pre-training and fine-duning。
    Now bird at its。 pre-training stage what it did was it had all of this corpus
    and basically it trained on this unlabeled。 data。 It made two tasks。 The first
    task was about the next sentence prediction so if you have a corpus。 it would
    create a task that would have if one sentence is the next sentence of the previous
    one。
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 鸟类之所以特别，是因为它使用预训练和微调的方式。现在在鸟类的预训练阶段，它使用了所有这些语料库，并基本上在这些未标记的数据上进行了训练。它设置了两个任务。第一个任务是下一个句子预测，因此如果你有一个语料库，它会创建一个任务，判断一句话是否是前一句的下一个句子。
- en: a 50% of the time and then they would create just a random sentence after another
    sentence。 and then that would be labeled as false。 Another task that they use
    was the mass language model。 which will be coming in the coming slides。 And at
    the fine-tuning stage what it did was。 once you have all of these weights that
    have been trained on these two tasks you basically just。
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 50%的时间，然后他们会在另一句话之后创建一个随机句子，这会被标记为错误。他们使用的另一个任务是**掩码语言模型**，将在接下来的幻灯片中介绍。在微调阶段，它的做法是，一旦你有了在这两个任务上训练好的所有权重，你基本上就。
- en: fine-tune this neural network for your particular task which could be sequence
    classification。 which could be question answering which is what you see in reading
    comprehension。 You could see。 if one sentence is a paraphrase of the other and
    so on。 So bird multilingual actually had this。 one pre-processing technique which
    is the wordpiece processing and the reason it has been proven so。
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 微调这个神经网络以适应你的特定任务，比如序列分类、问答等，这在阅读理解中可以看到。你可以判断一句话是否是另一句话的同义句，等等。因此，鸟类多语言实际上有这种预处理技术，即词片处理，这也是它被证明有效的原因。
- en: useful for multilingual systems is that you can actually share vocabulary across
    various languages。 So like the bird vocabulary for this multilingual model is
    about 120，000 and it tokenizes on the。 basis of how likely our character sequences
    to be seen together。 So in this what piece pre-processing， what we do is we basically
    start from the character level and for all of our languages which is。
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 对于多语言系统来说，**共享词汇**是非常有用的。因此，这个多语言模型的鸟类词汇大约有120,000个，它是基于我们字符序列共同出现的可能性进行标记的。在这一预处理过程中，我们基本上是从字符层面开始，适用于我们所有的语言。
- en: 104 languages that the bird has been trained on we look at which characters
    are much more likely。 to be seen together than others and this is how we form
    these subboards。 So subboards are something。 in between a character and a word。
    So in this example I'm using the hugging face transformers。 and tokenizers library。
    If you've ever worked with NLP you must be aware of this but if you're。
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 104种语言是鸟类训练的语言，我们查看哪些字符更有可能一起出现，而这就是我们形成这些子词的方式。所以子词是介于字符和单词之间的一种东西。在这个例子中，我使用的是Hugging
    Face的变换器和标记器库。如果你曾经与自然语言处理(NLP)打过交道，你一定对这个有所了解，但如果你没有。
- en: hearing about this for the first time definitely go check this out。 This library
    actually makes。 it very easy for us to train and evaluate transformers and it's
    a really good library for beginners。 So I just load the bird multilingual model
    and I load the equivalent tokenizer。 Then in line 5。 I basically tokenize the
    Spanish query and if you look at the pieces that we return。
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你是第一次听说这个，**一定要去看看**。这个库实际上使我们训练和评估变换器变得非常简单，对于初学者来说是一个非常好的库。所以我只需加载鸟类多语言模型和相应的标记器。然后在第5行，我基本上对西班牙语查询进行标记，如果你查看我们返回的部分。
- en: we're getting in return you will be seeing that you don't get the same words
    as are separated by。 white space right and that is because those are subboards
    and those are the word pieces。 So if you see a hash hash in the beginning of the
    subboard that's because that means that it was。 supposed to be appended to the
    previous one。 So as you can see Ola has been divided into two subboards。
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们所获得的结果是，你会看到分隔的词并不是同样的词，因为它们是由空格分隔的，这就是因为那些是子板块，都是词片段。所以如果你看到子板块开头有一个哈希值，那是因为它意味着它应该附加到前一个词上。所以你可以看到
    Ola 被分成了两个子板块。
- en: Kiara has been divided into two subboards and so on。 In the next example in
    line 6 I tokenize a Hindi， query where you see that the first word has been tokenized
    into three parts and in the last example。 if you're an office fan here is B。 It's
    Battlestar Collectica。 So if you look at beats it has been again， divided into
    two subboards which is B and TS and again the hash is in between the beginning
    they。
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: Kiara 被划分为两个子板块，依此类推。在第 6 行的下一个示例中，我对一个印地语查询进行了标记，你会看到第一个词被分为三个部分，在最后一个示例中，如果你是办公室的粉丝，这就是
    B。这是《太空堡垒卡拉迪加》。所以如果你查看节拍，它再次被划分为两个子板块，即 B 和 TS，而哈希值则在它们之间。
- en: just mean that the word has to be appended to the previous one。 So let us look
    at some statistics of， languages and how the bird actually leverages so many languages
    in just a single vocabulary right。 So let's look at the right-hand side graph
    first。 The right-hand side graph actually gives you fertility。 and fertility is
    a term that has been borrowed from statistical machine translation which。
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 这仅意味着该词必须附加到前一个词上。所以让我们看一下语言的统计数据，以及鸟是如何在一个单一词汇中利用这么多语言的，对吧？所以我们先看右侧的图。右侧的图实际上给你提供了繁殖力，而繁殖力是从统计机器翻译中借用的一个术语。
- en: and what it means is that what is the average number of bird word pieces corresponding
    to a。 single real token。 So in the last example we saw that Ola was actually divided
    up into two word。 pieces right。 So if you're looking at the fertility of that
    word it just means it's two。 But for a。 particular language what we do is we calculate
    the average number of word pieces that each word。
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着对应于一个单一的真实标记，鸟的词片段的平均数量是多少。在最后一个示例中，我们看到 Ola 实际上被分成了两个词片段，对吧？所以如果你在看那个词的繁殖力，它的意思就是它有两个。但对于某种特定语言，我们会计算每个词在该语言中被分成的词片段的平均数量。
- en: in that language is divided into。 So on the left-hand side of this graph where
    you see fertility versus。 the language you see that there are certain languages
    like Portuguese， Hebrew， English which。 have a fertility of around one which means
    that they are retaining their own original vocabulary。 So that means that most
    of the words in English actually have or have been divided into just one。
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个图的左侧，你可以看到繁殖力与语言的关系，某些语言，如葡萄牙语、希伯来语和英语，繁殖力大约为 1，这意味着它们保留了自己的原始词汇。所以这意味着英语中的大多数单词实际上被分成了仅一个词片段。
- en: word which means that yeah it's just the original one。 On the right-hand side
    of these。 graph you see that there are Tamil， Telugu， Armenian， Greek， these languages
    right。 And that just means， that these languages are broken up into more than
    two word pieces。 So an average word if you take， in these languages it is more
    likely to be broken up into more number of word pieces。
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着它只是原始的一个。在这些图的右侧，你可以看到泰米尔语、泰卢固语、亚美尼亚语、希腊语等，这些语言的词被分为两个以上的词片段。所以在这些语言中，如果你取一个平均词，它更可能被分成更多的词片段。
- en: So in the left， inside of the graph the left graph if you see you are plotting
    frequency with respect to length in。 characters。 So this means that how many word
    pieces we see of how much length。 So again it's a very。 common graph because as
    the number of characters increases you see that the frequency decreases。 and that's
    very common because you're likely to break it down into smaller and smaller word
    pieces。
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在图的左侧，如果你看到你正在绘制与字符长度有关的频率。这意味着我们看到多少个词片段以及它们的长度。因此，这又是一个非常常见的图，因为随着字符数量的增加，你会看到频率下降，这是非常常见的，因为你可能会将其分解为越来越小的词片段。
- en: which actually have a much more likelihood of being seen。 So over here we talk
    about the。 PERT mass language model。 Now the special thing about this is that
    it was bidirectional which means。 that if you're looking to predict a word you
    basically look at the context from left to right。 as well as from right to left。
    So in PERT what the authors did was they masked 15% of all the word。
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，这些词被看到的可能性更高。因此，我们在这里讨论的是PERT被遮罩语言模型。这个模型的特别之处在于它是双向的，这意味着在预测一个词时，你基本上会从左到右和从右到左看上下文。因此，在PERT中，作者对所有词的15%进行了遮罩。
- en: piece tokens at random and they replaced this every 15 per every word of the
    15% with a 80%。 probability that it would be a mask with a 10% probability that
    it would be a random token。 and with a 10% probability that it would just remain
    the same original token。 And the way they。 would evaluate this language model
    is by just predicting how accurate your masked words are。
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 随机选择部分标记，并以15%的概率替换其中每个词，80%的概率会遮罩，10%的概率会变为随机标记，以及10%的概率会保持原始标记不变。而评估这个语言模型的方法是通过预测被遮罩词的准确性。
- en: So here we define a function just to predict a given word in a given piece of
    sequence。 So here again I'm using the transformers library and you just load the
    model which is PERT for。 masked LM and you load the multilingual one because we're
    going to be looking at certain multilingual。 examples and in this function what
    we're doing is we take a piece of text we tokenize it then this is。
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 所以在这里我们定义了一个函数，用于在给定序列中预测特定词汇。在这里我再次使用transformers库，加载的是用于被遮罩语言模型的PERT模型，并加载多语言模型，因为我们将查看某些多语言示例。在这个函数中，我们取一段文本，进行标记化。
- en: the second argument which is the word that you want to predict and you basically
    just give it a。 mask over here and you feed it to the model it predicts you take
    the art max of probability。 and then you see which one is the most likely word
    that is being predicted by the model。 you also look at certain other predictions
    just to see if your desired prediction isn't the top K。
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个参数是你想预测的词，你基本上只需在这里给一个遮罩，然后将其输入模型，模型会进行预测，你取概率的最大值，然后查看哪个词是模型最可能预测的。你还会查看某些其他预测，以便确认你期望的预测是否在前K个中。
- en: All right now that we've actually defined our function to predict the。 masked
    word let's actually do that for a couple of examples。 Here the first example I
    give it a。 code switched piece of text where the first few words are in English
    the last few words are in。 Spanish which just means live and let live。 So basically
    what I want to do is I want to predict。
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，现在我们已经定义了预测被遮罩词的函数，接下来我们来做几个例子。在第一个例子中，我给了一段切换语言的文本，前几个词是英文，最后几个词是西班牙文，意思就是“活着，让活着”。所以基本上，我想要预测的是这个被遮罩的词“Tejar”。
- en: this masked word Tejar。 Now the model is actually able to predict a lot of the
    words that make。 sense so if you look at a couple of them solo means just which
    means live and just live。 live and live well Tambian also live and also live so
    all of them make sense and it's able to。 recognize the context of what we're talking
    about even though there's mixed language。
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 现在模型实际上能够预测出很多有意义的词，所以如果你查看其中几个，"solo" 意为“只是”，而“live” 则意味着“活”，而“Tambian”也有“也活着”的意思。因此，所有的词都有意义，模型能够识别我们讨论的上下文，尽管语言混合。
- en: In this other example I give it the same piece of text but I want to predict
    we will know。 Now the good thing to notice over here is that if you look in the
    list of predicted tokens。 you will be able to see that you have Viva which means
    live and again it's able to recognize the。 context I think that's mainly because
    of another VV in this sentence。
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在另一个例子中，我给出了相同的文本，但我想要预测“we will know”。这里需要注意的是，如果你查看预测的词汇列表，你会看到“Viva”，意味着“活”，并且模型能够识别上下文，我认为这主要是因为句子中还有另一个“VV”。
- en: Now I also try to do this for a Hindi example because I'm much more familiar
    with Hindi。 and basically what I saw was that when you give it a piece of text
    which is very small。 it's able to predict this word which is the actual word and
    it's top one accuracy is very high but。 all of these other sentence predictions
    did not make sense at all given the input sequence。
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我还尝试用印地语进行这个实验，因为我对印地语更熟悉。基本上，我看到的是，当你给一段非常小的文本时，模型能够预测出实际的词，其准确度非常高，但其他的句子预测在给定的输入序列中根本没有意义。
- en: So now let's look at another code switched example and this is a slightly longer
    example in which we。 have Hindi script as well as English interspersed in the
    middle。 So the first few words and the boxed words you can see are in Hindi script。
    The remaining words are in English and you see these phrases that occur together
    a lot right like。
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们来看另一个代码切换的例子，这个例子稍微长一些，其中包含印地语和夹杂的英语。所以你可以看到前几个单词和框内的单词是印地语，剩下的单词是英语，你看到这些短语经常一起出现，比如。
- en: Hindi blogs， Hindi bloggers， popular Hindi blogs and so on。 So here's what I
    did。 I gave it this piece of text and asked it to predict the word blog。 Now in
    the predicted list of tokens you can see that it's actually recognizing this word
    as the。 first candidate itself and that is good。 In the other predictions you
    can see that it's giving you。
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 印地语博客、印地语博主、流行的印地语博客等等。所以我做了这个。我给它这段文本，并要求它预测单词“博客”。现在在预测的标记列表中，你可以看到它实际上把这个词识别为第一个候选，这很好。在其他预测中，你可以看到它给出了一些。
- en: the capitalized blog， publisher， forum which means that it's actually able to
    recognize。 the context of the given word and that's because it's appearing in
    multiple phrases right。 So you see in different phrases it's occurring as best
    Hindi blogs， popular Hindi bloggers。 popular Hindi blogs again。 So the reason
    that it's able to recognize publisher。
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 大写的“博客”、“出版商”、“论坛”意味着它实际上能够识别给定词的上下文，因为它在多个短语中出现。所以你可以看到它在不同短语中出现，比如“最佳印地语博客”、“流行的印地语博主”、“再次流行的印地语博客”。所以它能识别“出版商”。
- en: blog or forum is because it's reading all of them time and again and because
    it's pie directional。 So you will often see this。 So if you give but a very long
    piece of text which has。 too much of contextual information it will be able to
    recognize the word pretty nicely。 Irrespective of whether you have inter switched
    language in the middle。
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: “博客”或“论坛”的原因是它一次又一次地读取所有这些内容，而且因为它是双向的。所以你会经常看到这个。因此，如果你给它一段非常长的文本，里面有太多上下文信息，它会很不错地识别出这个词，无论你中间是否切换语言。
- en: Now another place where you see a lot of translated text is the lyrics。 So I'm
    a huge polywood fan and anytime I don't know the lyrics of a Bollywood song I'll
    just go to。 Google and search it out。 The results will usually be in the English
    alphabet though which is why。 we get a lot of translation。 So now I give birth
    this huge piece of text which is lyrics of a song。
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个看到大量翻译文本的地方是歌词。所以我非常喜欢宝莱坞，每当我不知道一首宝莱坞歌曲的歌词时，我都会去谷歌搜索。结果通常是用英文字母写的，这就是我们获得大量翻译的原因。所以现在我给它这段很大的文本，正好是歌词。
- en: or popular song in Bollywood and I ask it to predict this word which is MEIN。
    Now if you notice the next word to MEIN is HAI and it appears in another context
    again the next word。 is HAI and again the next word is HAI。 Now if you look at
    the list of predicted tokens you will。 see that it is able to recognize that the
    targeted word was MEIN and if you look at some of the。
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 或者流行的宝莱坞歌曲，我要求它预测这个词“MEIN”。现在如果你注意到“MEIN”旁边的下一个词是 HAI，它在另一个上下文中再次出现，下一个词也是 HAI。现在如果你查看预测的标记列表，你会看到它能够识别目标词是
    MEIN，如果你查看一些。
- en: tokens over there most of them are just appendings and if you look at one token
    which is HAI。 Now that appears a lot in the text right in fact that is the next
    word to our mast word。 and the reason it's able to or it's giving us this prediction
    is because it's seeing that this。 word occurs a lot in the same context。 So if
    you give birth a lot of information it will look at。
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 那里的标记大多数只是附加词，如果你看一个标记是 HAI。这个词在文本中出现得很频繁，实际上这是我们主词的下一个词。它之所以能给出这个预测，是因为它看到这个词在相同的上下文中出现得很多。所以如果你提供大量信息，它会查看。
- en: the right context it will look at the left context and it will be able to recognize
    the mast word。 given that the same word appears multiple times。 Okay so now that
    we've looked at how the mast language model performs let's look at how we can。
    evaluate certain tasks that we perform with birth right。 So let's say you have
    this multilingual piece， and you are trying to give POS tags to every word piece。
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 右侧上下文，它会查看左侧上下文，并能够识别出主词，因为同一个词出现了多次。好的，现在我们看过主语言模型如何表现后，让我们看看如何评估我们用它进行的某些任务。假设你有这段多语言文本，你正在尝试为每个单词部分提供
    POS 标签。
- en: The first thing that you want to do is you， want to organize it because that
    is how birth works you have to organize it into word pieces。 and the way you can
    assign them tags is by giving the first word piece of the associated word a tag。
    So which means that if you are looking at this sentence and Jim is one word hence
    and is another。 word hence and is divided into hen and son word pieces you would
    basically give a tag to the。
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 首先你需要做的是组织它，因为这就是出生的运作方式，你必须将其组织成词片。你可以通过给相关词的第一个词片打标签来为它们分配标签。这意味着如果你在看这句话，"Jim"是一个词片，而"and"是另一个。词片"and"被分为"hen"和"son"，你基本上会给这个词片打标签。
- en: first word piece hen and you would leave out the last word piece。 Similarly
    if over here。 if you are giving it a word piece of puppet and an ear you would
    leave out the last word piece。 and then just assign a tag to the first one。 Now
    there are various ways to do this you can either。 use the birth representations
    as input to another neural network or you can fine tune birth completely。
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个词片是"hen"，你会省略最后一个词片。同样在这里，如果你给一个词片"puppet"和"ear"，你也会省略最后一个词片，然后只给第一个词片打标签。现在，有多种方法可以做到这一点，你可以将**BERT**表示作为另一个神经网络的输入，或者完全微调**BERT**。
- en: For generation or evaluation metrics actually look pretty similar to what they
    would do a normal。 neural machine translation model。 If you are looking at the
    mass language model you would want to see。 with how much prediction or how much
    accuracy the birth model is actually predicting。 So you could。 look at the top
    10 accuracy or top 3 accuracy top 5 accuracy according to you。 Now a lot of the。
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 生成或评估度量实际上与正常的神经机器翻译模型所做的非常相似。如果你查看**大规模语言模型**，你想要了解的是，**BERT**模型的预测准确性有多高。所以你可以查看前10的准确性、前3的准确性或前5的准确性，这取决于你。
- en: metrics in your machine translation like beam surgeon blues core are pretty
    useful for multilingual。 generation and that is because the multilingual generation
    and bird is based on word pieces。 So in word pieces what we have is a common vocabulary
    across languages which means that you。 actually don't have to switch every time
    if you have a multilingual piece of data and figure out。
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 你的机器翻译中的度量标准，比如**蓝色分数**，对于多语言生成非常有用。这是因为多语言生成和**BERT**是基于词片的。在词片中，我们有一个跨语言的共同词汇，这意味着如果你有一个多语言数据片段，你实际上不需要每次都切换并找出。
- en: a language model for different languages。 You basically have this word pieces
    and you will just see which。 word piece is predicted after the other。 So in beam
    search what we do is we look at each time step。 and the few best candidates at
    that point we evaluate all paths that are arising from them。 So let's say at time
    step one we figured out that these are the 30 best candidates we would evaluate。
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 针对不同语言的语言模型。你基本上有这些词片，你将观察哪个词片在另一个词片之后被预测。因此，在束搜索中，我们在每个时间步中查看，并评估在该点上出现的最佳候选者。假设在时间步一，我们找出了这30个最佳候选者，我们将评估。
- en: 30 best candidates at each again time step for all of them and then at the end
    we calculate the best。 possible candidate for our translation。 You could do this
    for transliteration you could do this for。 code switch piece of data anything。
    Now another way to actually evaluate how good your generation is。 is through the
    blues core and blues core is pretty widely used if you've ever worked with。
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个时间步中为所有候选者选出30个最佳候选者，最后我们计算出最佳的翻译候选者。你可以为音译做这件事，也可以为代码切换的数据片段做这件事。现在，还有另一种评估生成质量的方法，就是通过**蓝色分数**，如果你曾经使用过，它的使用相当广泛。
- en: your machine translation you know about this but the main idea behind blues
    core is that you look at。 n grams so up to 1 gram 2 gram 3 gram 4 grams in the
    candidate sequence that you have and you。 compare it to the reference sequence
    and you figure out how similar they are。 And now just to summarize what we've
    done till now if you want to make your model your NLP model。
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 你了解机器翻译的内容，但**蓝色分数**的主要思想是，你查看候选序列中的n-gram，直到1-gram、2-gram、3-gram和4-gram，并将其与参考序列进行比较，找出它们的相似程度。现在，总结一下到目前为止我们所做的，如果你想让你的NLP模型。
- en: more multilingual here's what you need to decide you need to decide if it's
    even required if what。 you're catering to actually needs a lot of languages which
    languages those are do you have enough data。 for them if not you need to add data
    and this you can do by just converting with a rule based。 transliterator like
    i mentioned or if you have any neural machine models that are already trained。
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 关于多语言，你需要决定是否真的需要，您所面对的任务是否需要多种语言，这些语言具体是哪些？你是否有足够的数据？如果没有，你需要添加数据，你可以通过像我提到的规则基础的音译工具进行转换，或者如果你有任何已训练好的神经机器模型。
- en: you choose the multilingual pre-trained model that you want to utilize and then
    you find。 you according to task let's say you want to do a classification task
    you do a softmax layer on top。 of per and if you want to regeneration task you
    customize accordingly and voila you have your。 multilingual model and with that
    we reached the end of the presentation so in this talk we've seen。
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 你选择你想要利用的多语言预训练模型，然后你根据任务进行调整。假设你想做分类任务，你需要在顶部加一个softmax层。如果你想进行生成任务，你则相应地进行自定义，瞧，这就是你的多语言模型。至此，我们结束了本次演示，在这次讲座中我们看到了。
- en: transliterated and code switch data and how they are extremely different grammars
    to monolingual。 corpus's or normal languages that we've seen because they mix
    a variety of them we've seen some challenges。 that are associated with language
    identification with these kind of cases we've seen how we can。 actually augment
    datasets by building on our own and we can do that by either using a rule based。
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 音译和代码切换数据，以及它们与单语语料库或我们看到的普通语言之间的极大差异，因为它们混合了多种语言。我们看到了一些与这些情况相关的语言识别挑战，我们还看到了如何通过构建自己的数据集来增强数据集，我们可以通过使用基于规则的系统或从特定数据源使用机器翻译来实现。
- en: system or by using machine translation from a specific data source we've seen
    how multilingual。 board performs how it does on transliterated data on code switch
    data given enough length。 and given enough context and in the last we've seen
    that we can make an inletive presentation。 without having a word cloud all right
    and with that we reached the end of this presentation。
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到了多语言模型在音译数据和代码切换数据上的表现，只要长度足够且上下文充足，最后我们看到可以在没有词云的情况下进行直观演示。好了，我们的演示到此结束。
- en: if you have any questions feel free to leave them in the comment section below
    or you can find me on。 linkedin use it this keyword code and also available on
    github or feel free to go through the notebooks。 and ask any questions that you
    have and i just like to thank bycon and the python software。 foundation for organizing
    this thank you， thank you， [BLANK_AUDIO]。
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您有任何问题，请随时在下方评论区留言，或者在LinkedIn上找到我，使用这个关键词代码，我也可以在GitHub上找到，或者随意查看笔记本并询问您有的任何问题，感谢bycon和Python软件基金会组织此次活动，谢谢，谢谢，[BLANK_AUDIO]。
- en: '![](img/655df6621bc0adf7ffe519682da6a324_5.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](img/655df6621bc0adf7ffe519682da6a324_5.png)'
- en: you。
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 你。
