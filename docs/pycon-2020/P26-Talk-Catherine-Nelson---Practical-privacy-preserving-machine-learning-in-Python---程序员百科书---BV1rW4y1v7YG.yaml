- en: P26：Talk Catherine Nelson - Practical privacy-preserving machine learning in
    Python - 程序员百科书 - BV1rW4y1v7YG
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: P26：演讲者Catherine Nelson - 在Python中进行实用的隐私保护机器学习 - 程序员百科书 - BV1rW4y1v7YG
- en: '[silence]。'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '[沉默]。'
- en: '![](img/a45ae89f522de28987158e0b61b63946_1.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a45ae89f522de28987158e0b61b63946_1.png)'
- en: '![](img/a45ae89f522de28987158e0b61b63946_2.png)'
  id: totrans-3
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a45ae89f522de28987158e0b61b63946_2.png)'
- en: So， hi and welcome to my PyCon talk， Practical Privacy Preserving Machine Learning
    in PyCon。 I want to say thanks to all the PyCon organizers for all their。 hard
    work and clear communication this year。 And also thanks for the opportunity to
    still give this talk。 even though we don't get together in Pittsburgh。 [silence]，
    So。
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，嗨，欢迎来到我的PyCon演讲，主题是《在PyCon中进行实用的隐私保护机器学习》。我想感谢所有PyCon组织者今年的辛勤工作和清晰沟通。同时也感谢有机会尽管我们不能在匹兹堡见面，仍然可以进行这个演讲。[沉默]，所以。
- en: I'd like to introduce myself because it's going to be a。 while before you actually
    get to meet in person。 In my day job。 I'm a senior data scientist at Concur Labs，
    part of SAP Concur。 I work in a small team that drives innovation at SAP Concur，
    which is SAP's business。
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 我想介绍一下自己，因为在你们实际见到我之前还要等一段时间。在我的日常工作中，我是Concur Labs的高级数据科学家，属于SAP Concur。我在一个小团队中工作，推动SAP
    Concur的创新，这是SAP的业务。
- en: travel and expense tool。 And we evaluate new technologies and recommend them
    to the rest of the company。 In the past， we've built Concur， travel and expense。
    and all sorts of alternative interfaces from Slack and Outlook。 to Alexa and even
    expenses from a car。 And now we have a big focus on machine learning。
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 旅行和费用工具。我们评估新技术并向公司其他部门推荐它们。在过去，我们构建了Concur、旅行和费用，以及从Slack和Outlook到Alexa甚至汽车费用的各种替代接口。现在我们重点关注机器学习。
- en: and how we can recommend machine learning tools and technologies to the rest
    of Concur。 I'm also a co-organizer for Seattle PILADES。 I'd like to give a big
    shout out to Ms。 Meetop。 for getting me started in Python in the first place。
    And I'm also co-authoring an arrayly book called Building Machine Learning Pipelines。
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 以及我们如何向Concur的其他部分推荐机器学习工具和技术。我还是西雅图PILADES的共同组织者。我要特别感谢Ms。Meetop，让我最初接触Python。我还共同撰写了一本名为《构建机器学习管道》的书。
- en: and I'm a Google developer expert in machine learning。 But today I want to talk
    about one thing that I'm really interested in。 that is privacy preserving machine
    learning。 I first got interested in data privacy in early 2018。 when everyone
    was thinking and talking a lot about the GDPR， the general data protection regulation。
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我是机器学习方面的Google开发者专家。但今天我想谈论我非常感兴趣的一件事，那就是隐私保护机器学习。我第一次对数据隐私产生兴趣是在2018年初，当时每个人都在讨论GDPR（一般数据保护条例）。
- en: as these laws were being introduced。 But privacy issues definitely haven't gone
    away。 We're still talking about a lot， particularly in the current situation。
    as you can see by all these headlines from the past few weeks。 And machine learning
    is deeply involved in issues around privacy。
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 随着这些法律的出台，隐私问题确实没有消失。我们仍然在讨论很多，特别是在当前情况下，正如你可以从过去几周的所有头条新闻中看到的那样。机器学习与隐私问题息息相关。
- en: As you can see from this tweet from back in 2016 from Andrew。 the standard view
    is that more data is better， especially when it comes to deep neural networks。
    But let's not be so abstract about data， let's be specific。 What data should we
    be worrying about when we talk about data privacy？ Well。
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 从2016年Andrew的这条推特中可以看出，标准观点是更多的数据更好，尤其在深度神经网络方面。但我们不要对数据过于抽象，应该具体讨论。当我们谈论数据隐私时，我们应该关注哪些数据呢？那么。
- en: we can divide this into two categories。 There's private data。 which is personally
    identifiable information， such as your name， address， your email。 And then there's
    quasi identifying data that partially identifies someone。 if you bring enough
    of it together。 So things like location， credit card transactions。
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将其分为两类。一类是私人数据，即可识别的个人信息，例如你的姓名、地址、电子邮件。另一类是部分识别某人的准识别数据。如果将足够多的数据汇集在一起，就会部分识别某人。比如位置、信用卡交易。
- en: And there's also a lot of private data in user inputs。 If there's free text
    forms。 you'll end up with all sorts of things like， names， email addresses， phone
    numbers。 And then there's sensitive data that has some kind of consequences if
    it leaks out。 So this could be things like health related personal data or company
    proprietary data。
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 用户输入中也有很多私人数据。如果有自由文本表单，你会遇到各种各样的东西，比如姓名、电子邮件地址、电话号码。还有一些敏感数据，如果泄露会产生某种后果。这可能包括健康相关的个人数据或公司专有数据。
- en: So when we're dealing with private or sensitive data。 the simplest way to keep
    it private is to just not even collect it in the first place。 We have a project
    that helps us with this。 It's called the Data Washing Machine。 It removes personal
    data from natural language， such as names， addresses。
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，当我们处理私人或敏感数据时，保持其私密的最简单方法就是根本不收集这些数据。我们有一个帮助我们实现这一目标的项目，叫做“数据洗衣机”。它可以从自然语言中移除个人数据，例如姓名、地址。
- en: '![](img/a45ae89f522de28987158e0b61b63946_4.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a45ae89f522de28987158e0b61b63946_4.png)'
- en: all that kind of thing。 We deal with a lot of receipt data and concur。 and that
    contains a lot of personal data that we don't actually need to deliver the service
    we provide。 So we use machine learning models to identify the different parts
    of PII that we might find。 in the receipt， such as names， addresses， phone numbers
    and so on。 And then we can remove this data。
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这类事情我们处理了大量的收据数据和共识，而这些数据中包含了许多我们实际上并不需要提供服务的个人数据。因此，我们使用机器学习模型来识别收据中可能存在的各种个人可识别信息（PII），例如姓名、地址、电话号码等等。然后我们可以移除这些数据。
- en: So that works well for us in our situation， but often there are use cases where
    without collecting the data。 we can't build the model at all。 And also if we don't
    collect sensitive data such as people's genders or their ages。 we don't know if
    our model is fair to all of our users。 So we do need to collect that sensitive
    data。 And it seems like privacy and machine learning are opposed。
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 这在我们的情况下运行良好，但通常有一些用例，在不收集数据的情况下，我们根本无法构建模型。如果我们不收集敏感数据，例如人们的性别或年龄，我们就不知道我们的模型对所有用户是否公平。因此，我们确实需要收集这些敏感数据。看起来隐私和机器学习是相对立的。
- en: because data privacy is knowing less about you， but machine learning wants to
    know more about you。 But actually， privacy and machine learning can have the same
    goals。 In many cases。 the idea is that we want to learn about a population not
    about a single individual。 Both of these technologies want to generalize， not
    personalize。
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 数据隐私意味着对你的了解更少，但机器学习希望对你了解得更多。实际上，隐私和机器学习可以有相同的目标。在许多情况下，理念是我们希望了解一个群体，而不是单个个体。这两种技术都希望实现概括，而不是个性化。
- en: And my motivation with this talk is to show how it's still possible to build
    accurate machine learning models。 and also give your users privacy。 And there's
    privacy preserving machine learning technologies to help us with this。 But most
    of these appear in research papers。 How do I do this practically in Python？ To
    do this。 the most important question to ask is， who do you trust with your personal
    data？
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我这次演讲的动机是展示如何仍然可以构建准确的机器学习模型，同时也能保护用户的隐私。为了实现这一点，我们有隐私保护的机器学习技术来帮助我们。但大多数技术都出现在研究论文中。那么我该如何在Python中实际操作呢？要做到这一点，最重要的问题是，你信任谁处理你的个人数据？
- en: And to put this into perspective， I'll talk about a very simplified machine
    learning system。 with all the players who we might trust or not trust。 And in
    our simple machine learning system。 the data is collected from the user， put into
    some central storage system。 And then it's transferred from that storage system
    to a machine learning model。
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让这一点更清晰，我将讨论一个非常简化的机器学习系统，涉及到我们可能信任或不信任的所有参与者。在我们的简单机器学习系统中，数据从用户那里收集，并放入某个中央存储系统。然后，它从该存储系统转移到机器学习模型中。
- en: and it's used to train that model。 And then the model makes some predictions。
    and these are returned to the user and shown to them。 And I'll discuss three different
    ways of providing privacy in our machine learning system。 First one of these is
    differential privacy。 If you want to ensure that there's no personal data in your
    model predictions。
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 它被用来训练模型，然后模型做出一些预测，并将这些预测返回给用户并展示给他们。我将讨论在我们的机器学习系统中提供隐私的三种不同方式。其中一种是差分隐私。如果你想确保模型预测中没有个人数据。
- en: then this is the one to use。 Machine learning models and in particular deep
    learning models。 can expose rare examples from their training data in their predictions。
    And this gives users a loss of privacy。 And the definition of differential privacy
    is a formalization of the idea。 that a query should not reveal whether a person
    is in a data set。
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你需要使用这个方法，机器学习模型，特别是深度学习模型，可能会在其预测中暴露其训练数据中的稀有示例。这会导致用户隐私的丧失。差分隐私的定义是这一理念的形式化，即查询不应揭示某人是否在数据集中。
- en: So the probability that a query or a transformation gives a certain result。
    is nearly the same on both these data sets。 One of them has the central person
    and the other one doesn't。 The idea is that if your personal data doesn't affect
    the result of something。 then you're happy to include it in a data set， because
    there's some kind of deniability。
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，查询或变换给出某个结果的概率在这两个数据集上几乎是相同的。一个数据集包含中心人物，而另一个则没有。这个想法是，如果你的个人数据不影响某个结果，那么你愿意将其包含在数据集中，因为有某种否认性。
- en: Or to put it another way， this gives you the ability to say that for any possible
    output of the program。 well， that was just as likely without my data as with it。
    And the way differential privacy is achieved is by some kind of randomization。
    that masks an individual's data values。 There's many different ways to achieve
    differential privacy。
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，这使你能够说，对于程序的任何可能输出，嗯，这在没有我的数据时和有数据时是一样可能的。而实现差分隐私的方法是某种随机化，掩盖个人数据值。有很多不同的方法可以实现差分隐私。
- en: but I'll briefly talk through one of the simplest， which is the concept of randomized
    response。 So this is commonly used in survey questions。 So if you've got a question
    that has some kind of sensitive answers。 such as did you stay out late last night，
    then you can flip a coin。 And if the coin comes up heads。 then the person taking
    the survey answers truthfully， if it comes up tails。
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 但我会简要介绍其中最简单的一个，即随机响应的概念。这通常用于调查问题。如果你有一个敏感回答的问题，比如你昨晚是否熬夜，那么你可以翻转硬币。如果硬币显示正面，那么调查者会如实回答，如果显示反面。
- en: then they flip again and select randomly from yes or no。 And because we know
    the probabilities in flipping the coin， if we ask a lot of people this question。
    we can back calculate the proportion of people who answered yes or no to this
    question。 And this gives people some deniability。 They could say， oh， it was just
    the。
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 然后它们再次翻转，并随机选择“是”或“否”。因为我们知道翻转硬币的概率，如果我们问很多人这个问题，我们可以反向计算回答“是”或“否”的人比例。这给人们提供了一些否认性。他们可以说，哦，我只是给了随机答案。这样他们就不会失去隐私。但这与机器学习有什么关系呢？这只是一个总体统计。
- en: I just gave the random answer。 And that way they don't lose privacy。 But what's
    this have to do with machine learning？ This is just an overall statistic。 Well。
    one of the ways that we can have differential privacy in machine learning。 is
    provided in the TensorFlow Privacy Library， which lets us train differentially
    private models。
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，我们在机器学习中实现差分隐私的方法之一是通过TensorFlow隐私库，它让我们训练具有差分隐私的模型。
- en: And it offers us a strong mathematical guarantee that it prevents the user's
    data being memorized。 but it still gets us the most accurate model that we can。
    And this technology is particularly useful because it fits in really easy to our
    existing workflows。 So if we start with a classic simple TensorFlow Keras model，
    we've got three layers here。
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 它为我们提供了强有力的数学保证，防止用户数据被记住，但仍能获得最准确的模型。这项技术特别有用，因为它很容易融入我们现有的工作流程。因此，如果我们从一个经典简单的TensorFlow
    Keras模型开始，这里有三层。
- en: two fully connected dense layers and a sigmoid output layer。 And we want to
    add privacy。 add differential privacy to the simple Keras model。 What we have
    to do is provide a new optimizer and a new loss。 So in TensorFlow Privacy。 there's
    this gradient descent Gaussian optimizer。 And this takes stochastic gradient descent。
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 两个全连接的密集层和一个sigmoid输出层。我们想要添加隐私，即向简单的Keras模型添加差分隐私。我们必须提供一个新的优化器和新的损失函数。因此，在TensorFlow隐私中，有这个梯度下降高斯优化器。它采用随机梯度下降。
- en: but adds Gaussian noise to it。 And there's a couple of really important new
    parameters that go in here that aren't in a normal optimizer。 There's this L2
    non-clip and this noise multiplier。 So what this means is it clips the gradients
    so that they don't get too large。 and it varies the amount of noise that goes
    into the model。 So what's going on here？ Well。
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 但它添加了高斯噪声。而且这里有几个重要的新参数，这些参数在普通优化器中并不存在。有这个L2非剪裁和噪声乘数。这意味着它会剪裁梯度，使其不会过大，并且会变化输入模型的噪声量。那么这里发生了什么呢？好吧。
- en: instead of the normal batch of data that we send to the model， we split them
    into many batches。 smaller batches of data。 And for each one of these， we clip
    the gradients。 we cut off the largest values， because we don't want the model
    to be affected by outliers。 because that would break the differential privacy
    promise of the transformation not being affected。
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将正常发送给模型的数据批次分成许多更小的数据批次。对于每一个，我们裁剪梯度，切掉最大值，因为我们不希望模型受到异常值的影响，因为这会破坏转化未受影响的差分隐私承诺。
- en: too much by one individual。 We then average the gradients after their clips
    and add noise so that。 again， the gradients from the individual person are masked。
    And then we pass these to the model。 And then once we've done that， we can pass
    our new optimizer。 and our new loss to the model and then compile it and fit it
    as normal。
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 由单个个体造成的影响过大。然后，我们在裁剪后的梯度上取平均，并添加噪声，以使个体的梯度被掩盖。接着，我们将这些传递给模型。完成后，我们可以将新的优化器和新的损失传递给模型，然后像往常一样编译并拟合。
- en: This is just a standard Keras training setup， but we passed in that new optimizer
    and loss。 The next question we have to ask in differential privacy is how do we
    measure the amount of privacy。 that we've produced here？ So we have this concept
    called epsilon。 epsilon is one of the parameters in differential privacy that
    measures how excluding or including。
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这只是一个标准的 Keras 训练设置，但我们传入了新的优化器和损失。在差分隐私中，我们下一个需要问的问题是，我们如何衡量在这里产生的隐私量？我们有一个称为
    epsilon 的概念。epsilon 是差分隐私中的一个参数，用于衡量包含或排除的程度。
- en: an individual point is going to change the probability of any particular transformation
    occurring。 So e to the power of epsilon is the maximum difference between the
    outcome of two transformations。 or two model predictions in this case。 And if
    epsilon is small， we've added more noise。 we've clicked the gradients more， then
    the transformation is more private。
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 个别数据点将改变任何特定转化发生的概率。因此，e 的 epsilon 次方是两次转化结果之间的最大差异，或者在这种情况下是两个模型预测之间的最大差异。如果
    epsilon 很小，我们添加了更多噪声，裁剪了梯度，那么转化就更具隐私性。
- en: And there's a smaller probability of the model changing if one person's data
    is included or excluded。 And we can calculate this using some of the built-in
    methods in TensorFlow privacy。 In particular。 we use this compute_db_sdd_privacy。
    Sdd is stochastic gradient descent。 the optimizer that we used in the model。 And
    we pass in the batch size that we used and the noise multiplier。
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个人的数据被包含或排除，模型变化的概率会更小。我们可以利用 TensorFlow 隐私中的一些内置方法来计算这一点。具体来说，我们使用 `compute_db_sdd_privacy`。Sdd
    是随机梯度下降，是我们在模型中使用的优化器。我们传入使用的批量大小和噪声倍增器。
- en: we tell it the number， of epochs that we trained for。 and we give it a value
    of delta which is one over the approximate， size of the data set。 And from this
    we can get a calculation of epsilon and know how private our model is。 Differential
    privacy is particularly useful when you don't want to expose predictions from
    the model。
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们告诉它训练的 epoch 数，并给它一个 delta 值，即数据集大小的倒数。由此我们可以计算出 epsilon，并知道我们的模型有多隐私。差分隐私在你不想暴露模型预测时特别有用。
- en: But to do this， you still need to collect the data in the first place。 You still
    need users to give you access to their raw data。 And it's particularly useful
    because it provides mathematical definitions and guarantees of privacy。 The second
    piece of privacy preserving technology I'd like to talk about today is encrypted
    machine learning。
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 但要做到这一点，你仍然需要首先收集数据。你仍然需要用户给你访问他们原始数据的权限。这尤其有用，因为它提供了数学定义和隐私保证。我今天想谈的第二个隐私保护技术是加密机器学习。
- en: There's two ways we can use this in our machine learning system。 And the first
    one is when we encrypt the data before it gets moved to the central storage system。
    and before the model is trained。 And then the predictions are decrypted and then
    passed back to the user。 And we can do this in Python with TF encrypted which
    handles all the encryption part for us。
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在机器学习系统中以两种方式使用这一点。第一种是我们在数据移动到中央存储系统之前对其进行加密，并在模型训练之前。然后预测被解密并传回用户。我们可以通过
    TF encrypted 在 Python 中完成这一过程，它为我们处理所有加密部分。
- en: It keeps the basic Keras interface。 And it allows us to share previously unused
    data。 so data that is normally too sensitive for us to touch。 Because it can be
    encrypted before it even leaves its original location。 To do this。 we just define
    a function that provides batches of training data。 And then add the data at tfe。
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 它保留了基本的 Keras 接口。并且允许我们共享以前未使用的数据。因此，对于我们来说，通常太敏感的数据可以在离开其原始位置之前进行加密。为此，我们只需定义一个提供训练数据批次的函数，然后将数据添加到
    tfe。
- en: localcomputation。 Tfe in this case is TF encrypted。 And this handles the encryption
    process for us。 We can then pass that data to our Keras model。 Import Tfe encrypted
    as tfe。 And continue as normal。 We build our model， define it using the Keras
    sequential API， add our encrypted layers to it。 And then we can go ahead and train
    and make predictions on that encrypted data。
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 本地计算。在这种情况下，Tfe 是 TF 加密，它为我们处理加密过程。然后我们可以将数据传递给我们的 Keras 模型。导入 Tfe 加密为 tfe，然后继续正常操作。我们构建模型，使用
    Keras 顺序 API 定义它，添加我们的加密层。然后我们可以继续训练并对这些加密数据进行预测。
- en: '![](img/a45ae89f522de28987158e0b61b63946_6.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a45ae89f522de28987158e0b61b63946_6.png)'
- en: The second way of using encryption in our machine learning system is to encrypt
    a model that's already been trained。 so that it serves encrypted predictions。
    And in this case。 we collect the raw data from the user and transfer that to a
    central storage。 train the model on raw data， encrypt the model， and then the
    predictions are encrypted。
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的机器学习系统中使用加密的第二种方式是对已经训练好的模型进行加密，以便它能够提供加密预测。在这种情况下，我们从用户那里收集原始数据，并将其传输到中央存储，使用原始数据训练模型，加密模型，然后预测也被加密。
- en: And we can also do this in TF encrypted。 We can use the tfe。keras。models。clone
    model argument and this will give us encrypted predictions。 There's a few more
    steps to it in this。 So what's going on here is that first of all。 we load and
    preprocess the data locally on the client。 Then the data is encrypted on the client。
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以在 TF 加密中做到这一点。我们可以使用 tfe.keras.models.clone_model 参数，这将为我们提供加密预测。在这方面还有几个步骤。那么这里发生了什么呢？首先，我们在客户端本地加载和预处理数据。然后数据在客户端进行加密。
- en: The encrypted data is sent to the service。 And they will make a prediction on
    the encrypted data。 send the encrypted prediction back to the client， and decrypt
    the prediction on the client and show the result to the user。 But this gives you
    a very nice way of taking a model that you've already trained and turning it into
    something that's more private。 So when should we use encrypted machine learning？
    There's two options encrypting the training data or just the model。
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 加密数据被发送到服务。然后他们会对加密数据进行预测，将加密预测发送回客户端，并在客户端解密预测，将结果展示给用户。但这为你提供了一种很好的方式，将你已经训练好的模型转变为更私密的东西。那么，何时应该使用加密机器学习呢？有两种选择：加密训练数据或仅加密模型。
- en: We can encrypt the data if the model owner is not trusted。 And we can encrypt
    the model if the training data is public but inference is private。 This gives
    us the possibility of training models on data that's too sensitive to share。 So
    there's lots of medical applications here。 The third technology I want to talk
    about today is federated learning。
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 如果模型拥有者不被信任，我们可以对数据进行加密。如果训练数据是公共的，但推理是私有的，我们可以对模型进行加密。这使我们能够在过于敏感的数据上训练模型。因此，这里有很多医疗应用。我今天想讨论的第三项技术是联邦学习。
- en: And this is suitable for when data is spread across lots of different devices
    like mobile phones。 So this technology is used by Google in their Gboard keyboard
    to preserve privacy。 In federated learning， the raw data never leaves a user's
    device。 And instead。 the model weights are sent to each device。 The weights are
    updated。
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这适用于数据分布在许多不同设备（如手机）上的情况。因此，这项技术被谷歌用于他们的 Gboard 键盘，以保护隐私。在联邦学习中，原始数据永远不会离开用户的设备。相反，模型权重会发送到每个设备，权重会被更新。
- en: And then they return to some secure aggregation service。 And then they're combined
    together into the model。 And the model can make predictions that are returned
    to the user。 And this gives users privacy because the model owner can never see
    their raw data。 In Python。
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 然后它们返回到某个安全聚合服务，并结合到模型中。该模型可以进行预测，并将结果返回给用户。这为用户提供了隐私，因为模型拥有者永远无法看到他们的原始数据。在
    Python 中。
- en: federated learning is provided by the Pysys project。 And this is for PyTorch
    and TensorFlow。 And by TensorFlow federated。 I'm not going to give a code example
    here because it gets really complex with all the different bits of infrastructure
    that you need。
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 联邦学习是由Pysys项目提供的。这适用于PyTorch和TensorFlow，以及TensorFlow federated。我在这里不打算给出代码示例，因为涉及到许多不同的基础设施时会变得非常复杂。
- en: But I'll walk through all the steps that make a very simple federated learning
    set up。 So the first thing is to create the virtual workers that can live on each
    device。 Then we make a set of pointers that point to the training data that's
    already living on each worker。 This might be what you typed into your keyboard
    or something like that。
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 但我会逐步讲解一个非常简单的联邦学习设置所需的所有步骤。首先是创建可以在每个设备上运行的虚拟工作者。然后我们制作一组指针，指向已经存在于每个工作者上的训练数据。这可能是你在键盘上输入的内容或类似的东西。
- en: Then we send the model weights from the model owner to each worker。 The model
    is trained on each worker device。 And then the workers send the weights back to
    the model owner after they've been updated。 And they also send the loss back to
    the model owner。 And then the model owner combines the model into the new model。
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们将模型权重从模型所有者发送到每个工作者。模型在每个工作者设备上训练。然后工作者在更新后将权重发送回模型所有者。同时，他们也将损失值发送回模型所有者。然后模型所有者将模型合并为新模型。
- en: which has been improved by the addition of the new data。 What's missing from
    this very simple walkthrough is that we haven't actually provided any increase
    in privacy yet。 We can often infer the original data from the model weights。 So
    we need to add some secure way of averaging the model weights before they can
    reach the model owner。
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 通过添加新数据，已经对其进行了改进。这个非常简单的操作步骤中缺失的是，我们实际上还没有提供任何隐私保护的增强。我们往往可以从模型权重推断出原始数据。因此，我们需要在模型权重到达模型所有者之前，添加一些安全的平均方式。
- en: And this is what you do in a full-blown system。 And also scale。 you need something
    that's going to increase in size with many， many devices。 Federated learning is
    particularly useful when we want to build personalized models。 like in the keyboard
    example I mentioned already。 It's good when the data is already decentralized。
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是你在完整系统中所做的事情。同时也要考虑扩展性。你需要某种东西，能够随着许多设备的增加而增大。联邦学习在我们想要构建个性化模型时特别有用，就像我已经提到的键盘示例。当数据已经是分散式时，它是很有帮助的。
- en: so it's being collected on users' devices or on a browser， and obviously sensitive
    or personal data。 And we also need the labels to have already been added because
    we can't look at the data after it's been collected。 So privacy preserving machine
    learning really comes down to who do you trust。 who gets to see the raw data。
    And if you trust the model owner。
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 因此数据是在用户的设备上或浏览器中收集的，显然是敏感或个人数据。而且我们还需要标签已经被添加，因为在数据收集后我们无法查看数据。所以，隐私保护的机器学习实际上归结为你信任谁，谁能看到原始数据。如果你信任模型所有者。
- en: then they can use the raw data but offer private predictions。 and we can do
    this with differential privacy or encryption。 And then if you don't trust the
    model owner with your data。 you can encrypt the data or keep it on each user's
    device。 And just a few caveats here。
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 然后他们可以使用原始数据，但提供私人预测。我们可以通过差分隐私或加密来实现这一点。如果你不信任模型所有者处理你的数据，可以加密数据或将其保留在每个用户的设备上。这里有几个注意事项。
- en: Right now there's always a cost to adding privacy to make your machine learning
    model。 whether that's lower accuracy， longer training times， or a more complex
    infrastructure。 Don't assume that just because you're using one of these technologies。
    you're still providing complete privacy for your users。
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，添加隐私以使你的机器学习模型是有成本的，无论是准确性降低、训练时间延长，还是基础设施更复杂。不要假设仅仅因为你在使用这些技术中的某一种，就意味着你为用户提供了完全的隐私保护。
- en: And this isn't going to save you from all ethical issues。 If the product that
    you're building isn't ethically sound in the first place。 this isn't going to
    help。 If you'd like to learn more about the technologies I mentioned today。 you
    can read about them in the book that I'm currently co-authoring with Hannah's
    Hacker。
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 这并不能让你免于所有伦理问题。如果你构建的产品本身就没有伦理基础，这并不会有所帮助。如果你想了解更多关于我今天提到的技术，可以在我目前与汉娜·哈克合著的书中阅读。
- en: Building Machine Learning Pipeline， which is out through O'Reilly。 I'd also
    encourage you to support the open source project I've mentioned， TF encrypted。
    PIPF and TensorFlow privacy。 And if you've got questions， you can find me on Twitter。
    Thank you very much for listening。 [BLANK_AUDIO]。
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 构建机器学习管道，这本书由O'Reilly出版。我还鼓励你支持我提到的开源项目，TF加密、PIPF和TensorFlow隐私。如果你有问题，可以在Twitter上找到我。非常感谢你的聆听。[BLANK_AUDIO]。
- en: '![](img/a45ae89f522de28987158e0b61b63946_8.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a45ae89f522de28987158e0b61b63946_8.png)'
