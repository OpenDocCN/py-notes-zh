- en: P83：Talk_ Jigyasa Grover_Rishabh Misra_ Sculpting Data for Machine Learning
    V02 - VikingDen7 - BV1f8411Y7cP
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: P83：Talk_ Jigyasa Grover_Rishabh Misra_ 雕刻机器学习的数据 V02 - VikingDen7 - BV1f8411Y7cP
- en: \>\> Hi， everyone。 We'll start off with our next talks。 We have Jigess， our
    grower， and。
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: \>\> 大家好。我们将开始下一场演讲。我们有我们的种植者Jigess和。
- en: '![](img/56ec2fbcfc37569d605d60b3f8a7b8ec_1.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](img/56ec2fbcfc37569d605d60b3f8a7b8ec_1.png)'
- en: '![](img/56ec2fbcfc37569d605d60b3f8a7b8ec_2.png)'
  id: totrans-3
  prefs: []
  type: TYPE_IMG
  zh: '![](img/56ec2fbcfc37569d605d60b3f8a7b8ec_2.png)'
- en: Richard Mishra， who will be consulting data for machine learning。 We do not
    have Q and。 A's during or after the session， but if you would like to ask questions
    to the speakers。 feel free to meet them in the hallway。 Yep， that's pretty much
    it。 Over to you guys。 \>\> Thank you。 \>\> Hey， everyone。 I'm Jigess， our grower，
    and I work as a machine learning engineer in。
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 理查德·米什拉，他将为机器学习提供数据咨询。在会议期间或结束后我们没有问答环节，但如果你想向演讲者提问，欢迎在走廊与他们交流。好的，就这些。现在交给你们。
    \>\> 谢谢。 \>\> 嗨，大家好。我是Jigess，我们的种植者，我是一名机器学习工程师。
- en: the Performance Ads ranking team at Twitter。 And my co-presenter， Richard Mishra，
    he works。 in the content quality team at Twitter。 Today， we'll be talking about
    how we can sculpt。 the right kind of data for our machine learning models。 The
    last couple of years have seen。 an immense growth of machine learning in multiple
    domains。 From influencing our shopping carts。
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: Twitter上的广告排名团队。而我的共同演讲者理查德·米什拉，他在Twitter的内容质量团队工作。今天，我们将讨论如何为我们的机器学习模型雕刻出正确的数据。在过去的几年里，机器学习在多个领域经历了巨大的增长。从影响我们的购物车开始。
- en: to cars self-driving themselves around the town。 Unquestionably， machine learning
    is。 the most used and the most abused and the most used sub-domain of artificial
    intelligence。 presently。 It is being wielded in improving healthcare and advancing
    warfare， scrutinizing。 your resume to determine your credit worthiness in creating
    music and meaningful lyrics， to。
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 让汽车在城镇里自动驾驶。毋庸置疑，机器学习是目前使用最广泛、滥用最严重的人工智能子领域。它正在改善医疗保健和推动战争，审查你的简历以确定你的信用worthiness，创作音乐和有意义的歌词，等等。
- en: synthesizing pictures and videos of people non-existent on this planet。 Regardless
    of。 our fascination or load for it， it is influencing our decision making power
    heavily and is dominating。 our lives every millisecond。 So dominant leaps are
    leaving the scientists， investors， policy。 makers， business leaders and audience
    bedazzled。 That human-like intelligence in machines might。
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在合成不存在于这个星球上的人的图片和视频方面。无论我们对此的兴趣或负担如何，它都在深刻影响我们的决策能力，并在每一毫秒中主导我们的生活。因此，巨大的进步让科学家、投资者、政策制定者、商业领袖和观众感到眼花缭乱。机器中的类人智能可能。
- en: be just around the corner。 Nonetheless， progress in machine learning has been
    very impressive。 but there's a lot of pending explanation and examination which
    keeps this research going， on。 So for the state-of-the-art machine learning algorithms
    to work their magic， it's important。 to focus on three key dimensions。 Well calibrated
    data， sophisticated algorithms and efficient。
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 可能就在拐角处。然而，机器学习的进展非常令人印象深刻，但还有许多待解释和检查的内容，促使这项研究继续进行。因此，为了让最先进的机器学习算法发挥其魔力，关注三个关键维度是重要的。经过良好校准的数据、复杂的算法和高效的。
- en: computation。 Since we have progressed from the rule-based approach to a more
    data-driven， approach。 it goes without saying that machine learning algorithms
    are trained to capture。 implicit information from the data provided。 It is true
    that these approaches are very。 data hungry and without going into a lot of details，
    for example deep learning algorithms。
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 计算。由于我们已经从基于规则的方法进步到更数据驱动的方法，毫无疑问，机器学习算法是经过训练以从提供的数据中捕获隐含信息的。确实，这些方法非常“数据饥渴”，而不去深入细节，比如深度学习算法。
- en: they have a lot of parameters that need to be tuned and therefore need a lot
    of data in。 order to come up with a more generalizable model。 So in that sense，
    having a lot of data。 is the key to coming up with good training sets for those
    approaches。 So look at this。 pyramid which is the AILR key of needs。 It is drawn
    parallel to Maslow's hierarchy of。
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 它们有很多参数需要调整，因此需要大量数据来提出更具普适性的模型。因此，从这个意义上说，拥有大量数据是为这些方法提出良好训练集的关键。看看这个。金字塔，它是AILR的需求关键。它与马斯洛的需求层次理论平行。
- en: human needs by Monica Rogatti， who is a renowned data scientist and AI advisor。
    She puts forward。 that worthy data collection forms the foundation of this pyramid
    of the AILR key of needs。 Data。 literacy， data collection and data flow。 These
    forms the basic needs which must be satisfied。 in order for us to achieve self-actualization
    or nirvana which in our case would be the。
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 这是著名的数据科学家和人工智能顾问莫尼卡·罗加蒂提出的“人类需求”。她指出，值得的数据收集构成了AILR需求金字塔的基础。数据素养、数据收集和数据流动，这些构成了必须满足的基本需求，以便我们实现自我实现或涅槃，在我们这个案例中将是。
- en: attainment of artificial intelligence。 So nonetheless， the quantity of data
    we also。 have to take care of about the type of data that we feed into the algorithms
    which has。 a profound effect on the success of the algorithm。 As we say garbage
    in， garbage out。 So machine。 learning models are essentially as good or as bad
    as the data that you have。 Peter Norvig。
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 达成人工智能的目标。因此，除了数据的数量，我们还必须关注输入算法的数据类型，这对算法的成功有深远影响。正如我们所说，垃圾进，垃圾出。因此，机器学习模型的好坏本质上取决于你所拥有的数据。——彼得·诺维格。
- en: has put it in very clearly that more data beats clever algorithms but better
    data beats more， data。 So in that case， we should talk about data， curation and
    first of all， let's talk about academia。 Core machine learning and data science，
    research groups in academia are oriented more towards novelty and advancing the
    fields。 Many are times weighing it much more than the money making logic or performance
    scaling。
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 他非常清楚地指出，更多的数据胜过聪明的算法，但更好的数据胜过更多的数据。在这种情况下，我们应该讨论数据、数据策划，首先，让我们谈谈学术界。核心机器学习和数据科学的学术研究团队更注重新颖性和推进领域，往往比盈利逻辑或性能扩展更为重要。
- en: In this attempt to work on new problems， finding collaborators from pertinent
    domains and seeking。 funding for research groups are not the only obstacles they
    face。 They also have to look。 for relevant data sources in the absence of in-house
    data， contrary to corporate giants。 like Facebook， Google who have access to tons
    of data。 So these established organizations。
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在努力解决新问题的过程中，寻找相关领域的合作者和为研究团队寻求资金并不是他们面临的唯一障碍。他们还必须在没有内部数据的情况下寻找相关的数据源，这与像Facebook、Google这样的企业巨头不同，后者可以访问大量数据。因此，这些成熟的组织。
- en: on the other hand seldom have obstacles in obtaining computation power， hiring
    folks。 with expertise in corresponding domains or accessing relevant data。 The
    challenge， however。 comes in scaling up the solutions to massive user base。 Any
    organization that would have。 a lot of unstructured data available on its hand
    in the form of raw logs， however， developing。
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，获取计算能力、聘用相关领域的专家或访问相关数据几乎没有障碍。然而，挑战在于将解决方案扩展到庞大的用户基础。任何拥有大量非结构化数据（如原始日志）的组织在这一点上都是发展中的。
- en: data processing pipeline for them remains a task。 And for creating such resilient
    pipelines。 apart from relevant technology， we also require the skills to identify
    and curate meaningful。 data sets， unbiased data sets from the sea of unstructured
    data that we have。 And that's。 where we go for search of data sources。 And it
    should be one to take it。 So， yeah。
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 数据处理管道对他们来说仍然是一项任务。创建这样稳健的管道，除了相关技术外，我们还需要识别和策划有意义的数据集、无偏见的数据集的技能，这些数据集来自我们拥有的大量非结构化数据。这就是我们寻找数据源的地方。因此，确实应该有所作为。所以，没错。
- en: so there can be like different approaches to building like a good quality data
    set。 And。 like we'll be like giving you guys some overview on like how we can
    approach that。 Basically。 some pointers regarding that。 But before we can do that，
    like let me introduce you to。 three of our data sets that we have collected ourselves。
    And these are available on Kaggle。
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，可以有不同的方法来构建高质量的数据集。我们将为大家提供一些关于如何实现这一点的概述。基本上，一些关于这方面的提示。但是在我们这样做之前，让我给你们介绍我们自己收集的三个数据集。这些数据集在Kaggle上可用。
- en: so feel free to explore them。 So， the first data set is clothing fed data set。
    So， this。 data set was collected from our e-commerce website called mod cloth。
    So， there people。 kind of report their transactions that they have done。 And what
    all like they have liked。 about the product and specifically whether the product
    was fitting to them or not。 So。
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 所以请随意探索它们。第一个数据集是服装数据集。这个数据集来自我们名为ModCloth的电子商务网站收集。在这里，人们报告他们的交易情况，以及他们对产品的喜好，特别是产品是否合适他们。
- en: the data which we have here is user ID， product ID， fit feedback， size purchased，
    ratings。 and reviews of the product that has been purchased and some sort of like
    customer measurements。 And second data set we have is sarcasm detection data set。
    So， these we have collected from。 two website。 So， the onion and half post。 So，
    past studies in sarcasm detection were mostly。
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们这里的数据包括用户ID、产品ID、适合度反馈、购买的尺码、评分以及已购买产品的评论和某种客户测量数据。我们有的第二个数据集是讽刺检测数据集。这些数据是从两个网站收集的，分别是《洋葱报》和《半岛晨报》。过去的讽刺检测研究大多是。
- en: done from like extracting data set from Twitter。 But they tend to be like noisy
    in terms of。 like the labels they have and the language they contain。 So， people
    use like lots of， abbreviations。 They are like spelling mistakes。 So， we collected
    this high quality data set。 from two news websites。 So， the onion contains like
    if you know about it like sarcastic versions。
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 从Twitter提取数据集的工作已完成，但在标签和语言方面往往比较嘈杂。因此，人们使用了很多缩写，拼写错误。因此，我们从两个新闻网站收集了高质量的数据集。《洋葱报》包含了如果你了解的话，是讽刺版的内容。
- en: of like the current events going on。 And the half post contains like real news
    reports。 So。 we can treat them as like non-circastic。 So， we kind of combine them
    and made this。 like one good data set。 And lastly like we also had like news category
    data set。 So， this。 was extracted just from half post。 So， these contains like
    real news articles and we have。
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 当前的事件《半岛晨报》包含真实的新闻报道，因此我们可以将其视为非讽刺的。因此，我们将它们结合在一起，制作成一个好的数据集。最后，我们还拥有新闻类别数据集，这些数据是从《半岛晨报》中提取的，包含真实的新闻文章。
- en: details like headline， preview text， tags， authors and date published。 So， essentially。
    if we have like news from different categories that could potentially help us
    in extracting。 like exciting。 Sorry。 And just kind of banging out。 Exciting。 Yeah。
    Exciting findings。 Sorry。 So。 yeah， I'll be talking about those like later on
    like how that is kind of relevant， in our case。 So。
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 细节如标题、预览文本、标签、作者和发布日期。因此，如果我们有来自不同类别的新闻，这可能有助于我们提取一些令人兴奋的发现。对不起，只是在不断地提到令人兴奋的结果。对不起。所以，我会稍后谈论这些在我们案例中的相关性。
- en: let's talk about searching for the data sources to build good quality， data
    sets。 So。 they are like basically two types of scenarios that we'll come across。
    So。 first would be that we have like a problem statement in mind that we want
    to solve using。 machine learning。 So， for that case like we'll be using a guided
    search approach。 And second。
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们谈谈寻找数据源以构建高质量数据集的过程。因此，基本上有两种情境我们会遇到。第一种是我们心中有一个问题陈述，想用机器学习解决。在这种情况下，我们将采用有指导的搜索方法。第二种。
- en: case is when let's say we don't have any specific problem in mind， but we would
    like。 to like contribute something like word file to the community。 So， in that
    way like we'll。 be using like unguided search approach。 So， I'll be talking about
    like the guided search。 approach first。 So， this is where like we have like specific
    problem in mind。 And so， the。
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们没有任何具体问题在心中，但我们想为社区贡献一些东西，比如文档。在这种情况下，我们将采用无指导的搜索方法。因此，我会先谈论有指导的搜索方法。在这里，我们有一个具体的问题在心中。
- en: main challenge for this type of data search approach is that when we know what
    problem。 we have to solve， we have to kind of formally state it out and understand
    the problem thoroughly。 so that we can identify what all data we actually need
    to collect。 So， that like we can address。 the problem。 So， it could be daunting，
    but like we'll be sharing some pointers so that。
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 这种数据搜索方法的主要挑战在于，当我们知道需要解决什么问题时，我们必须正式表述并彻底理解问题，以便识别我们实际需要收集的数据。这样，我们才能解决这个问题。这可能会让人感到望而生畏，但我们将分享一些指引。
- en: it's like little bit easier。 So， first step for that would be defining the problem
    formally。 So。 what I mean by that is let's say we are trying to solve this size
    recommendation problem。 where we have like a customer and we have to recommend
    like a clothing product to them。 So。 that the size for that is fits kind of perfectly
    to the user。 So， defining the problem。
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这就像简单一点。因此，第一步就是正式定义问题。那么，我的意思是，假设我们在解决这个尺寸推荐问题，我们有一个客户，我们需要向他们推荐一款服装产品。这样，尺寸就能完美贴合用户。所以，定义这个问题。
- en: formally means like identifying all the aspects of it。 And what are like there
    are all the。 variables involved。 So， for size recommendation problem like we can
    say that given a user。 and a product with different catalog sizes， we have to
    recommend a product size that fits。 the user's best。 So， this essentially kind
    of lays out all the variables that are kind。
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 正式地说，这意味着识别所有方面，以及所有涉及的变量。因此，对于尺寸推荐问题，我们可以说，给定一个用户和一个具有不同目录尺寸的产品，我们必须推荐一个最适合用户的产品尺寸。因此，这本质上列出了所有相关的变量。
- en: of involved in solving this kind of problem。 So， after we have done that like
    we have to。 determine essential data signals that are involved in like solving
    this kind of problem。 So。 in this case like as you can see in the formal definition。
    So， we'd see that probably， user ID。 product ID， size of the product purchased
    and the fit feedback from the user。
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 参与解决这类问题的数据。因此，在我们完成这一点后，我们必须确定在解决这类问题中涉及的基本数据信号。在这种情况下，如你所见，正式定义中，我们可能会看到用户ID、产品ID、购买的产品尺寸以及用户的适配反馈。
- en: are kind of essential things that would help us in like addressing this kind
    of problem。 So， yeah。 So， basically these will then contain these will constitute
    all the essential signals。 that we need to like address this problem。 And we could
    have like other types of data。 as well like product category or price， but they
    could help in solving the problem in， a better way。
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 有一些基本要素将帮助我们解决这类问题。因此，嗯，基本上这些将包含所有我们需要解决这个问题的基本信号。我们也可以有其他类型的数据，比如产品类别或价格，但它们可以更好地帮助解决问题。
- en: but they are not like absolutely essential。 Then next is like data volume consideration。
    So。 whenever we are looking for the data source， we have to see that we are able
    to build like a sufficiently large data set。 So， for example， if we go to some
    websites that don't have like enough reviews on the product or let's。 say if you
    are looking at like news website， they don't have enough historical articles。
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 但它们并不是绝对必要的。接下来是数据量的考虑。因此，每当我们寻找数据源时，我们必须确保能够构建一个足够大的数据集。例如，如果我们访问某些网站，它们没有足够的产品评价，或者如果你查看的是新闻网站，它们没有足够的历史文章。
- en: that they have published。 So， those might not be like very good kind of data
    sources for。 us to construct our data sets。 And the reason is like if we don't
    have like a good enough。 data set where each let's say entity has sufficient amount
    of information available。 our machine learning models might not be able to like
    learn good parameters from such like。
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 他们已经发布了。因此，这些数据源可能不是构建我们数据集的非常好的选择。原因在于，如果我们没有足够好的数据集，比如每个实体都有足够的信息可用，我们的机器学习模型可能无法从中学习到良好的参数。
- en: underpowered data set。 So， yeah。 This was it for a guided search。 Some pointers
    that can。 help if you have like some problem in mind。 So， like you can follow
    maybe these pointers。 to see like how you can collect like a good quality data
    set。 So， now the next scenario。 is like unguided search。 This is let's say for
    like creative souls who so this could be。
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集性能不足。因此，这就是关于有指导搜索的内容。一些提示可以帮助你，如果你心中有某个问题。所以，你可以遵循这些提示，看看如何收集一个高质量的数据集。接下来的场景是无指导搜索。这对于创造性思维者来说，这可能是。
- en: like similar to let's say if you want to do research and you are looking for
    the problems， to solve。 So， this could be like drawn parallel to that。 So， if
    you want are looking to make。 like data set contributions and don't have like
    specific problem in mind like you can。 try to follow this approach and see if
    like it provides any help。 And we will be referring。
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于如果你想进行研究并寻找要解决的问题。所以，这可以与之类比。如果你希望进行数据集贡献，而没有具体的问题在心中，你可以尝试遵循这种方法，看看它是否提供任何帮助。我们将会提到。
- en: to the news category data set for this because that was like specifically collected
    without。 having any problems in mind and then kind of backtracking to see how
    it can be like useful。 So。 yeah。 So， the biggest challenge for unguided search
    is that it has like lots of uncertainty。 So。 we don't know like what problem we
    are going to solve。 So， there's no like proper。
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这是针对新闻类别数据集，因为该数据集是专门收集的，没有任何问题考虑，并且回溯以查看它如何能派上用场。所以，嗯，针对无指导搜索的最大挑战是它有很多不确定性。因此，我们不知道将要解决什么问题。所以，没有明确的方法。
- en: structure in the way like we could approach our search operation。 Let's say
    when I say。 search operations test like Google and on the web。 And so， although
    it makes things a little。 difficult but the pointers that I will be just talking
    about will provide some structure， to that。 So， first like we have to see whether
    the data that we are trying to collect whether。
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 结构化我们的搜索操作的方式。比如说，当我提到搜索操作时，就像在谷歌和网页上那样。因此，虽然这让事情变得有点困难，但我将讨论的一些要点将为此提供一些结构。所以，首先我们必须看看我们尝试收集的数据是否能够解决某个实际问题。
- en: it can address some practical problem or not。 And whether it has the ability
    to lead us。 to some fascinating insights or not。 So， yeah。 To take an example
    like for the news category。 data set。 So， we can train a classifier on this data
    set and it can help us identify。 some writing style of the news articles whether
    some writing style belongs to like a political。
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 以及它是否有能力引导我们获得一些令人着迷的洞察力。是的。以新闻类别的数据集为例。我们可以在这个数据集上训练一个分类器，它可以帮助我们识别新闻文章的某种写作风格，看看某种写作风格是否属于政治类。
- en: category or humorist tone or it's based on like fashion trends or something
    like that。 And it can also help us to tag and track news articles and understand
    how basically the。 writing styles kind of differ and so on。 Yeah。 So， then the
    next thing after we have determined。 let's say that something is worth estimating。
    We have to see that we have sufficient metadata。
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 类别或幽默风格，或者它基于时尚趋势或其他类似的东西。这也可以帮助我们标记和跟踪新闻文章，理解写作风格是如何不同的。是的。所以，接下来在我们确定某些事情值得估计之后，我们必须确保我们有足够的元数据。
- en: available around that。 So， having enough metadata will help your machine learning
    models to。 draw associations between like different attributes and will help in
    ultimately like predicting。 the target。 For example， let's say the news category。
    So， another example would be like。 let's say if you are trying to predict like
    a product price from just like user ID product， ID。 So。
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，拥有足够的元数据将帮助你的机器学习模型在不同属性之间建立关联，并最终帮助预测目标。例如，假设是新闻类别。另一个例子是，假设你试图仅凭用户ID和产品ID来预测产品价格。
- en: that might not go very well and we need like sufficient data around the product。
    let's say brand of it and the fabric of it for us to able to like give like a
    meaningful。 predictions。 Now， data volume requirement applies here as well but
    this has like a relaxed。 constraint because we are not looking to solve like any
    specific problem we have in mind。 So。
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能不是很顺利，我们需要围绕产品的足够数据。比如说它的品牌和面料，以便我们能够提供有意义的预测。现在，数据量的要求在这里也适用，但这个约束是放宽的，因为我们并不是想解决某个特定的问题。
- en: we can always like adjust the problem definition according to what data we have。
    So， for example。 in new scattering data set like we had many instances where let's
    say there was some news。 category which had less than 100 articles。 So， if you
    have some underpowered category。 our machine learning model might not be able
    to learn well。 So， but that then stops from。
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们总是可以根据我们拥有的数据调整问题定义。例如，在新的散布数据集中，我们有很多实例，比如说有一些新闻类别的文章少于100篇。因此，如果你有一些数据不足的类别，我们的机器学习模型可能无法很好地学习。但这会阻止我们可用的数据。
- en: making it let's say a high quality data set because our purpose was not specifically
    to。 predict some category just that like we have like different categories and
    if those have。 like sufficient data then only like we will be able to train good
    models on it and make。 the model useful。 Sorry， yeah。 So， based on the data availability
    we can like always adjust。
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们假设这是一个高质量的数据集，因为我们的目的是并不是特别去预测某个类别，只是我们有不同的类别，如果这些类别有足够的数据，只有这样我们才能够训练出好的模型，并使模型变得有用。抱歉，是的。所以，根据数据的可用性，我们总是可以进行调整。
- en: the problem statement and finally like this is just like it's like a critical
    point like。 we should always because there is some uncertainty involved and we
    don't know what all is out。 there。 So， if we have some ideas always try to check
    if something of that sort is already。 not on the web。 So， we don't want to like
    duplicate the efforts and if something is already available。
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 问题陈述，最后，这是一个关键点。我们应该总是考虑，因为有一定的不确定性，我们不知道外面有什么。因此，如果我们有一些想法，总是要检查一下是否已经在网上有类似的内容。我们不想重复努力，如果某些东西已经可用。
- en: we just want to see let's say what are the shortcomings of those and try to
    kind of improve。 on that。 Now， yeah。 So， that was some like pointers on like how
    to structure a search。 operation and now after let's say we have collected the
    data like we have found out the。 data sources extracted the data out of it that
    is just the beginning。 So， that at that。
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只想看看那些方面的缺陷，并试图进行改进。现在，是的。这些是关于如何构建搜索操作的一些指针，现在在我们收集数据之后，比如说我们找到了数据源并提取了数据，这只是开始。
- en: time we have like a raw data available to us but still we have to like kind
    of groom it。 for it to be like very useful to our machine learning models。 So，
    we will be talking about。 some of the techniques that can be used to kind of groom
    your data set。 So， first technique。 is data trimming。 So， data trimming。 So， I
    will take an example from the clothing， with data set。
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们总是有原始数据可用，但我们仍然需要对其进行修整，使其对我们的机器学习模型非常有用。因此，我们将讨论一些可以用来修整数据集的技术。第一个技术是数据修剪。以服装合适度数据集为例。
- en: So， there could be like some reviews。 So， it's not always the case that。 all
    the essential signals are available in each transaction like the data could be
    very。 noisy and if we don't have like a transaction with fit feedback available
    or size purchase。 those are not really useful。 So， and then our job becomes to
    kind of let's say sanitize。
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，可能会有一些审查。因此，并不是每个交易中都能获得所有必要的信号，数据可能会非常嘈杂，如果我们没有像适合反馈或规模购买这样的交易，那就不是很有用。因此，我们的工作变成了清理。
- en: our data set try to trim all the records which won't be ultimately helpful and
    predicting。 what we want to predict basically let's say recommend whether a particular
    product fits。 a customer or not。 So， we have to kind of remove those and one thing
    to keep in mind is。 after we have done data trimming like we still have to ensure
    that the volume is kind of。
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的数据集尝试去除所有最终不会有助于预测的记录。基本上，我们想预测的就是推荐某个产品是否适合客户。所以，我们必须去除那些，需记住的是，在数据修剪后，我们仍然需要确保数量是适当的。
- en: good enough let's say we are not like drastically reducing the data volume。
    So， yeah for the。 next few set of data preparation techniques the guess we talking
    more about that。 So。 we've already talked about how we can collect a data set
    and build a data set from。 different sources and in an ideal scenario we would
    assume that we are using a single。
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 足够好，假设我们没有大幅度减少数据量。所以，是的，在接下来的几组数据准备技术中，我们会讨论更多。我们已经谈到了如何从不同来源收集数据集并构建数据集，在理想情况下，我们假设我们使用的是单一的。
- en: web source to create that data set。 For example， for the clothing fit data set
    we use mod cloth。 but there would be many circumstances where we would have to
    gather the different attributes。 from different kind of sources to be it for more
    inclusive meaningful and available data， set。 So。 coalescing these data from separate
    sets separate sources into one unified view。
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 网络来源以创建那个数据集。例如，对于服装合适度数据集，我们使用了mod cloth。但是，很多情况下，我们需要从不同的来源收集不同的属性，以便更具包容性、意义和可用性的数据集。所以，将来自不同来源的数据合并为一个统一的视图。
- en: is what is called data integration。 For example， let's talk about vertical integration
    remember。 the data set that we had for sarcasm detection there was data collected
    from the onion which。 was non-circastic and then we had data collected from half
    posts which was non-circastic。 So。 in that case the attributes collected from
    both these sources were exactly the same and。
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是所谓的数据集成。例如，让我们谈谈垂直整合。记得我们用于讽刺检测的数据集吗？有来自洋葱的数据，这些数据是非讽刺性的，然后我们从半个帖子收集的数据也是非讽刺性的。所以，在这种情况下，从这两个来源收集的属性是完全相同的。
- en: all we needed was to integrate them in a vertical sense because the attributes
    were the same。 So。 we created a final data set with the label say one sarcastic
    and non-circastic by combining。 the source data from these two sources。 So， this
    is called vertical because the attributes。 that belong for example the headline
    the author and is it sarcastic or not sarcastic were kind。
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们所需要的就是在垂直方向上整合它们，因为属性是相同的。因此，我们通过结合这两个来源的数据创建了最终的数据集，标记为一个讽刺和非讽刺。这被称为垂直整合，因为属性，比如标题、作者和是否讽刺等都是相同的。
- en: of same across both the sources。 The other could be horizontal integration which
    is when for。 example for to create one single record of the data set you have
    to combine sources。 For example。 let's just assume we are building a data set
    for different cities and their attributes。 So。 we would assume like feels like
    state， country， area of population come from one。
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 横向整合是另一种关键技术，例如，为了创建一个数据集的单一记录，你必须合并多个来源。例如，假设我们正在建立一个关于不同城市及其属性的数据集。因此，我们可以假设州、国家、人口区域等来自于一个来源。
- en: kind of source and the other like administrative， religion， language festivals
    come from the， other。 So， in that case you would be identifying one city ID and
    integrating them horizontally。 In such case this is how you would approach the
    data integration and it is very important。 to try and make your data set as exhaustive
    as possible to have as many records and attributes。
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 各种来源，例如行政、宗教、语言节日等，可能来自其他方面。因此，在这种情况下，你会识别一个城市ID并进行横向整合。在这种情况下，这就是你进行数据整合的方法，尽可能全面地构建数据集，以拥有尽可能多的记录和属性是非常重要的。
- en: Further down the line when you do feature engineering in all this trim that
    but it's。 always necessary to have like a good quality robust data set。 And for
    the other thing you。 also need to do data transformation to make it more robust
    as I said because there is so。 much content available on the web these days that
    it's impossible to check the correctness， of all。
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 进一步来说，在进行特征工程时，虽然会有所删减，但始终有必要保持良好的质量和稳健的数据集。另一个方面，你还需要进行数据转换，使其更加稳健，因为现在网上有大量内容，检查所有内容的正确性几乎是不可能的。
- en: For example， in the reviews or the content posted by the publisher like the
    product， name。 the product sizing they have to be moderated to some extent however
    people might use small， large。 medium， as sizes or someone would use like 12，
    13， 14。 So， it totally depends。 Some people use different just two letters for
    identifying the state， someone writes completely。
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在发布者发布的评论或内容中，像产品名称、产品尺码必须在一定程度上进行审核。然而，人们可能会使用小号、大号、中号作为尺码，或者使用12、13、14这样的数字。因此，这完全取决于情况。有些人用两个字母来识别状态，有人则完全写出全称。
- en: the state name or someone would just use like month， date， year or someone。
    So， depending。 for the date formats。 So， it's very important to sanitize the data
    to transform it so that。 it's all in one form for it to be a robust data set。
    And so to say the data transformations。 primary focus is to scrub the data sets
    raw values to remove redundancies and correct。
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 状态名称或有人可能只使用月份、日期、年份等。因此，具体情况而定，日期格式很重要。因此，清洗数据以转化为统一形式，对于构建稳健的数据集至关重要。因此，数据转换的主要重点是清洗数据集的原始值，以去除冗余并纠正错误。
- en: the language and the logical semantics that you have。 So， the some like some
    techniques。 for transforming your data set could be handling special characters，
    handling grammar， spellings。 error， encoding the sizes in numerical order， changing
    the format of date， etc。 So， these。 are just a few cues on how you can scrub your
    data set。 So， as I mentioned that once you。
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 语言和逻辑语义也很重要。因此，一些转换数据集的技术可能包括处理特殊字符、语法、拼写错误，按数字顺序编码尺码，改变日期格式等。因此，这些只是一些提示，告诉你如何清洗数据集。正如我提到的，一旦你完成。
- en: have your big data set， you know， with all the records and with all the attributes，
    it's。 necessary to proceed to feature engineering before you input this data set
    as is to a。 machine learning model。 So， if you consider data to be the crude oil，
    features are the。 fuel that we need。 So， feature engineering is nothing but applied
    machine learning。 It。
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有一个庞大的数据集，包含所有记录和属性，在输入这个数据集到机器学习模型之前，进行特征工程是必要的。因此，如果你把数据视为原油，特征就是我们需要的燃料。因此，特征工程就是应用机器学习的过程。
- en: is also one of the key techniques and the processes that does not get it the
    due limelight。 in the machine learning workflow。 Much of the success of machine
    learning algorithms is。 attributed to the success in engineering features from
    the data so that the learner can understand。 This is where your intuition comes
    into play and you know， black art and creativity and。
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这也是机器学习工作流程中未受到应有重视的关键技术和过程。机器学习算法的成功在于从数据中工程化特征，使学习者能够理解。这就是直觉发挥作用的地方，包括黑暗艺术和创造力。
- en: domain knowledge。 So， this is the step that should be considered much more important
    than。 the model itself or hyper parameter tuning and whatever shenanigans people
    do and because。 machine learning algorithm as you said is as powerful as the data
    you supply。 So， you。 have to take care of what your intuition says domain knowledge
    trial and error。 So， that。
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 这一环节应该被视为比模型本身或超参数调优更为重要，因为如你所说，机器学习算法的强大取决于你提供的数据。因此，你必须关注你的直觉、领域知识以及试错过程。
- en: is what feature engineering is all about and choosing the right features in
    right format。 can buy for boost the performance using even using simpler machine
    learning models。 So。 it increases the transparency you can better explain the
    model if you have fewer features。 but they are much more powerful。 It reduces
    the need for heavy ensemble learning techniques。
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 特征工程的核心就是选择正确格式的合适特征，甚至可以通过使用更简单的机器学习模型来提升性能。因此，它提高了透明度，如果特征更少，你可以更好地解释模型，但它们却更强大。这减少了对复杂集成学习技术的需求。
- en: and also further reduces the need for hyper parameter tuning and model optimization。
    So。 the bottom line for this entire talk would be that typical machine learning
    workflow looks。 like this。 You go from defining a problem statement to developing
    an intuition of how。 machine learning can tackle your problem， get the data， process
    the data and defining the。
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 还进一步减少了对超参数调优和模型优化的需求。所以，这整个讨论的底线是典型的机器学习工作流程看起来像这样。你从定义问题陈述开始，发展出机器学习如何解决你的问题的直觉，获取数据，处理数据并定义。
- en: appropriate success metrics choosing the model that you feel would fit your
    problem the best。 deploying the model and then monitor it。 The problem comes when
    you focus more on defining。 the success metrics and choosing appropriate model
    and just get stuck in this loop。 However。 the focus should be on how you source
    your data， how you create your data set and how。
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 适当的成功指标，选择你认为最适合你问题的模型，部署模型，然后进行监控。当你过于专注于定义成功指标和选择合适模型时，就容易陷入这个循环。然而，重点应该放在你如何获取数据、如何创建数据集上。
- en: you do feature engineering because these form the part of applied machine learning
    and can。 boost your machine learning algorithm by 100x。 And that is what our experience
    has taught， us。 Even Andrew Angie who is a machine learning expert has started
    a campaign for data centric。 AI and he says that machine learning model is nothing
    but code like AI is equals to code。
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 你进行特征工程是因为这属于应用机器学习的一部分，并且可以将你的机器学习算法提升**100倍**。这正是我们的经验所教导我们的。甚至机器学习专家**Andrew
    Ng**也发起了以数据为中心的AI运动，他表示机器学习模型不过是代码，而AI等同于代码。
- en: plus data and for so many years we focused on code which is like how to create
    a more complex。 architecture， how to include more layers by keeping the data stagnant
    but now is the time。 that we have state of the art algorithms and the focus should
    be on data。 How can we enrich。 our data？ How can we make more creative features
    and how we can have more signals into our data。
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 多年来，我们专注于代码，比如如何创建更复杂的架构，如何增加更多层而让数据保持不变，但现在是时候关注数据了。我们如何丰富数据？我们如何创造更多的特征，以及如何为我们的数据注入更多信号？
- en: to basically boost the performance of a machine learning algorithm。 So that
    being on the same。
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 以基本提升机器学习算法的性能为目标。因此，掌握同一领域的知识是至关重要的。
- en: '![](img/56ec2fbcfc37569d605d60b3f8a7b8ec_4.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](img/56ec2fbcfc37569d605d60b3f8a7b8ec_4.png)'
- en: lines with a co-author book called cutting data for machine learning and this
    is a QR code。 if anyone wants to check it out on Amazon it's also available on
    Kindle and it talks about how。 to identify signals from web sources to creating
    your own data set using Python， Selenium and all。 the other open source tools。
    And it also does synopsis of data pre-processing and feature engineering。
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 与共同作者合著的书《机器学习数据处理》有相关内容，这里有一个二维码，如果有人想在亚马逊查看，也可以在Kindle上找到，书中讲述了如何从网络来源识别信号，以及使用Python、Selenium等开源工具创建自己的数据集，同时还概述了数据预处理和特征工程。
- en: introducing you to machine learning algorithms from a data perspective and we
    are glad that is。 endorsed by leading ML experts in the industry and you can read
    forwards by Lawrence Morny。 Julian Mcauley and Mankting Warren and that's it。
    Thank you so much。 If you have any questions。 please feel free to reach us out。
    Thank you。
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 从数据的角度向你介绍机器学习算法，我们很高兴这一点得到了业界领先的机器学习专家的认可，你可以阅读由**劳伦斯·莫尼**、**朱利安·麦考利**和**曼克廷·沃伦**撰写的前言。就是这样。非常感谢。如果你有任何问题，请随时与我们联系。谢谢。
- en: '![](img/56ec2fbcfc37569d605d60b3f8a7b8ec_6.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](img/56ec2fbcfc37569d605d60b3f8a7b8ec_6.png)'
