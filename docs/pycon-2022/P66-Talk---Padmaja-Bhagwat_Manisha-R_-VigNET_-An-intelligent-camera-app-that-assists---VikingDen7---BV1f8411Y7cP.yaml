- en: P66：Talk - Padmaja Bhagwat_Manisha R_ VigNET_ An intelligent camera app that
    assists - VikingDen7 - BV1f8411Y7cP
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Good evening everyone。 We have a next talk that is on Visionet， an intelligent
    camera。
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e5e281b99ab1922feb99f35c9a0cb653_1.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
- en: app that assists you in understanding your surroundings by Padma Jha Bhagavat
    and Manish， Shah。 Over to you Padma Jha。 Thank you so much。 Helen Keller once
    said that there is no better way to thank God for your sight than by giving。 help
    in hand to someone in the dark。 A very pleasant evening to everyone。 I am Padma。
    Jha Bhagavat and I am super excited to present Project Minute to you all。
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: Before we get into the talk here is a small introduction about us。 I am Padma
    Jha Bhagavat。 and this is my dear friend Manish Shah。 We both work as data scientist
    at Glance India。 I work on monetizing the content for Glance。 Manisha here works
    on personalizing the Glance。 user experience which by the way has 150 million
    daily active users。
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: So originally this talk was supposed to be given by both Manisha and I but unfortunately。
    she couldn't join us here today。 So you see I now have double the responsibility
    to make。 this talk as interesting and fun as possible and it tells me it's not
    an easy job to fit。 into her shoes。 But I will try my best。 So let's get into
    the main part of the talk。
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: First things first what are we going to learn today？ We are going to see how
    to build an。 end to end deep learning based application particularly we will be
    looking into a visual。 question answering based application and this is how the
    outline of today's talk is going。 to look like。 First we will look into the data
    and model that enabled this application。
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: then we will look into how to quickly prototype such deep learning based application
    and the。 final part would cover how can we productionize these application by
    building services on top。 of your model particularly we will be looking into front
    and service and an API service。 With that said let's dive into the talk。 Some
    of you might be aware that more than 253 million。
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: of words population suffers from visual impairment which is roughly 1 in 30。
    For them simple every。 day task like locating a matching pair of saw or telling
    what dollar bill it is can become。 extremely difficult。 Hence we decided to build
    a visual question answering based application。 which can truly transform your
    visual word into an audible experience。 Now you might be。
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: wondering what visual question answering is。 Simply put it's an intelligent
    camera app。 which takes image and a question related to that image and gives an
    answer out。 For instance。 let's say you are on to your everyday evening walk where
    you come across this really cute。 puppy you take its picture and ask a question
    like what is the puppy eating。 Now both these。
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: inputs then go into our VQA model which gives out an answer saying it's eating
    stick or。 rather playing with stick。 So that's the goal of today's application
    that we are going to。 build the model which can take both image and a question
    and give out an answer in the form。 of audio。 So with that goal in mind let's
    see how our data set looks like。 We used VQA。
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 输入将进入我们的 VQA 模型，它会给出一个答案，说明它在吃棍子或实际上是在玩棍子。因此，今天应用的目标是构建一个模型，它可以同时接受图像和问题，并以音频的形式给出答案。带着这个目标，让我们看看我们的数据集是什么样的。我们使用了
    VQA。
- en: data set in order to train our model。 This is openly available on visualqa。org。
    So what。 this data set contains is it has 120，000 plus images and 600，000 plus
    questions。 Each image。 has at least three questions associated with it and every
    question has 10 ground to answer。 Of course there can just be one correct answer
    depending on the context in which the question。
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 用于训练我们模型的数据集。这在 visualqa.org 上公开可用。那么，这个数据集包含什么呢？它有超过 120,000 张图像和超过 600,000
    个问题。每张图像至少有三个相关的问题，每个问题都有十个正确答案。当然，根据问题的上下文，可能只有一个正确答案。
- en: is asked。 You can see on the right that the data set contains variety of images
    including。 the image of a woman whose moustache is made up of bananas。 So it does
    have very interesting。 question like what is the moustache made up of or what
    is the color of her eyes。 You see。 these are not extremely straightforward question。
    We want our model to understand and be able。
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 被询问。你可以在右侧看到数据集中包含各种图像，包括一位女士的图像，她的胡子是由香蕉组成的。因此，确实有非常有趣的问题，比如她的胡子是由什么组成的，或者她的眼睛是什么颜色的。你会发现，这些问题并不是极其简单的。我们希望我们的模型能够理解并能够。
- en: to answer in such unusual and ambiguous scenarios as well。 And one more thing
    to note over here。 is that a data set contains both image as well as question
    in the form of text。 Hence。 we are trying to look for a model which can understand
    both these modalities which can。 understand both vision and language。 And yes
    that's the mighty vision and his famous。
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 也能在这样不寻常和模糊的场景中进行回答。还有一件事需要注意的是，数据集包含图像和文本形式的问题。因此，我们正在寻找一个能够理解这两种模态的模型，能够理解视觉和语言。是的，这就是强大的视觉及其著名的。
- en: dialogue for all the Marvel fans out there。 On that note I think we are good
    to look into。 the model which can understand both vision and language。 I present
    to you the vision language。 transformer also known as VIT。 Well I understand that
    this entire architecture might look quite。 intimidating to look at in the beginning
    but do not worry I will be breaking it down into。
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这是给所有漫威粉丝的对话。在这个意义上，我觉得我们可以开始研究能够理解视觉和语言的模型。我向你们介绍视觉语言变换器，也称为 VIT。虽然我知道这个整个架构一开始看起来可能相当令人畏惧，但请不要担心，我会将其逐步拆解。
- en: smaller components and we will go over it in detail。 So what you need to know
    over here。 is that our model is taking both text which is our question as well
    as image as an input。 Now first let's see how the model is processing the input
    text。 So the concept is similar to。 that of BERT and some of you might already
    know that BERT stands for bidirectional encoder。
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 更小的组件，我们将详细讨论。因此，你需要知道的是，我们的模型同时接受文本（即我们的问题）和图像作为输入。现在，让我们先看看模型是如何处理输入文本的。这个概念类似于
    BERT，而你们中的一些人可能已经知道 BERT 代表双向编码器。
- en: representation from transformers。 Well even if you do not know that's okay because
    we'll。 see how BERT operates。 Let's take the very same question that we saw earlier
    what is。 the puppy eating。 So what BERT does is it breaks this input sentence
    into smaller pieces。 which we call as tokens。 So even when you are teaching a
    child you break down complex。
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 变换器表示。即使你不认识也没关系，因为我们将看到 BERT 是如何运作的。让我们以之前看到的同样的问题为例：小狗在吃什么。因此，BERT 的作用是将这个输入句子拆分成更小的部分，我们称之为标记。因此，即使在教孩子时，你也会将复杂的。
- en: sentence into smaller ones in order for it to learn right。 Similarly you break
    down the。 complex sentence into smaller tokens。 So once you do that the next step
    you will be masking。 a few words in the input sentence and then you will be adding
    some special tokens to it。 Here you can see that we have added CLS which stands
    for classification and SEP which stands。
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 将句子拆分成更小的部分，以便它能正确学习。同样，你可以将复杂的句子拆分成更小的标记。因此，一旦你这样做，下一步就是在输入句子中掩盖一些单词，然后添加一些特殊标记。在这里，你可以看到我们添加了
    CLS，代表分类，以及 SEP，代表分隔。
- en: for separator indicating that it's the end of this sentence。 So now that we
    have broken。 down our input sentence into smaller tokens we'll then have to convert
    it into vectors。 Why do we have to do that？ It's because ultimately we are teaching
    a machine and they do not。 understand anything other than numbers。 So once we
    have vectors out of your tokens you。
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 用于指示这是该句子的结束的分隔符。所以现在我们已经将输入句子拆分成更小的标记，我们将需要将其转换为向量。为什么我们必须这样做？因为最终我们是在教机器，而机器除了数字之外不理解任何东西。所以一旦我们从你的标记中得到了向量。
- en: need to add something called as positional encoding。 This positional encoding
    gives the。 information about the exact position where the token lies in your input
    sentence。 It's。 an extremely crucial information which your bird requires。 So
    now that we have input which。 captures both content as well as positional information
    we then feed it to the transformer。
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 需要添加一种称为位置编码的东西。这种位置编码提供了关于标记在输入句子中确切位置的信息。这是鸟类所需的极其重要的信息。所以现在我们有了既捕捉内容又捕捉位置信息的输入，我们就将其输入到变换器中。
- en: encoder and this is then trained to identify what the mast world is。 In our
    case the mast。 world is puppy。 Let's see how it is done exactly。 So this is how
    the transformer encoder。 architecture looks like。 It is taking the input text
    and the positional encoding and this。 is passed through series of self-attention
    layers which tries to extract as much information。
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器，然后训练它识别“mast world”是什么。在我们的例子中，“mast world”是小狗。让我们看看它是如何做到的。因此，变换器编码器架构是这样的。它接收输入文本和位置编码，并通过一系列自注意力层传递，这些层试图提取尽可能多的信息。
- en: as possible from your input data。 So with that it is then sent to the feed forward
    layer。 which learns to classify what the mast world is。 So you see the model is
    trying to derive。 context from the neighboring words。 So we know that this kind
    of bird based model is extremely。 fast and computationally efficient in terms
    of language models。 So why can't we use the。
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 尽可能地从你的输入数据中提取信息。所以，接下来它被发送到前馈层。前馈层学习如何分类什么是“mast world”。所以你可以看到模型试图从邻近的词汇中推导出上下文。因此，我们知道这种基于鸟类的模型在语言模型方面是极其快速和计算高效的。那么我们为什么不能使用呢？
- en: very same concept for images？ So yeah， control C， control V。 One thing that
    we are great at， doing。 So yeah let's use the very same concept of transformer
    for our images and that's how。 we get vision transformers。 So what it does is
    it takes your input image， breaks it down。 into smaller pieces which we call it
    as patches。 These patches then get converted into vectors。
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图像的同样概念？所以是的，控制C，控制V。我们擅长的事情之一。所以是的，让我们对图像使用相同的变换器概念，这就是我们得到视觉变换器的方式。因此，它所做的就是接收你的输入图像，将其拆分成更小的部分，我们称之为补丁。这些补丁随后被转换为向量。
- en: and we add positional encoding to it similar to that of earlier。 We send it
    to the transformer。 encoder which is then trained to identify what the input image
    is。 You can see that the。 transformer encoder looks exactly same as that of before
    except that instead of taking。 the text embedding it is now taking the patch embedding
    which is the vectorized form of your。
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为其添加位置编码，类似于之前的做法。我们将其发送到变换器编码器，然后训练它识别输入图像是什么。你可以看到变换器编码器看起来和之前完全相同，只是它不再接受文本嵌入，而是接受补丁嵌入，这就是你的图像的向量化形式。
- en: patches combined with the positional encoding。 So this idea was introduced in
    the paper called。 an images worth 16 x 16 words by Google。 I highly recommend
    you to go over that paper。 because this concept of vision transformer is now revolutionizing
    the field of computer， vision。 So we saw how best we can process our input text。
    We also saw how best we can process。
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 补丁与位置编码相结合。所以这个想法是在谷歌的论文《一张图像值16 x 16个单词》中提出的。我强烈建议你阅读那篇论文，因为视觉变换器的概念正在彻底改变计算机视觉领域。因此，我们看到了如何最佳处理我们的输入文本。我们也看到了如何最佳处理。
- en: our input image。 Now let's bring it all together and that's how we get VILT
    basically。 So we。 took the input question broke it down into smaller pieces or
    tokens。 We get the word。 embedding out of it。 We take the supporting image， break
    it down into smaller patches。 get the patch embedding out of it。 Now both this
    word embedding and patch embedding is。
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的输入图像。现在让我们将这一切整合在一起，这就是我们基本上得到的 VILT。因此，我们将输入的问题拆分成更小的部分或标记。我们从中获取单词嵌入。我们取支持的图像，将其拆分成更小的补丁。从中获取补丁嵌入。现在这两种单词嵌入和补丁嵌入是。
- en: sent to the vision the transformer encoder which is then trained to identify
    what the， mast word is。 Here you can see that the model is making use of both
    text as well as image。 It is trying to context using both this information and
    is trying to identify what the mast word。 is。 Hence our transformer encoder is
    kind of acting like a modality interaction layer。
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 被发送到视觉变换器编码器，随后被训练以识别“掩码”词。在这里你可以看到模型同时利用了文本和图像。它试图使用这两种信息的上下文，识别出“掩码”词是什么。因此，我们的变换器编码器有点像一种模态交互层。
- en: Modality it is combining both the modalities vision and the language。 So now
    our entire。 goal is not to predict the mast word。 What we want to do is we want
    a model which can。 answer the question given an image。 So you take this pre-trained
    VILT model and you fine。 tune it further using the V2 data set that we saw earlier。
    So when you fine tune a model。
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 模态，它结合了视觉和语言两种模态。因此，我们的整体目标不是预测“掩码”词。我们想要的是一个能够根据图像回答问题的模型。因此，你使用这个预训练的 VILT
    模型，并使用我们之前看到的 V2 数据集对其进行进一步微调。当你微调模型时。
- en: which means you use the same set of weights and you fine tune it further using
    the new， data set。 So when you fine tune it further you get a model which can
    answer the question， given an image。 And that is how we build the core part of
    our application。 If it still。 sounds complicated then I do have a good news for
    you。 Python provides all these amazing。
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着你使用相同的权重集，并且使用新的数据集进一步微调它。因此，当你进一步微调时，你会得到一个能够根据图像回答问题的模型。这就是我们构建应用程序核心部分的方法。如果这听起来还是复杂，那么我有个好消息告诉你。Python
    提供了所有这些惊人的功能。
- en: libraries out of the box for you。 So if you are on to the latest version of
    PyTorch all。 you need to do is from transformers you need to import VILT processor
    and VILT for question。 answering。 The best part is VILT for question answering
    is already fine tuned on to your。 VQA V2 data set so you don't have to do anything
    extra。 You just have to plug it into your。
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 现成的库可以为你提供支持。因此，如果你使用的是最新版本的 PyTorch，你所需做的就是从 transformers 中导入 VILT 处理器和 VILT
    用于问答的功能。最棒的是，VILT 的问答功能已经在你的 VQA V2 数据集上进行了微调，所以你无需做任何额外的工作。你只需将其集成到你的。
- en: code and it is good to be used。 Let's see how it's done。 So first you need to
    initialize。 your processor and then you need to initialize your model。 You send
    in your raw input image。 and question to the processor which converts it into
    vectors also known as encoding。 This。 encoding we then send it through our model
    which gives out the probability of answer。 Easy isn't it？
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 代码已经准备好可以使用。让我们看看如何实现。首先你需要初始化处理器，然后需要初始化模型。你将原始输入图像和问题发送给处理器，处理器将其转换为向量，也称为编码。然后我们将这个编码传递给模型，模型会输出答案的概率。这不是很简单吗？
- en: That's all there is。 I mean with just these five lines of code your core。 part
    of the application is ready which can be plugged into your application。 So one
    thing。 that you need to ensure that is you are on the latest version of the torch
    and transformers。 in order to be able to import these libraries。 So now the crux
    of our application is ready。
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样。我是说，凭借这五行代码，你的应用程序核心部分就准备好了，可以集成到你的应用程序中。所以你需要确保的一件事是，你使用的是最新版本的 torch
    和 transformers，以便能够导入这些库。现在我们应用程序的核心部分已经准备好了。
- en: So the next part is how do we quickly prototype it but before that the advantage
    of using。 such a transformer based model is that it's computationally fast and
    efficient and it。 has high expressive power。 You will soon get to know it when
    I'll be showing the demo but。 yeah the core part of the application is built using
    the transformer based model。 So the。
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 所以下一部分是我们如何快速原型开发，但在此之前，使用这种基于变换器的模型的优势在于它的计算速度快且高效，并且具有很强的表达能力。当我给你展示演示时，你很快就会知道这一点。但，是的，应用程序的核心部分是使用基于变换器的模型构建的。因此。
- en: next part is how can we quickly prototype it？ How can we quickly build a web
    application。 out of your model？ So you might be wondering why is prototyping so
    necessary？ Why do we。 have to build a web application？ Trust me it's the most
    interesting part of building。 the entire application。 So the reason for building
    a web application is that we as a。
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 下一部分是我们如何快速原型开发？我们如何能快速构建一个基于你模型的网络应用程序？你可能在想，为什么原型开发如此必要？为什么我们必须构建一个网络应用程序？相信我，这就是构建整个应用程序过程中最有趣的部分。因此，构建网络应用程序的原因在于，我们作为一个。
- en: data scientist are shielded from a lot of real world requirements and in order
    to be。 able to better communicate with your stakeholders having a web application
    helps a lot instead。 of showing the code files。 So you might again question me
    saying that building a web application。 is not an easy task and if you're someone
    like me for whom dealing with HTML and CSS。
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学家常常被隔绝于许多现实世界的需求，为了能够更好地与利益相关者沟通，拥有一个Web应用程序非常有帮助，而不是仅仅展示代码文件。因此，你可能会问我，构建Web应用程序并不是一件容易的事，如果你像我一样，不擅长处理HTML和CSS。
- en: code is like a nightmare then Python has a solution for that as well。 It provides
    an。 extremely elegant library code of streamlet which I absolutely love it which
    helps converting。 your data script into shareable web application in minutes。
    I mean literally in minutes。 Let's。 see how it's done。 So let's take a step back
    and first see what is it that we need out。
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 代码就像一场噩梦，但Python也有解决方案。它提供了一个极其优雅的库`streamlet`，我非常喜欢这个库，它可以帮助你在几分钟内将数据脚本转换为可分享的Web应用。我是说，字面上只需几分钟。让我们看看如何实现。
- en: of our web application。 How do you want your UI to look like？ So for visual
    question answering。 based application we need application which takes input images
    so that all you need to。 do is input streamlet as ST and all you need to do is
    type ST dot streamlet and mention。 the kind of file you are you want your application
    to accept。 After that you need a prompt which。
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的Web应用程序。你希望你的UI看起来怎样？所以对于基于视觉问答的应用程序，我们需要一个可以接收输入图像的应用程序，因此你只需输入`streamlet`作为`ST`，然后输入`ST.streamlet`并说明你希望应用程序接受的文件类型。之后，你需要一个提示。
- en: can take your question in right。 So that can just be done using ST dot text
    input and there， you go。 You also need a button you know which can submit both
    your input image and question。 and for that it's just ST dot button and finally
    a prompt to display the answer that。 your model has given and that can be done
    using ST dot right。 So seriously with just。
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 可以正确处理你的问题。这可以通过`ST.text_input`来完成，然后你就可以了。你还需要一个按钮，知道可以提交你的输入图像和问题。为此只需使用`ST.button`，最后一个提示用来显示你的模型给出的答案，这可以通过`ST.write`完成。所以，实际上只需。
- en: these five lines of code you have an entire web application up and up。 I'll
    also show。 you how easy it is to run such kind of streamlet based application。
    So let me go into the。
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这五行代码就能让你拥有一个完整的Web应用程序。我还会向你展示运行这种基于`streamlet`的应用程序有多简单。所以让我进入。
- en: '![](img/e5e281b99ab1922feb99f35c9a0cb653_3.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e5e281b99ab1922feb99f35c9a0cb653_3.png)'
- en: '![](img/e5e281b99ab1922feb99f35c9a0cb653_4.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e5e281b99ab1922feb99f35c9a0cb653_4.png)'
- en: code。 Let me stop this。 So all you need to do is type ST and provide the file
    path of。
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 代码。让我停止这个。所以你只需输入`ST`并提供文件路径。
- en: '![](img/e5e281b99ab1922feb99f35c9a0cb653_6.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e5e281b99ab1922feb99f35c9a0cb653_6.png)'
- en: the file where your code is running。 In our case it's running on to streamletservice。py。
    and you hit enter。 You get an amazing application up and running onto your local
    system and this。
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 你的代码运行的文件。在我们的例子中，它运行在`streamletservice.py`上。你按下回车，便会在本地系统上启动一个惊人的应用程序。
- en: '![](img/e5e281b99ab1922feb99f35c9a0cb653_8.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e5e281b99ab1922feb99f35c9a0cb653_8.png)'
- en: is how your prototype looks like。 Now if Manishu was here she would have been
    like Padmajia。 what if you are showing us the static page does it actually work。
    Well let's see if it， does。 So let's take the very same image that we saw earlier
    and let's type in a question。 saying what is the puppy playing with and hit submit。
    Well it does take some time to。
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是你的原型外观。如果Manishu在这里，她可能会问：“Padmajia，如果你给我们看的是静态页面，它真的能工作吗？”好吧，让我们看看它是否能工作。让我们拿之前看到的那张图像，输入一个问题，问“小狗在玩什么？”然后点击提交。确实需要一些时间来。
- en: give an answer but hates Manfuproototyping right。 So yeah it did give an answer
    saying， it's tick。 I was so excited when I first built it so I showed it to Manisha
    who by the way。 is not at all an easy person to please。 So she was happy to look
    at it but she did point。 out a bunch of valid concerns。 She is like Padmajia what
    if I want to take this from。
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 它给出了答案，但讨厌快速原型，对吧？所以，是的，它确实给出了答案，说是“滴答”。我第一次构建它时非常兴奋，所以我向Manisha展示了它，顺便说一下，她可不是一个容易取悦的人。因此，她很高兴看到它，但确实指出了一些有效的顾虑。她问：“Padmajia，如果我想从。
- en: total typing stage to my actual customers。 I cannot show this because firstly
    it's not。 taking an audio input and audio output。 I mean I'm ultimately designing
    this for visually。 impaired right and the output the font of the output is not
    big enough and it does take。 a while to load the entire web application and give
    an answer。 You cannot afford to have。
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 完全输入阶段对于我的实际客户。我无法展示这个，因为首先它没有接收音频输入和音频输出。我的意思是，我最终是为视觉障碍人士设计的，对吧？而输出的字体不够大，而且加载整个Web应用程序并给出答案需要一些时间。你不能承受这样的延迟。
- en: that kind of delay。 So with all these limitations what she does is she goes
    and she builds an。 entire front end service by herself using React。 Now this React
    is a JavaScript library built。
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 那种延迟。因此，面对所有这些限制，她所做的就是自己构建一个完整的前端服务，使用的是React。这个React是一个构建的JavaScript库。
- en: '![](img/e5e281b99ab1922feb99f35c9a0cb653_10.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e5e281b99ab1922feb99f35c9a0cb653_10.png)'
- en: '![](img/e5e281b99ab1922feb99f35c9a0cb653_11.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e5e281b99ab1922feb99f35c9a0cb653_11.png)'
- en: and maintained by Facebook。 This React gives extremely rich user interfaces。
    I must tell。 you it's extremely appealing to look at。 I'll show you how it looks
    very soon。 So and also。 you can build the components which can be reused across
    your web application。 You need。 not write the same code again and again and also
    there is no delay at all。 It's extremely。
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 由Facebook维护。这个React提供了极其丰富的用户界面。我必须告诉你，它看起来非常吸引人。我很快会给你展示它的样子。因此，你还可以构建可以在你的Web应用程序中重复使用的组件。你不需要一遍又一遍地编写相同的代码，而且根本没有延迟。它是极其流畅的。
- en: fast when it comes to rendering pages。 I'll soon show you how the final UI looks
    like。 but before that I have a question for you。 Can you guess who this is？ I'll
    give you some， time。 Well are you interested to know what are model things of
    this image？ I mean at， least I am。 But you know what？ We'll have to wait till
    the end of this talk in order to。
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在渲染页面时速度很快。我很快会给你展示最终的用户界面长什么样，但在此之前我有个问题想问你。你能猜出这是谁吗？我给你一些时间。你想知道这个图像的模型是什么吗？我的意思是，至少我很想知道。但是你知道吗？我们得等到这个演讲结束才能揭晓。
- en: see what are model things of this image。 So what we covered so far is that we
    saw how。 to build the core part of our application which is our model using VILT。
    We also saw。 how to build the front end service using React。 Now these two components
    are separate in itself。 How do we combine these two？ So the bridge between these
    two would be our API service。
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 看看这个图像的模型是什么。因此，到目前为止，我们已经涵盖了如何构建我们的应用程序核心部分，也就是使用VILT构建我们的模型。我们还看到了如何使用React构建前端服务。这两个组件本身是独立的。我们如何将它们结合起来？这两者之间的桥梁就是我们的API服务。
- en: API service helps exposing the amazing work that you have done on model to the
    external， world。 And it also helps in integrating your work with other application。
    In our case we。 want to integrate with the front end service and having an API
    service is always good because。 deploying it to the production is extremely easy
    with that。 Again we built it using fast， API。
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: API服务帮助将你在模型上所做的惊人工作暴露给外部世界。它也有助于将你的工作与其他应用程序集成。在我们的案例中，我们想与前端服务集成，并且拥有API服务总是好的，因为将其部署到生产环境是极其简单的。我们同样是使用fast
    API构建的。
- en: Let's actually see how it works。 It's very simple。 You need to import fast API
    and。 kind of files that you will be dealing with。 After that you need to initialize
    the fast， API。 And this is the function。 This is the predict function which we
    want to expose to。 the external world。 We want the logic of this function to be
    exposed to our UI。 So that。
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们实际看看它是如何工作的。非常简单。你需要导入fast API和你将要处理的各种文件。之后，你需要初始化fast API。这是函数。这个预测函数是我们希望暴露给外部世界的。我们希望这个函数的逻辑能够暴露给我们的用户界面。
- en: can be easily done by just by adding this decorator like this。 All you need
    to do is。 add the rate app。post and provide the relative URL where you want the
    logic to be exposed， to。 And that's how it's done。 Now if you want to test out
    your API， if you want to make。 sure whether the logic is working as expected，
    then fast API provides this amazing swagger。
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以通过简单地添加这个装饰器来轻松完成。你需要做的就是添加`rate app.post`并提供你希望逻辑暴露的相对URL。就这样完成了。现在如果你想测试你的API，想确保逻辑是否如预期那样工作，那么fast
    API提供了这个惊人的swagger。
- en: documentation out of the box for you。 You don't have to do anything extra。 All
    you need。 to do is go to the URL where your application is running。 So in order
    to test it out it usually。 runs on localhost 9000 when you are trying it on to
    your local machine。 All you need。 to do is type in friend slash docs and it gives
    this amazing swagger UI for you。 So here what。
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 为你提供开箱即用的文档。你不需要做任何额外的事情。你只需访问运行你应用程序的URL。因此，在测试时，它通常在本地主机9000上运行，当你在本地机器上尝试时。你所需做的就是输入friend/docs，它将为你提供这个惊人的swagger
    UI。那么在这里。
- en: we are interested to know is we are interested to test our predict API。 So you
    can see that。 it is able to infer the kind of input that your API requires。 It
    is questioned and an， image file。 It is able to infer the data type as well。 So
    all you need to do is hit on try。 it out and submit it。 So you will be getting
    an answer in the form of string。 And if your。
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们感兴趣的是测试我们的预测API。因此你可以看到，它能够推断出你的API所需的输入类型。它被询问的是一个图像文件。它也能推断出数据类型。所以你只需点击尝试并提交即可。你将得到一个字符串形式的答案。如果你的模型。
- en: model runs into any error you can also get a proper error message displayed
    as a JSON， format。 So you can see that it is extremely easy to test out an entire
    application or。 your API specifically using such kind of UI。 And I did not write
    any extra lines of code。 in order to bring up this UI。 So that is how you can
    build an API which connects both your。
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 如果模型运行时遇到任何错误，你也可以看到以JSON格式显示的适当错误消息。因此你可以看到，使用这种UI测试整个应用程序或特定于你的API是极其简单的。而且我并没有写任何额外的代码来引入这个UI。这就是你如何构建一个连接你。
- en: model logic to your friend and service。 So what we covered so far is we saw
    how to build a。 crux of our application using VILT。 We also saw how to build a
    friend and service using。 React and we saw that API service acts like a bridge
    between those two。 So that is how。 you build an end to end deep learning based
    application。 Now let's see this entire application。
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 模型逻辑与服务的API。到目前为止，我们覆盖了如何使用VILT构建我们应用程序的核心。我们还看到如何使用React构建服务，我们看到API服务充当这两者之间的桥梁。这就是如何构建一个端到端的深度学习应用程序。现在让我们看看这个完整的应用程序。
- en: in action through a demo and I will let Manisha to demo it for you。 Now let's
    see this is the React based application that we have built。 I will give a demo
    and how。 to use this。 So this is where we take a picture or upload an image。 If
    you are using your mobile。 image you can capture an image， ask a question related
    to that image and you will get an。
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 通过演示展示其实际应用，我将让Manisha为你演示。现在让我们看看这是我们构建的基于React的应用程序。我将演示如何使用它。这是我们拍照或上传图像的地方。如果你使用手机图像，你可以捕捉一张图片，问一个与该图像相关的问题，然后你将得到一个。
- en: answer out of this。 Let me upload an image。 Let me start off with a very simple
    question。
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个中得到答案。让我上传一张图像。让我先问一个非常简单的问题。
- en: '![](img/e5e281b99ab1922feb99f35c9a0cb653_13.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e5e281b99ab1922feb99f35c9a0cb653_13.png)'
- en: So this is where we ask the question。 We need to click on this microphone button
    in order。 to ask a question and we need to click on it back so that it stops listening
    to us。 How。 was the weather looking like？ And I click on get answer。 We get an
    answer both in audio。 and text format。 Cloudy。 It says Cloudy which is the correct
    answer。
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们提出问题的地方。我们需要点击这个麦克风按钮以便提问，然后需要再次点击它以停止监听我们。天气看起来怎么样？我点击获取答案。我们得到了音频和文本格式的答案。多云。它说多云，这是正确的答案。
- en: Now let us try out a bit more complicated， image and a set of questions。 Again
    let me start off with a very simple question here。 What is this？ Straight。 It
    says speed but I am not sure if it is accurate。 I identify all。 the things that
    are there in this image。 So let me ask one more question。 What do I see。
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们尝试一个稍微复杂一点的图像和一组问题。让我先问一个非常简单的问题。这是什么？直截了当地说，它显示的是速度，但我不确定这是否准确。我识别出图像中的所有内容。让我再问一个问题。我看到的是什么？
- en: in front of me？ Sign。 Interesting。 It says sign but I am not sure if it is actually
    able。 to identify the type of the signal that we are seeing。 So let me ask one
    more question。 What is the sign indicating？ The destination crossing。 Amazing
    right。 It is actually able to identify the type of the， signal as well in this
    image。 Okay。
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在我面前？标志。有趣。它说标志，但我不确定它是否真的能够识别出我们看到的信号类型。那么让我再问一个问题。这个标志表示什么？目的地交叉。惊人吧？它实际上能够识别出这个图像中信号的类型。好的。
- en: Let me try out one last image。 By the way this application。 has two visibility
    features and the accessibility score is 92 according to the Google lighthouse。
    too。 Now coming back to this let me ask a question。 Can I walk now？ Well。 No。
    It says no but I am not sure if it is because of the signal or something else。
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 让我试最后一张图像。顺便说一下，这个应用程序有两个可见性功能，根据Google Lighthouse的评分可达92。现在回到这个问题，让我问一个问题。我现在可以走吗？嗯。不行。它说不行，但我不确定是因为信号还是其他原因。
- en: So let me ask one more question。 I told you she is not easy to please。 What
    is the color。 of the signal？ Red。 It says red。 Okay。 Let me ask one more question。
    What is the sign indicating？
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 所以让我再问一个问题。我告诉你，她不容易取悦。信号的颜色是什么？红色。它说是红色。好吧。让我再问一个问题。这个标志表示什么？
- en: Stop。 Awesome。 It is actually able to identify the type of the signal as well。
    That is it。 from the demo。 I will hand it over to Patmature。 She has more in store
    for you folks。 So yeah looks like our model does get a lot of answers correctly。
    But let's see if it is。
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 停止。太棒了。它实际上也能够识别信号的类型。这就是演示的全部。我将把时间交给Patmature。她为你们准备了更多内容。所以是的，看起来我们的模型确实能够正确回答很多问题。但让我们看看它是否。
- en: '![](img/e5e281b99ab1922feb99f35c9a0cb653_15.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e5e281b99ab1922feb99f35c9a0cb653_15.png)'
- en: able to recognize the image that we saw earlier。 So yeah。 Here is the application。
    We upload。 the image。 Yeah。 Like some of you might have already guessed。 It is
    Kim Kardashian from。 last year's Med Gala。 I mean come on。 I am a Gen Z standing
    in front of you giving the。 serious talk。 I had to bring in this image。 So let
    me ask a question。 What is this？ Uh oh。 Statue。
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 能够识别我们之前看到的图像。所以是的。这就是应用程序。我们上传了这个图像。是的。像你们中的一些人可能已经猜到的。是去年的Met Gala上的金·卡戴珊。我是说，拜托。我是一个站在你面前的Z世代，进行严肃的谈话。我必须带来这个图像。那么让我问一个问题。这是什么？呃哦。雕像。
- en: I wanted things that it just touch you。 So it turns out that our model。 indeed
    cannot keep up with the Kardashians。 You know。 But you know it works when it has，
    to。 So that is how our entire visual question answering application works。 So
    how we plan。 taking it forward is by integrating it with the amazing application
    called as glance。 So。
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我想要一些能打动你的东西。所以结果证明我们的模型确实无法跟上卡戴珊家族。你知道的。但是你知道它在必要时是有效的。所以这就是我们的整个视觉问答应用程序的工作原理。那么我们计划如何推进，是将其与一个叫做Glance的神奇应用程序集成。所以。
- en: '![](img/e5e281b99ab1922feb99f35c9a0cb653_17.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e5e281b99ab1922feb99f35c9a0cb653_17.png)'
- en: glance is the words largest screen zero platform。 What I mean by that is you
    can browse all。
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: Glance是全球最大的屏幕零平台。我的意思是，你可以浏览所有。
- en: '![](img/e5e281b99ab1922feb99f35c9a0cb653_19.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e5e281b99ab1922feb99f35c9a0cb653_19.png)'
- en: the amazing content right on the lock screen of your phone。 So like I mentioned
    before。 glance has more than 150 million daily active users out of which more
    than 50% of them belong。 to smaller cities and towns。 In order to be able to address
    such kind of audience where the。 computer or smart devices is relatively low，
    you need to make your application extremely。
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 手机锁屏上的惊人内容。所以像我之前提到的，Glance每天有超过1.5亿的活跃用户，其中超过50%来自较小的城市和城镇。为了能够服务于这样的受众，在计算机或智能设备相对较少的情况下，你需要让你的应用程序变得极其。
- en: easy to use and what can be more easier than having your question answered right
    on the。 lock screen of your phone。 So this is how we plan on doing it。 On the
    lock screen you do have。 the access to the camera。 You just need to swipe right。
    Take the image of the object and。 you need to hit on ask glance which will then
    get redirected to the urban application。
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 使用起来简单，什么会比在你手机的锁屏上直接回答你的问题更简单呢？所以这就是我们的计划。在锁屏上你确实可以访问相机。你只需向右滑动。拍摄对象的图像，然后你需要点击“询问Glance”，这将重定向到城市应用程序。
- en: So this is how we plan on maximizing our reach， maximizing our user reach with
    the hope of。 providing an extra pair of eyes to everyone in need。 So yeah， even
    if you did not understand。 what I just spoke for the last 20 minutes here at the
    TLDR version， we used a VQA V2 data set。 We used a model using VIT。 We built a
    prototype using Streamlet and front end using React。
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这就是我们计划如何最大化我们的覆盖范围，最大化我们的用户覆盖率，希望能为每一个需要帮助的人提供额外的视角。所以是的，即使你没有理解我刚才在这里说的20分钟的内容，在TLDR版本中，我们使用了VQA
    V2数据集。我们使用了基于VIT的模型。我们使用Streamlet构建了原型，并用React进行了前端开发。
- en: Our entire API service was built using fast API。 You can go ahead and clone
    this repo in。 order to try it out。 Plus on this repo you can also see how to deploy
    it on cloud platform。 We have shown how to use it， how to deploy it on GCP。 So
    yeah， try this out yourself because。 don't go by the recorded demo。 You need to
    try it out。 Just a few things is that it works。
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: the best in Google Chrome and Microsoft Edge。 Safari and Firefox has a bit of
    audio issue。 which we will be fixing it soon。 So that's it for today， folks。 Thank
    you so much。 And this is the team that helped us build it rather of Shiva's and
    Pablo's because without。 whom this entire application wouldn't have been possible。
    So a big thanks to them and a。
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: huge thanks to you all for being such an amazing audience today。 Thank you so
    much。 I hope you enjoyed。
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e5e281b99ab1922feb99f35c9a0cb653_21.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
- en: '[Applause]， (applause)。'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
