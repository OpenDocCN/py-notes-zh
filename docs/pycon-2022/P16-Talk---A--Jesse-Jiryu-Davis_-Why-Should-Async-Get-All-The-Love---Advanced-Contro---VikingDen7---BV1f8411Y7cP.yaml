- en: P16：Talk - A. Jesse Jiryu Davis_ Why Should Async Get All The Love   Advanced
    Contro - VikingDen7 - BV1f8411Y7cP
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: P16：演讲 - A. Jesse Jiryu Davis_ 为什么异步应该得到所有的关注  高级控制 - VikingDen7 - BV1f8411Y7cP
- en: Hello everyone， welcome。 The next talk will be starting soon。 Let's prepare
    ourselves。
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 大家好，欢迎。接下来的演讲很快就要开始了。让我们做好准备。
- en: '![](img/c4ef1fa886775c408a105c6ab11f8e26_1.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c4ef1fa886775c408a105c6ab11f8e26_1.png)'
- en: '![](img/c4ef1fa886775c408a105c6ab11f8e26_2.png)'
  id: totrans-3
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c4ef1fa886775c408a105c6ab11f8e26_2.png)'
- en: to hear Jesse and why should a sink get all the love？ Once controlled flow with
    threats。 I couldn't agree more。 Jesse， please welcome with a warm applause。 Thanks
    a lot。 AsyncIO is really hip。 And not just asyncIO。 Twisted tornado。 Those are
    the older async， frameworks。 Curio and trio， the new async frameworks。 They're
    all super cool。 They're hyped and， deservedly so。
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 来听听Jesse，为什么异步应该得到所有的关注？一旦控制流与线程结合。我完全同意。Jesse，请以热烈的掌声欢迎他。非常感谢。AsyncIO真的很时髦。不仅是asyncIO。Twisted、tornado。这些是较早的异步框架。Curio和trio，新的异步框架。它们都超级酷。它们备受推崇，确实如此。
- en: They are amazing。 I'm a big fan。 I spent a lot of time in the early days。 contributing
    to tornado and to asyncIO。 My very first PyCon talk eight years ago in Montreal。
    was what is async？ How does it work and when should you use it？ I was one of the
    early boosters。 of async。 AsyncIO introduced a lot of Pythonistas to some advanced
    control flow techniques。
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 它们真是太棒了。我是个大粉丝。我花了很多时间在早期参与tornado和asyncIO。八年前我在蒙特利尔的第一场PyCon演讲就是关于什么是async？它是如何工作的以及何时使用的？我是async的早期支持者之一。AsyncIO让许多Python开发者接触到了高级控制流技术。
- en: like tasks and futures， chaining， asyncIO gather。 All of this was really awesome
    and it was very。 exciting。 But in the excitement， there was a message that was
    lost which is that all。 this was already possible with threads too。 Compared to
    asyncIO， threads seem hopelessly outdated。 The cool kids will laugh at you if，
    you use threads。
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 像任务和未来、链式调用、asyncIO聚合。这一切真的很棒，非常令人兴奋。但在兴奋中，有一个信息被忽视了，那就是这一切其实也可以通过线程实现。与asyncIO相比，线程似乎显得无比过时。如果你使用线程，酷小子们会嘲笑你。
- en: Now when should you use either threads or asyncIO？ Threads and asyncIO， they。
    are two ways to do concurrency。 Now concurrency is not parallelism。 Parallelism
    is when your。
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你什么时候应该使用线程或asyncIO？线程和asyncIO是两种并发的实现方式。并发并不是并行。并行是当你的。
- en: '![](img/c4ef1fa886775c408a105c6ab11f8e26_4.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c4ef1fa886775c408a105c6ab11f8e26_4.png)'
- en: app is actually doing multiple things on multiple CPUs at once。 And Python mostly
    can't do parallelism， because of the global interpreter lock。 You can understand
    the gill with this phrase which， is short enough to read on your hand。 One thread
    runs Python while others sleep or await， IO。
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 应用程序实际上在多个CPU上同时执行多个任务。而Python大多不能实现并行，因为全局解释器锁。你可以用这句话理解gill，这句话短到足以在你的手上读。一个线程在运行Python，而其他线程则处于休眠或等待IO状态。
- en: And if you want to learn more about the gill， I give a PyCon talk a few years
    ago at， this link。 So threads and asyncIO， they are kind of even here。 Neither
    threads nor asyncIO。 tasks can use multiple CPUs。 And just so you know that I
    know what you are all thinking。 let's do a quick aside about multiprocessing which
    is that if you really need parallelism。
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想了解更多关于gill的内容，我几年前在这个链接的PyCon演讲中讲过。所以线程和asyncIO在某种程度上是平衡的。线程和asyncIO任务都无法利用多个CPU。顺便说一句，我知道你们都在想什么。我们来简单谈谈多进程，如果你真的需要并行性。
- en: you should use multiprocessing。 But it's not the panacea that it's made out
    to be。 It is。 the only way for standard Python to use multiple cores but coordinating
    and exchanging data。 among processes has a lot of hidden complexity。 So beware。
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你应该使用多进程。但这并不是它被描述的万能解决方案。它是标准Python使用多个核心的唯一方法，但在进程之间协调和交换数据有很多隐藏的复杂性。所以请小心。
- en: '![](img/c4ef1fa886775c408a105c6ab11f8e26_6.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c4ef1fa886775c408a105c6ab11f8e26_6.png)'
- en: So what this talk is actually about is concurrency。 And concurrency is dealing
    with events in。 partial order。 That means you are waiting for several things to
    happen but you don't。 know what order they will happen in。 The main example by
    far of concurrency is waiting。 for data on network connections at once。 Some network
    needs concurrency。 Basically all network。
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这场演讲实际上是关于并发的。并发是处理事件的部分顺序。这意味着你在等待几件事情的发生，但你不知道它们会以什么顺序发生。目前并发的主要例子是同时等待网络连接上的数据。一些网络需要并发。基本上所有的网络。
- en: servers need concurrency， sometimes extremely high levels of concurrency。 And
    we can use。 threads or kind of async framework like asyncIO as our method of supporting
    concurrency。 So which one should you use？ Let's start with asyncIO's advantage。
    Very， very high concurrency。 programs are more efficient with asyncIO when it
    comes to memory。 Here's a chart of two programs。
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 服务器需要并发，有时需要极高的并发水平。我们可以使用线程或像 asyncIO 这样的异步框架作为支持并发的方法。那么你应该使用哪一个呢？让我们从 asyncIO
    的优势开始。非常，非常高的并发。就内存而言，asyncIO 的程序更高效。这里是两个程序的图表。
- en: The one in blue is making lots of threads。 The one in， red lots of asyncIO tasks。
    The red programs start using more memory just because it imported， asyncIO。 But
    that doesn't really matter。 What matters is that as you add more concurrent， things。
    threads or tasks， the memory usage for the asyncIO program grows more slowly。
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 蓝色的程序创建了大量线程。红色的程序创建了许多 asyncIO 任务。红色程序开始使用更多内存，仅仅是因为它引入了 asyncIO。但这并不重要。重要的是，随着你增加更多的并发元素，线程或任务，asyncIO
    程序的内存使用增长得更慢。
- en: And I'll share links to details about this benchmark at the end of the talk。
    From the previous chart， we saw that threads take about 10K each。 Python threads
    just don't。 take that much memory。 More than a few hundred threads is impractical。
    But below that， they。 really work quite nicely。 If you recall the problems with
    large numbers of threads that。
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我将在演讲结束时分享有关该基准测试的详细链接。从之前的图表中，我们看到每个线程大约需要 10K 的内存。Python 线程并不需要那么多内存。超过几百个线程就不切实际。但低于这个数量时，它们运作得相当不错。如果你回忆起大量线程所带来的问题。
- en: David Beasley pointed out many years ago， those were in Python 2。 Those have
    been fixed。 In comparison， in asyncIO， each task takes about two K-grams。 So it
    is more efficient。 AsyncIO is more efficient for huge numbers of mostly idle connections
    where it's not。 actually using the processor all that much for each one。 Is asyncIO
    faster than threads？ No。
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 大卫·比斯利多年前指出，这些是在 Python 2 中的问题。那些问题已经得到修复。相比之下，在 asyncIO 中，每个任务大约需要两 K-克。因此它更高效。asyncIO
    在大量大多数处于空闲状态的连接中更高效，实际使用 CPU 的频率相对较低。asyncIO 比线程快吗？不。
- en: As CalPetersen wrote， "Sadly， async is not go faster stripes for the Python
    interpreter。 Under realistic conditions， asynchronous web frameworks are slightly
    worse throughput and。 much worse latency variance。" The standard library is slower
    than a lot of。 multi-threaded frameworks。 That's just because asyncIO executes
    lots of Python to process each。
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 正如 CalPetersen 所写，“可悲的是，async 并不是 Python 解释器的加速条。在现实条件下，异步网络框架的吞吐量稍差，延迟方差更差。”标准库比许多多线程框架要慢。这只是因为
    asyncIO 执行了大量 Python 来处理每个请求。
- en: event。 Generally， frameworks are faster the more that they are implemented in
    C。 Some。 of the newer async frameworks that have lots of C and particularly those
    based on UV loop。 are quite fast。 But they're not faster than multi-threading。
    Generally， tail latency。 still seems to be a problem with them。 I'm not going
    to necessarily say that all async。
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: event。一般来说，框架在用 C 实现得越多时，速度越快。一些较新的异步框架，尤其是基于 UV 循环的框架，运行得相当快。但它们的速度仍然不及多线程。通常，尾部延迟似乎依然是个问题。我不打算说所有的异步框架都是这样。
- en: frameworks are slower than all multi-threaded frameworks。 I think what I can
    say without。 getting into trouble is asyncIO isn't faster。 It's efficient， again，
    only for huge numbers。 of mostly idle connections and only for the data。 What
    about compatibility？
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 框架的速度比所有多线程框架都慢。我想我可以说，asyncIO 并不是更快。它高效，只有在大量大多数处于空闲状态的连接时，才能发挥作用。那么兼容性如何呢？
- en: Does asyncIO work with the tools that you're already using？ Well， what web frameworks
    do you use？
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: asyncIO 是否能与你已经使用的工具兼容？那么，你使用什么网络框架呢？
- en: Here are the most popular web frameworks last year。 The total is more than 100%
    because people could choose multiple。 The flask and Django。 the most popular ones
    are multi-threaded frameworks。 They don't， for the most part。 work with asyncIO。
    Only three of these are asyncIO compatible。 So if you want to use asyncIO。
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这是去年最流行的网络框架。总数超过 100%，因为人们可以选择多个。Flask 和 Django，这两个最受欢迎的框架都是多线程框架。在大多数情况下，它们不与
    asyncIO 一起工作。这其中只有三个与 asyncIO 兼容。所以如果你想使用 asyncIO。
- en: that probably， means that you're going to have to rewrite large portions of
    your app。 AsyncIO is incompatible， with the most popular frameworks。 How tricky
    is it to write correct concurrent code with threads versus asyncIO？ Let's make。
    a function to something。 It will increment a global counter。 And we can run it
    on two threads。
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能意味着你需要重写应用的大部分内容。异步 I/O 与最流行的框架不兼容。用线程与异步 I/O 编写正确的并发代码有多棘手？让我们做一个函数来增加一个全局计数器。我们可以在两个线程上运行它。
- en: at once。 Will counter always eventually equal two？ No， unfortunately it won't
    because plus。 equals is not atomic。 It's got several substeps。 It has to load
    the value from the global add。 one to it and save it back。 And if two threads
    interleave during this process， one of the。 updates can be lost and the final
    value might only be one， not two。
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 一次性处理。计数器是否最终会等于二？不，遗憾的是不会，因为加等式（plus equals）不是原子操作。它有几个子步骤。它必须从全局加法中加载值，给它加一，然后保存回来。如果在这个过程中两个线程交替运行，可能会丢失其中一个更新，最终的值可能只有一，而不是二。
- en: We need to protect plus equals with a lock。 I think that doing this correctly，
    it's kind， of tricky。 This is a lot of code and a subtle gotcha for a very simple
    task。 This blog post。 from 2014 talks about this trickiness problem。 It's by glyph，
    the author of the twisted async。 framework。 It's pretty old， but it's still my
    favorite argument about this topic。 Glyph， writes。
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要用锁来保护加等式。我认为正确地做到这一点有点棘手。这是一段很长的代码，对于一个非常简单的任务来说，这是一个微妙的陷阱。2014 年的这篇博客文章讨论了这个棘手的问题。作者是
    Glyph，他是 Twisted 异步框架的创始人。这篇文章虽然很旧，但仍然是我对这个主题最喜欢的论点。Glyph 写道。
- en: as we know， threads are a bad idea for most purposes。 I disagree with Glyph。
    This， is my point。 He says threads make local reasoning difficult and local reasoning
    is perhaps the。 most important thing in software development。 Glyph's point is
    that we should use async。 not because it's faster than threads and not even because
    it's more efficient in memory。
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所知，线程在大多数情况下都是个坏主意。我不同意 Glyph 的看法。这是我的观点。他说线程使得局部推理变得困难，而局部推理可能是软件开发中最重要的事情。Glyph
    的观点是我们应该使用异步，不是因为它比线程快，甚至不是因为它在内存方面更高效。
- en: but rather because it's less tricky。 Let's see this in action。 Here's our counter
    value。 incrementer， but now it's a co-routine。 It's async def。 We'll run it on
    two tasks at once。 You can tell just by looking at the code where it might interleave
    with another co-routine。 Anywhere that there's an await expression， it couldn't
    leave。 That means anywhere that， there isn't。
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 而不是因为它更棘手。让我们看看这个实际应用。这里是我们的计数器值，增加器，但现在它是一个协程。它是 async def。我们将在两个任务上同时运行它。你可以通过查看代码就能判断它可能在哪些地方与另一个协程交替。任何有
    await 表达式的地方，它就可能中断。这意味着任何没有 await 的地方，它就不会。
- en: it can't。 That means that plus equals， it's atomic because there's no plus，
    equal。 there's no await in plus equals。 So we don't need a lock。 This means async。io
    is。 better because it's less tricky than multi-threading。 So in summary， speed，
    threads are at least。 just fast。 Compatibility， threads work with Flask， Django，
    without rewriting your app for async。io。
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 它无法做到。这意味着加等式是原子性的，因为没有加号，也没有等号。加等式中没有 await。所以我们不需要锁。这意味着异步 I/O 更好，因为它比多线程更简单。因此，总结一下，速度方面，线程至少是快速的。兼容性方面，线程可以与
    Flask、Django 一起使用，而不需要重写你的应用以适应异步 I/O。
- en: In summary， async。io efficiently waits on huge numbers of mostly idle network
    connections。 and trickiness。 Async。io is less error prone than threads。 But my
    point is that it doesn't。 need to be this way。 I think it's time to take another
    look at threads。 All along it's。 been possible to write elegant， correct concurrent
    code with threads。 It doesn't have to be tricky。
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 总而言之，异步 I/O 高效地等待大量大部分处于闲置状态的网络连接，处理这些棘手的问题。异步 I/O 比线程更不容易出错。但我的观点是，这不必是这样的。我认为是时候重新审视线程了。一直以来，使用线程编写优雅且正确的并发代码都是可能的。它不必那么棘手。
- en: and complicated。 Threads are actually cool。 So now we're getting into the heart
    of this， talk。 how to write less tricky code with threads。 To begin， let's see
    how to use threads， with futures。 Threads had futures first before async。io。 Futures
    let us express control flows。 that you'd struggle to write with the old locks
    and condition variables。 Confusingly， async。io。
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 线程实际上很酷。所以现在我们要深入探讨这个话题：如何使用线程编写更简单的代码。首先，让我们看看如何使用带有未来（futures）的线程。线程在异步 I/O
    之前就有了 futures。Futures 让我们能够表达控制流，而用旧的锁和条件变量编写这些流会很困难。令人困惑的是，异步 I/O。
- en: has a different future class from threads。 But I've never had to use both in
    the same program。 at once， so it's fine。 Here's our previous code example with
    the lock that protects the， counter。 We can use futures to make it better。 First，
    let's add this。 We want to know what。 the counter value is at the very end。 And
    then let's rewrite this with views。 So the first。
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 有一个与线程不同的未来类。但我从未在同一个程序中同时使用过这两者，所以没关系。这是我们之前的代码示例，带有保护计数器的锁。我们可以使用未来来改进它。首先，让我们添加这个。我们想知道最后计数器的值是什么。然后让我们用视图重写这个。因此，第一个。
- en: thing we do is we import from concurrent futures。 This is where all the cool
    thread stuff lives。 It's been here since Python 3。2， so a while now。 Now our do
    something function。 It takes。 a future as an argument and it sets the result to
    one。 This is called resolving the future。 And one here is just how much to add
    to the counter。 We'll run do something on two threads。
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先从concurrent futures导入。这是所有酷线程内容所在的地方。自Python 3.2以来，它就一直在这里，所以有一段时间了。现在我们的do
    something函数。它接受一个未来作为参数，并将结果设置为一。这被称为解决未来。而这里的一就是要加到计数器的数量。我们将在两个线程上运行do something。
- en: and we'll pass in the two futures as arguments。 And then we will wait for the
    futures to be。 resolved， calling future。result， waits for some other thread to
    call set result。 Once。 both of these futures are resolved， we sum the results
    and that's our counter。 This code。 isn't really much of an improvement yet， but
    I'm just showing you how futures work。
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将把两个未来作为参数传入。然后我们将等待未来被解决，调用future.result，等待其他线程调用set result。一旦这两个未来都解决了，我们求和结果，这就是我们的计数器。这段代码还不算太大的改进，但我只是向你展示未来是如何工作的。
- en: We're going to get there。 Here's a better example。 Let's use a thread pool executor。
    This is a thread pool that runs things on threads and reuses threads efficiently。
    Now our do something， function。 It doesn't take a future and resolve it。 It just
    returns one。 And we'll call it using， executor。submit that starts the function
    running very soon on a background thread。
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们会做到的。这是一个更好的例子。让我们使用一个线程池执行器。这是一个在线程上运行任务并高效重用线程的线程池。现在我们的do something函数。它不接受未来并解决它。它只返回一个。我们将使用executor.submit调用它，这会很快在后台线程上启动函数。
- en: And it returns， a future which is resolved when that function returns。 The rest
    of our code here is the， same as before。 We wait for the results of the two futures
    and sum them。 This is looking， much better than old fashioned code using a lock。
    but there's more room for improvement。 Executor。map。 It's like the built-in map。
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 它返回一个未来，当该函数返回时得到解决。我们这里的其余代码与之前相同。我们等待两个未来的结果并对它们求和。这看起来比使用锁的老式代码要好得多，但还有更多改进的空间。Executor.map。它像内置的映射。
- en: but it's concurrent。 It runs the function， on each of the arguments all at the
    same time。 So we'll give it range of two to undo something， twice concurrently。
    We don't actually need the input arguments， so we'll just use a dummy， argument
    here。 There's no more explicit futures here， no explicit threads。 It's all hidden。
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 但它是并发的。它在所有参数上同时运行函数。因此，我们将提供一个范围为二的值以同时撤销某件事情。我们实际上并不需要输入参数，所以在这里只会使用一个虚拟参数。这里没有更多显式的未来，也没有显式的线程。这一切都是隐藏的。
- en: inside the map implementation。 I think this looks really nice and it's not error
    prone， at all。 What about more complex workflows？ See， you want to make some French
    press coffee。 Like I did this morning。 I brought a hand grinder with me to Salt
    Lake City， so grinding the。 coffee takes a little bit of time。 And then I heated
    the water。 I put them both in the。
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在映射实现内部。我认为这看起来真的很好，并且一点也不容易出错。更复杂的工作流程呢？你想要做一些法式压滤咖啡。就像我今天早上做的那样。我带了一个手动研磨机到盐湖城，所以研磨咖啡需要一点时间。然后我加热水。我把它们都放进了。
- en: press to brew and four minutes later I drank it。 Obviously this is not very
    efficient。 I should be heating the water and grinding the coffee concurrently，
    so it should really。 look like this。 So how do we code this kind of workflow with
    threads？ Let's make a thread。 pool executor。 Then there's functions to heat the
    water and grind the coffee。 And then this。
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 压滤壶来冲泡，四分钟后我喝了它。显然，这并不是很高效。我应该同时加热水和研磨咖啡，所以它应该真的。看起来像这样。那么我们如何用线程编码这种工作流程？我们来创建一个线程池执行器。然后有加热水和研磨咖啡的函数。然后这个。
- en: brew function， it takes two futures and waits until both have been completed。
    And then it。 waits four minutes for the coffee to brew。 So we can use the executor，
    the coffee， heat。 the water and grind the coffee concurrently。 And then call brew
    with those two futures。 And once it's done we can drink the coffee。 So far so
    good。 Let's add some more steps。
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 冲泡函数，它接受两个未来并等待直到两者都完成。然后它等待四分钟让咖啡冲泡。因此，我们可以使用执行器，让咖啡、加热水和研磨咖啡并发执行。然后用这两个未来调用冲泡。一旦完成，我们就可以喝咖啡。到目前为止一切顺利。让我们增加更多步骤。
- en: to this workflow and see how modern thread programming handles the extra complexity。
    So there's a quick step after heating the water。 You have to pour it into the
    press and。 there's a step after grinding the coffee。 You have to pour that into
    the press。 So these。 can be finished in either order， but we want to do the red
    step as soon as it's blue step。
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这个工作流程展示了现代线程编程如何处理额外的复杂性。因此，在加热水之后还有一个快速步骤。你必须将其倒入压滤器，并且在研磨咖啡之后还有一个步骤。你必须将咖啡倒入压滤器。因此，这两个步骤可以按任意顺序完成，但我们希望尽快完成红色步骤。
- en: is completed。 So we want to do two things as their prerequisites are done。 Here's
    how， to code this。 So now heat water and grind coffee， they've got return values，
    they produce， some sort of product。 And our brew function now， we're going to
    use as completed。 This。 is also in concurrent futures in the standard library。
    And it returns the futures in the。
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 工作完成后。所以我们希望在它们的前置条件完成时做两件事。以下是如何编码这一点。现在加热水和研磨咖啡都有返回值，它们产生某种产品。而我们的冲泡函数现在将作为完成的内容来使用。这也在标准库的并发未来中，并且它返回未来的结果。
- en: order that they are completed。 So if the water is heated first， we handle that
    first， or if。 the coffee is ground first， we handle that first。 Once both are
    done， then we wait four， minutes。 And the rest of the code is as before。 Imagine
    if you had to write this using。 locks and condition variables to signal when each
    of these steps was done and to trigger。
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 按照它们完成的顺序。所以如果水先加热，我们就先处理那个，或者如果咖啡先研磨，我们就先处理那个。一旦两者都完成，我们就等待四分钟。其余的代码与之前相同。想象一下，如果你必须使用锁和条件变量来发出信号，说明每个步骤完成并触发下一步该多麻烦。
- en: the next step， this would be a nightmare。 But using concurrent futures， this
    is actually。 really slick。 Of course， it's not really modern code yet， because
    we don't have any。 type annotations。 Here I've annotated the return types of heat
    water and grind coffee。 And if。 you have a future that resolves to a certain type，
    you just subscripted like this。 So this。
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步，这将是一个噩梦。但使用并发未来，这实际上是非常流畅的。当然，这还不是真正的现代代码，因为我们没有任何类型注解。在这里，我已经注解了加热水和研磨咖啡的返回类型。如果你有一个解析到某种类型的未来，你可以像这样进行下标。所以这个。
- en: will be a future string。 And then the type system knows that when you call future。result。
    that will be a string， or a well-thrown exception。 Let's take another look at
    this workflow。 What。 if things get even more complex？ What if this workflow is
    one component of like a whole。 afternoon that I'm spending？ So this is just the
    coffee workflow。 I've also got my chores， workflow。
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这将是一个未来的字符串。然后类型系统知道，当你调用 `future.result` 时，它将是一个字符串，或一个妥善抛出的异常。让我们再看看这个工作流程。如果事情变得更加复杂怎么办？如果这个工作流程是我整个下午花费的一个组成部分呢？所以这只是咖啡的工作流程。我还有我的家务工作流程。
- en: So first I'm going to make and drink coffee， and then that gives me the motivation，
    to do my chores。 And of course， I'm listening to a podcast concurrently the entire
    time that， I'm doing this。 How would we handle a complex set of workflows like
    this with modern threading？
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 所以首先我要做的是制作和饮用咖啡，然后这给了我做家务的动力。当然，我在整个过程中一直在听播客。我们如何用现代线程处理这样复杂的工作流程呢？
- en: The nice way to structure this is with a with statement。 So it's start a block
    like with。 thread pull executor and run a function on that executor。 And you can
    start an inner。 executor using another with statement。 When you leave the block，
    either normal or with。 an exception， it joins and shuts down the executor automatically。
    So all threads begin。
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 结构化这个的好方法是使用 `with` 语句。因此，你可以像 `with thread pool executor` 开始一个块，并在那个执行器上运行一个函数。你可以使用另一个
    `with` 语句启动一个内部执行器。当你离开这个块时，无论是正常还是因为异常，它都会自动连接并关闭执行器。因此，所有线程开始。
- en: within that block must finish one way or another before you can leave the block。
    And you can。 have multiple inner blocks within that outer block。 This style is
    called structured concurrency。 And it's been popularized in a few languages and
    frameworks。 Python people have mostly。 learned about it from Nathaniel Smith's
    Trio async framework。 And task groups are going。
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在那个块内必须以某种方式完成，才能离开该块。你可以在外部块内有多个内部块。这种风格被称为结构化并发。它在一些语言和框架中变得流行。Python的人们大多是从Nathaniel
    Smith的Trio async框架中了解它的。任务组也在进行中。
- en: to be useful for this with async。io in the standard library in Python 3。11。
    Unfortunately。 you can't really do this with threads fully。 The kind of definition
    of structured concurrency。 is that if one of the tasks in a block fails， then
    all of the other threads that that block。 started or will start should be canceled
    very soon。 And unfortunately， we can't really。
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在Python 3.11的标准库中，与async.io一起使用是有用的。不幸的是，你无法真正完全使用线程。这种结构化并发的定义是，如果块中的一个任务失败，那么该块启动或将要启动的所有其他线程都应该很快被取消。不幸的是，我们实际上无法做到。
- en: do that with threads because threads are not as good at cancellation as async。io。
    I'm going to show you a handwritten solution。 You'd have to do something like
    this with， your app。 First， you want to make a custom exception type thread canceled
    error。 I'm。 going to call it inherit from base exceptions。 So it's kind of hard
    to catch。 And then you。
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 用线程来做到这一点，因为线程在取消方面不如async.io。我将展示一个手写的解决方案。你需要在应用程序中做一些这样的事情。首先，你想创建一个自定义异常类型，称为线程取消错误。我将让它继承自基本异常。因此，捕获它有点困难。然后你。
- en: create a token， which is initially not canceled。 And it's got a method for checking
    if it's。 canceled。 Now in our do something function， we have to very frequently
    check if the token。 is canceled or not。 And you can't forget to check。 Otherwise，
    the whole thing won't work。 The rest of the code creates a token and a thread
    pull executor starts the function on。
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个令牌，最初未被取消。它有一个检查是否被取消的方法。现在在我们的执行某些操作的函数中，我们必须非常频繁地检查令牌是否被取消。你不能忘记检查。否则，整个事情就无法正常工作。其余代码创建一个令牌和一个线程池执行器，开始运行该函数。
- en: a thread passing in the token。 A second later， it marks the token canceled。
    And then sometime。 soon after that， the thread should kill itself。 We can wait
    for that by waiting on future。result。 which will then throw the canceled error。
    Python doesn't control the thread scheduler。 the way that it controls the async。io
    event loop。 So cancellation with threads just， it。
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 一个线程传入令牌。一秒钟后，它将令牌标记为取消。然后，在那之后不久，线程应该自杀。我们可以通过等待future.result来等待，这将抛出取消错误。Python并不控制线程调度程序的方式就像它控制async.io事件循环那样。因此，线程的取消就是这样。
- en: can't ever be as good as with async。io。 There are some better ideas out there
    like this one。 from Nathaniel Smith。 I'm curious if anybody's got a pep in progress
    to improve thread。 But enough for the bad news about threads， let's get back to
    the good news。 Here's a real。 life example that I coded a few months ago。 So I
    made a toy Paxos implementation。 Paxos。
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 永远无法像async.io那样好。有一些更好的想法，比如Nathaniel Smith的这个。我很好奇是否有人在进行改进线程的PEP。但关于线程的坏消息够多了，让我们回到好消息。这是我几个月前编码的一个真实示例。所以我做了一个玩具Paxos实现。Paxos。
- en: is a protocol for servers to work together for fault tolerance。 So we've got
    a group of。 servers here host 01 and 02， and they all talk to each other。 But
    how does each server know。 who all of the members of the group are？ Let's give
    them a config file with the list， of servers。 So here it is。 But now， how does
    each server know which one in the list it， is？
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个服务器协同工作以实现容错的协议。所以我们这里有一组服务器，主机01和02，它们彼此之间都在通信。但每个服务器如何知道组内所有成员是谁？我们给他们一个配置文件，列出服务器。所以就是这样。但是现在，每个服务器如何知道自己在列表中是哪个？
- en: This is actually surprisingly hard in cloud deployments or even in a data center。
    Every server usually has a couple of IP addresses， a couple of DNS names。 It's
    not actually that。 easy to know。 If you've got a DNS name， is this my name or
    not？ Just calling gethost。 name won't give you the information you need。 So the
    solution to this， it's kind of amazing。
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 这在云部署甚至数据中心中实际上是相当困难的。每个服务器通常有几个IP地址和几个DNS名称。实际上，这并不容易知道。如果你有一个DNS名称，这是否是我的名称？仅仅调用gethost.name并不能给你所需的信息。因此，这个解决方案，真是令人惊讶。
- en: First of all， every server when it starts up generates a unique ID for itself。
    Next， each。 server sends a request to all the servers in the list asking them
    for IDs。 That includes。 asking itself for its own ID， but it doesn't know that
    because it doesn't know which one， it is。 So here's host 0， requesting all of
    the server's IDs。 The other servers are doing， this too。
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，每个服务器在启动时都会为自己生成一个唯一的ID。接下来，每个服务器会向列表中的所有服务器发送请求，询问它们的ID。这包括向自己请求自己的ID，但它不知道，因为它不知道自己是哪一个。因此，这里是主机0，请求所有服务器的ID。其他服务器也在这样做。
- en: Host 0 and host 1， they respond with different IDs。 And host 0 also responds。
    to itself with its own ID。 It sees that they are equal so it knows， aha， that's
    me， I'm， host 0。 This is actually how MongoDB Replica sets and lots of other distributed
    systems， solve this problem。 But it's complicated because a server can process
    any other requests。
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 主机0和主机1以不同的ID响应。主机0还向自己响应自己的ID。它看到这些ID是相等的，所以它知道，啊哈，那是我，我是主机0。这实际上是MongoDB副本集和许多其他分布式系统解决这个问题的方式。但这很复杂，因为服务器可以处理任何其他请求。
- en: until it knows a list entry corresponds to itself。 Also， servers can start in
    any order。 So this is a complex workflow。 How are we going to make this clean
    and nice with threads？
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 直到它知道列表项对应于自己。此外，服务器可以以任何顺序启动。因此这是一个复杂的工作流程。我们如何使这个过程在多线程中干净而优雅？
- en: Here's the server code。 First thing， we create a unique ID。 I'm going to use
    Flask for the。 server because I love Flask， it's the most popular。 And to create
    this self-future which。 will be resolved once I've found myself。 Here's a flat，
    flat， flat， flat， flat， flat， flask。 root to return the server ID to anybody who
    requests it。 And here's some client request， handler。
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这是服务器代码。首先，我们创建一个唯一的ID。我将使用Flask作为服务器，因为我喜欢Flask，它是最流行的。为了创建这个self-future，一旦我找到自己，它就会被解析。这是一个扁平的Flask根，用于返回服务器ID给任何请求它的人。这里是一些客户端请求处理器。
- en: We can't handle any client requests until we know who we are。 So we have to
    call， self-future。result。 That'll wait for self-future to be resolved。 And then
    once it's resolved， this handler。 this 。result call will return right away。 When
    the server starts up， it loads， up its config。 creates an executor。 And it runs
    Flask in the background on a thread。 And。
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们知道自己是谁之前，我们无法处理任何客户端请求。因此，我们必须调用self-future.result。这将等待self-future被解析。一旦解析完成，这个处理器的result调用将立即返回。当服务器启动时，它加载其配置，创建一个执行器，并在后台线程上运行Flask。
- en: that means that the main thread is able to keep doing stuff。 And what is it
    doing？ It's。 launching the search for itself。 So here's the search loop。 First
    we note the time。 And。 then while it's been less than 60 seconds and we haven't
    found a self， we will continue。 We will search every server name in the list。
    And for each one of those， we'll use the requests。
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着主线程能够继续进行操作。它在做什么？它正在启动对自己的搜索。因此，这是搜索循环。首先我们记录时间。然后，只要不超过60秒并且我们没有找到自己，我们将继续。我们将搜索列表中的每个服务器名称。对于每一个，我们将使用请求。
- en: client library to request that server's unique ID。 That means that we'll hit
    this request。 handler either on this server or another server， but we don't know
    yet。 If the server IDs， match。 then we found ourselves。 So that means that we
    can resolve the future。 And that will。 unblock any threads that are waiting to
    process client requests。 Or we might throw an exception。
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 客户端库请求该服务器的唯一ID。这意味着我们将在该服务器或其他服务器上触发这个请求处理器，但我们还不知道。如果服务器ID匹配，那么我们找到了自己。这意味着我们可以解析未来。这将解除任何等待处理客户端请求的线程，或者我们可能会抛出异常。
- en: We could throw an exception because we're trying to contact another server and
    it's not。 started yet。 Servers can start in any order。 We might even get an exception
    trying to contact。 ourselves because we're starting Flask on a thread in the background
    and it can take any。 amount of time to get started。 So we have to be prepared
    for that also to fail if we。
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可能会抛出一个异常，因为我们试图联系另一个尚未启动的服务器。服务器可以以任何顺序启动。我们甚至可能会在尝试联系自己时遇到异常，因为我们在后台线程上启动Flask，启动所需的时间是不可预测的。因此，我们也必须为这种失败做好准备。
- en: don't wait long enough。 So if there's any failure， if we haven't fed ourselves，
    then。 we sleep for a second and then keep trying。 If we do find ourselves， then
    we reach the， end。 And we wait for the app done future to be resolved。 We got
    that from executor submit， up above。 So this means there were letting Flask run
    in the background until it gets control。
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 不要等待太久。如果发生任何故障，或者我们没有满足自己的需求，那么我们会睡一秒钟，然后继续尝试。如果我们找到了自己，那么就到达了结束。我们在等待应用完成未来的结果。这是我们从上面的执行器提交中获得的。因此，这意味着我们让Flask在后台运行，直到它获得控制权。
- en: C signal and decides to quit。 This is pretty clean and clear， right？ We had。
    a complex problem statement and we used executors and futures to come up with
    a very straightforward。 and legible way of expressing this with threads。 We didn't
    have to use async。io and that means。 that we could use Flask and we could use
    requests。 If we'd used async。io， we would not have been。
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: C信号决定退出。这很简单明了，对吧？我们有一个复杂的问题陈述，我们使用执行器和未来来提出一种非常直接且易读的方式来表达这个问题。我们不必使用async。io，这意味着我们可以使用Flask并使用requests。如果我们使用async。io，就不会是这样。
- en: able to use Flask and requests。 We would have had to use excellent alternatives
    but not。 the ones that we were used to。 So threads are cool。 Don't let the async。io。
    kids make you feel like a nerd。 Threads are a better choice than async。io for
    most concurrent。 programs。 They're at least as fast as async。io。 They're compatible
    with all the popular frameworks。
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 能够使用Flask和requests。我们本可以使用优秀的替代方案，但不是我们习惯的那些。因此，线程是很酷的。不要让async。io的孩子们让你觉得自己是个书呆子。对于大多数并发程序来说，线程是比async。io更好的选择。它们至少和async。io一样快，并且与所有流行框架兼容。
- en: and with the techniques that we just look at using executors and futures。 It's
    possible。 to write clean， elegant concurrent code。 Thanks。 [ Applause ]。
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 使用我们刚刚看到的执行器和未来的技术，可以编写干净、优雅的并发代码。谢谢。[掌声]
- en: '![](img/c4ef1fa886775c408a105c6ab11f8e26_8.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c4ef1fa886775c408a105c6ab11f8e26_8.png)'
- en: '![](img/c4ef1fa886775c408a105c6ab11f8e26_9.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c4ef1fa886775c408a105c6ab11f8e26_9.png)'
