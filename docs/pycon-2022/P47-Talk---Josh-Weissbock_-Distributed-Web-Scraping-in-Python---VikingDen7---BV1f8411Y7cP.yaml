- en: P47：Talk - Josh Weissbock_ Distributed Web Scraping in Python - VikingDen7 -
    BV1f8411Y7cP
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: P47：演讲 - Josh Weissbock_ Python中的分布式网络抓取 - VikingDen7 - BV1f8411Y7cP
- en: So yeah， I'm here today to talk about distributed webscraping。
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，我今天在这里讨论分布式网络抓取。
- en: '![](img/d0fd66aaacea9ae07aa87bfadc637056_1.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d0fd66aaacea9ae07aa87bfadc637056_1.png)'
- en: It's my first PyCon since 2015 in Montreal。 First time I've been selected to
    actually speak at PyCon。 So I'm pretty excited about that。 Today I'm going to
    be talking about distributed webscraping with Python and some lessons I've。 learned
    along the way。 So like I said， the goal of this presentation is to motivate you
    on when and why and how。 you should use distributed webscrapers and how you can
    build one if you need to。
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我自2015年在蒙特利尔以来的第一次PyCon。这是我第一次被选中在PyCon上发言。所以我对此非常兴奋。今天我要讨论的是使用Python进行分布式网络抓取，以及我在这个过程中学到的一些经验教训。正如我所说，本次演讲的目标是激励你了解何时、为什么以及如何使用分布式网络抓取工具，以及如果你需要的话，如何构建一个。
- en: So I'll give you an introduction to distributed webscraping。 I'll set the stage
    when you'd actually want to use one。 I'll kind of walk through you my journey
    of how I came to actually needing to build one。 what it finally looks like and
    then a bunch of tips and tricks and management and Python。
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我将为你介绍分布式网络抓取。我会设定一个场景，告诉你在什么情况下实际上需要使用它。我将分享我需要构建一个的旅程，最终它的样子是什么，以及一些技巧、管理和Python的相关内容。
- en: packages and some cool stuff I found that will help you if you want to build
    distributed， scrapers。 So quick introduction to myself so you know that I have
    some validity of what I'm talking， about。 My name is Josh。 I'm a Canadian。 I live
    in Colorado right now。 I'm actually moving back to Canada this summer。 I've been
    using Python for about 14 years now。
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 包和我发现的一些酷东西，如果你想构建分布式抓取器，这些会对你有所帮助。所以我先简单介绍一下我自己，这样你知道我说的内容是有依据的。我的名字是Josh。我是加拿大人，目前住在科罗拉多州。今年夏天我会搬回加拿大。我已经使用Python大约14年了。
- en: back in 2。5 when I was in university。 I did my undergrad West Coast Canada and
    I did my masters in the East Coast in Ottawa where。 I did machine learning， natural
    language processing and artificial intelligence。 I use Python all through that。
    Right now I'm employed with Department of Defense here in the West where I use
    data science。 and data engineering primarily in the airspace domain。
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在我上大学的2015年，我在加拿大西海岸完成了我的本科，在东海岸的渥太华获得了硕士学位，专攻机器学习、自然语言处理和人工智能。我在整个过程中一直使用Python。目前，我在西部的国防部工作，主要从事数据科学和数据工程，尤其是在航空领域。
- en: I've consulted multiple times for professional sports teams and leagues in a
    data science。 data analytics type role building pipelines， doing data science，
    doing analysis， providing。 analysis to decision makers。 And of course I want to
    give a shout out to Statistics of the Borders。 Someone I've been volunteering
    with for last actually just this year is my first time I'm。
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 我曾多次为职业体育团队和联盟提供咨询，担任数据科学、数据分析类型的角色，构建管道，进行数据科学和分析，并向决策者提供分析结果。当然，我还想感谢边界统计学。我今年第一次与他们合作，实际上是我第一次参与这个项目。
- en: my first job。 They're a great organization to if you want to get back to community
    and do some data。 type work。 So if you've ever done data science you've probably
    seen these five stages of data science。 of the project or program。 You need to
    collect your data， you need to store your data。 you need to clean your data， prepare
    your data。 And after 80% of that work is done you can finally actually build your
    models and do the。
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我的第一份工作。他们是一个很棒的组织，如果你想回馈社区并进行一些数据相关的工作，推荐你参与。所以如果你曾经做过数据科学，你可能见过这五个数据科学阶段，项目或程序的。你需要收集数据，需要存储数据，需要清理数据，准备数据。在完成80%的工作后，你终于可以构建模型并进行分析。
- en: work you need to do and analyze it。 This is where this talk is focused on is
    the first few steps here in this process。 80% of my work is just in that first
    stage of collecting data。 Most of my responsibilities come from collecting large
    amount data on pipelines that run overnight。 I need to ensure that the data is
    collected and it's up to date so I can provide the rest。
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要做的工作并进行分析。这次演讲的重点就是这一过程中的前几步。我80%的工作都在于数据收集的第一阶段。我的大部分职责来自于收集大量在夜间运行的管道数据。我需要确保数据被收集且保持最新，以便能够提供其余数据。
- en: of my team and they can do the actual analysis on it。 So that's really where
    this talk is focused on and where I'm motivated to come from。 So what is web scraping？
    What is a distributed web scraping？
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我的团队可以对其进行实际分析。所以这正是这次演讲的重点，也是我来这里的动力。那么，什么是网络爬虫？什么是分布式网络爬虫？
- en: We need to talk about that and make sure on the same page。 Work scraping is
    the act of extracting data usually from a website。 You're taking some information
    from it， processing it and storing it。 When you move to distributed we're applying
    where we're spreading this work over large。
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要讨论一下，并确保在同一页面上。网络爬虫是从网站提取数据的行为。你从中获取一些信息，处理并存储它。当你转向分布式时，我们是在将这项工作分散到大规模。
- en: number of computers。 This typically means we're looking at large amounts of
    data。 Typically we're requesting around 25，000 pages at one time。 That's usually
    the volume that I work at。 So giving a quick brief before I jump into the journey
    of how I got there。 this is generally， what I would present to you as a mental
    model of a distributed web scraper looks like。
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机的数量。这通常意味着我们正在处理大量数据。通常我们一次请求大约25,000个页面。这通常是我工作的量。因此在我跳入我的旅程之前，给你一个简要概述，这通常是我呈现给你关于分布式网络爬虫的心理模型。
- en: You have your main controller， consumer， which generates the jobs， sends it
    to a bunch of。 different nodes。 They do their work and they send it back to the
    consumer which then stores and do the。 final processing of the data。 So to show
    you how I got there I'm going to walk through my journey of web scraping throughout。
    the last decade。 Just at the stage， like I said， you're looking at 25，000 pages。
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 你有你的主控制器，消费者，生成作业，将其发送到一堆不同的节点。它们完成工作后将结果发送回消费者，消费者随后存储并进行最终数据处理。为了展示我如何到达这里，我将回顾我在过去十年中的网络爬虫之旅。就像我说的，你在处理25,000个页面。
- en: So the intent is you're using a situation where you want to collect a lot of
    data source。 where the layout is going to be the same。 It's going to be the same
    website。 You want to do it as quickly and efficiently as possible and ensure you
    have the fidelity。 of the data。 So for example， say you want to make a good reads
    competitor and you need book data。
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 所以意图是，你在一个你想要收集大量数据源的情况下，布局将是相同的。这将是同一个网站。你希望尽可能快速高效地完成，并确保数据的准确性。例如，假设你想要做一个
    Goodreads 的竞争者，你需要书籍数据。
- en: your， intent would be to scrape book data off of Amazon。 If you're going to
    be requesting a whole bunch of book information from their individual， pages。
    you're going to be working with the same templates and you're going to be extracting。
    the same information， page numbers， prices， titles， authors， et cetera。
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 你的意图是从亚马逊抓取书籍数据。如果你要从他们各自的页面请求大量书籍信息，你将使用相同的模板，并提取相同的信息，如页码、价格、标题、作者等。
- en: This is the place where a distributed web scraper would typically jump where
    you would。 want to use one。 So my first start at the very first place is what
    you've probably done if you've ever。 worked with the tutorial in web scraping
    and Python。 You create a simple script。 it requests the resource and then it stores
    the results。
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个分布式网络爬虫通常会跳入的地方，你希望使用一个。所以我第一次尝试的地方是，如果你曾经和 Python 中的网络爬虫教程一起工作过，你可能已经做过的。你创建一个简单的脚本，请求资源，然后存储结果。
- en: '![](img/d0fd66aaacea9ae07aa87bfadc637056_3.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d0fd66aaacea9ae07aa87bfadc637056_3.png)'
- en: Python makes that really easy。
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: Python让这一切变得非常简单。
- en: '![](img/d0fd66aaacea9ae07aa87bfadc637056_5.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d0fd66aaacea9ae07aa87bfadc637056_5.png)'
- en: Python makes that really easy with the request library or URL lib or URL lib2。
    You can easily request websites。 You can then pass them to something like beautiful
    soup。 There's other alternatives like lxml。 They make it easy to process that
    page and store it。 You then have to decide how you want to store it， whether you
    want to store it to a CSV file。
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: Python通过请求库、URL lib或URL lib2让这一切变得非常简单。你可以轻松请求网站。然后将其传递给像Beautiful Soup这样的工具。还有其他替代品，如lxml。它们让你处理该页面并存储变得简单。然后你必须决定如何存储它，无论是存储到CSV文件中。
- en: you have a database， a text file or just output it to the screen。 So doing this
    once， super easy。 You can make a script， run it， get your data。 That's good。 So
    from here。 when I was working with that base case of collecting a whole bunch
    of books， from Amazon。 I didn't want to collect one book， I wanted to collect
    25，000 books。
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 你有一个数据库，一个文本文件或者只是输出到屏幕。所以做一次，这非常简单。你可以写一个脚本，运行它，获取数据。这很好。从这里开始，当我处理从亚马逊收集一大堆书籍的基本案例时。我不想只收集一本书，我想收集25,000本书。
- en: So I moved it to an iterative approach。 I now have two phases in my scraping
    job。 The first phase would be to collect all the pages I would need to scrape
    from that could。 be going through a search or an index and then I would need to
    scrape each of those individual。 pages。 This has done the exact same way as a
    single request but just looped over and over and over。
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我转向了迭代的方法。我现在在抓取任务中有两个阶段。第一个阶段是收集所有我需要抓取的页面，可以通过搜索或索引，然后我需要抓取每一个单独的页面。这与单个请求的方式相同，只是不断循环。
- en: and over and over， repeating until you're completed。 However， like I said。 once
    I started working with the volume of 25，000 pages， a number of， issues quickly
    arose。 This was very slow。 There was a lot of idling while you're waiting for
    your networks to just return the websites。 Say even if it's one second page times
    by 25，000， that's a lot of time。
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 一遍又一遍，重复直到完成。然而，正如我所说，一旦我开始处理25,000页的量，很多问题迅速出现。这非常缓慢。你在等待网络返回网站时会有很多空闲时间。即使是每页一秒，25,000页也是相当长的时间。
- en: The second issue was bot detection。 Websites would quickly detect me as being
    a bot。 If they see the same script requesting resources over and over and over
    and over and over really。 fast， it picks it up as being suspicious。 At this stage，
    I didn't have any tracking tools。 I didn't know if I was requesting websites twice。
    Same thing。 When things failed。
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个问题是机器人检测。网站会迅速将我识别为机器人。如果他们看到同一个脚本一遍又一遍请求资源，速度非常快，他们会觉得这很可疑。在这个阶段，我没有任何跟踪工具。我不知道我是否请求了同一个网站两次。同样的事情发生在失败时。
- en: which will happen a lot when you're working with networks， I had to。 start the
    script from scratch and you're basically starting at zero。 Of course。 as I've
    been working with this over the last decade， the internet's changed， too。 Around
    2010， 2012。 2014， you had a lot of pages that were static HTML。 Nowadays。
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 当你在处理网络时，这种情况会频繁发生，我不得不从头开始启动脚本，基本上是从零开始。当然，随着我在过去十年的工作，互联网也发生了变化。大约在2010年、2012年、2014年，很多页面是静态HTML。如今。
- en: you're using a lot more dynamic websites built up JavaScript， so if you're。
    using something like request， you're not going to get that dynamically generated
    HTML。 Then I had to address these issues with my next approach。 When this。 I introduced
    a number of different advanced improvements or intermediate improvements。
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 你使用了更多基于JavaScript构建的动态网站，因此如果你使用类似于请求的东西，你将无法获取动态生成的HTML。然后我必须在下一个方法中解决这些问题。当我引入了许多不同的高级改进或中间改进时。
- en: to help myself。 I'd introduce a middle proxy layer in order to help replicate
    my actions as if a human。 were making all these requests。 It would randomly pick
    a proxy at random。 feed it through that proxy， so every time the website。 would
    see the request coming from a different IP。 I started adding headers to my request。
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 为了帮助自己，我引入了中间代理层，以帮助模拟我的行为，就像人类在发出所有这些请求一样。它会随机选择一个代理，通过该代理发送请求，因此每次网站都会看到来自不同IP的请求。我开始在我的请求中添加头部。
- en: The base requests package in Python doesn't add any headers to it。 Website sees
    a request come in with no headers of what browser type it's looking at。 It often
    detects it as being a bot。 You're not just adding header， but adding random headers。
    Every time it looked like a different browser was going to website。 For example。
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: Python中的基础请求包不会添加任何头部信息。网站看到的请求没有浏览器类型的头部信息，通常会将其检测为机器人。你不仅要添加头部，还要添加随机的头部。每次看起来像是不同的浏览器访问网站。例如。
- en: all of us here at the conference going to the Python website were all probably。
    going to be on the same IP， but it's going to look as if a whole bunch of different
    browsers。 are viewing it because it is a whole bunch of different browsers for
    everyone's devices。 One thing I found helpful was adding delays between my requests。
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在会议上，所有访问Python网站的人可能都会在同一个IP上，但它看起来就像是一大堆不同的浏览器在查看，因为每个人的设备上都是一大堆不同的浏览器。我发现一个有用的做法是在请求之间添加延迟。
- en: One you don't want to hammer a website indeed awesome。 But you also want to
    add random delays。 so it's not a consistent amount of time between， every request。
    Dealing with the JavaScript issue。 I started moving off of a beautiful soup and
    request， and go to Selenium。 Selenium is typically used for testing， but it's
    great for dynamically generating content。
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 你确实不想猛烈攻击一个网站。但你也想添加随机延迟，这样在每个请求之间的时间就不会是一致的。在处理JavaScript问题时，我开始从Beautiful
    Soup和Requests转移到Selenium。Selenium通常用于测试，但它对动态生成内容非常有效。
- en: in the next track that each HTML。 I also added try until succeed methods。 Like
    I said。 networks are tricky。 They can often fail， and I'm sure you've gone to
    a website before and it didn't load。 but then you refresh and load it again。 Before
    in the previous iteration。 these would cause the scripts to fail。 So we started
    adding wrappers to my functions that would simply just keep retrying the request。
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个HTML中，我还添加了“尝试直到成功”的方法。正如我所说，网络是棘手的。它们经常会失败，我相信你之前访问过一个网站，它没有加载，但你刷新后又能加载。在之前的迭代中，这会导致脚本失败。因此，我们开始给我的函数添加包装器，简单地不断重试请求。
- en: until it's succeeded。 You only tried two or three times， so you don't want to
    loop forever。 And I started adding some basic tracking to my scripts themselves。
    So we know what URLs we need to collect from and what we've already succeeded
    in collecting。 Of course， this introduced more issues。 This was even slower。
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 直到它成功。你只尝试了两三次，所以你不想无限循环。我开始在我的脚本中添加一些基本的跟踪。因此，我们知道需要从哪些网址收集，以及我们已经成功收集了哪些。当然，这引入了更多的问题。这甚至更慢。
- en: Once you're slowing down your requests， you're going to add time。 Once you're
    dealing with dynamically generated JavaScript that has to load， you're going to。
    be even slower。 You don't have wasted resources。 When you have all those proxies
    sitting there doing nothing。 so for example， if you have， five proxies and you're
    rotating randomly， every time。
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你减缓请求的速度，你会增加时间。当你处理必须加载的动态生成的JavaScript时，你会变得更加缓慢。你没有浪费资源。当你有所有这些代理在那儿无所事事时。例如，如果你有五个代理，并且每次都随机轮换。
- en: 80% of them are doing nothing。 If you're spinning these up on， say， AWS or DigitalOcean。
    they're sitting there and， you're paying for resources to do nothing。 At this
    point。 my progress tracking and restarting was really weak。 There were still issues
    that popped up again which really inspired the distributed scraper。
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 其中80%的代理什么都不做。如果你在AWS或DigitalOcean上启动这些，它们就会无所事事，而你却在为这些无用资源付费。这时，我的进度跟踪和重启非常薄弱。仍然会出现一些问题，这激励了分布式抓取器的开发。
- en: This is around the time when I started thinking about distributed scraping。
    So the intent was to address all these previous issues that we've just discussed。
    I want to have multiples working on scraping jobs at the same time。 I still wanted
    to retain that two-phase approach in generating URLs and processing them。
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这大约是我开始考虑分布式抓取的时间。因此，意图是解决我们刚讨论的所有先前问题。我希望同时有多个工作在抓取任务。我仍然希望保留生成网址和处理它们的两阶段方法。
- en: And this is what came up to my mental model。 We'll break it down a bit more。
    but as I first introduced it， you have a controller which， generates your list
    of work。 It's going to feed those into a work queue。 Each year's scraping nodes
    can then start working off that queue。 They can individually scrape and then they'll
    return results back to the results queue which。
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我心理模型的构建方式。我们会更详细地分解，但我第一次介绍时，你有一个控制器，它生成你的工作列表。它将把这些工作输入到工作队列中。每年的抓取节点随后可以开始处理该队列。它们可以单独抓取，然后将结果返回到结果队列中。
- en: can then be consumed and saved and stored for how you need in the future。 I
    should add that there's five in this picture but you can have as many nodes as
    you want。 So breaking that down a bit more detailed， you have the first phase
    which is your controller。 The controller does the same work that I first mentioned
    in generating your list of URLs。
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 然后可以被消费并保存，以满足你未来的需求。我应该补充的是，这张图片中有五个，但你可以根据需要拥有任意数量的节点。因此，稍微详细分解一下，你有第一阶段，也就是控制器。控制器执行我最初提到的生成URL列表的相同工作。
- en: It goes through your search index or an index page and generates your list of
    URLs and stores。
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 它通过你的搜索索引或索引页面生成你的URL列表并存储。
- en: '![](img/d0fd66aaacea9ae07aa87bfadc637056_7.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d0fd66aaacea9ae07aa87bfadc637056_7.png)'
- en: into a work queue。
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 到工作队列中。
- en: '![](img/d0fd66aaacea9ae07aa87bfadc637056_9.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d0fd66aaacea9ae07aa87bfadc637056_9.png)'
- en: These work queues offer some cool tools such as ensuring no duplicates。 I'll
    talk about them a little bit more。 Once it's done collecting all the results。
    it actually publishes them to the work queue， so that way the nodes can start
    working。 The scraping node is basically a smaller version of the previous iteration
    of the scraper that。
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这些工作队列提供一些很酷的工具，例如确保没有重复。我会稍微多谈谈这些。一旦它完成收集所有结果，它实际上会将它们发布到工作队列中，这样节点就可以开始工作。抓取节点基本上是之前抓取器的一个小型版本。
- en: we talked about before。 It pulls a URL from that work queue and start scraping
    with the same techniques we've introduced。 before。 It requests the page， it parses
    the page and returns the results。 We're turning it back to the result queue for
    the final stage。 One cool side effect of introducing all these nodes with these
    queues is that you can add。
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前讨论过。它从工作队列中提取一个URL，并开始使用我们之前介绍的相同技术进行抓取。它请求页面，解析页面并返回结果。我们将其返回到结果队列以进行最终阶段。引入所有这些节点和队列的一个有趣的副作用是你可以添加。
- en: and destroy queues mid-job。 If you need to add more queues because you want
    to speed it up。 you can spin up more， nodes。 They can start pulling from the queue
    without any disruptions and start adding to the result。 queue without disruption。
    Same thing because of the cool things that queues can give you。 If you destroy
    a node， even if it's mid-job， you won't lose that progress and you'll be。
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在工作进行中销毁队列。如果你需要添加更多队列以加快速度，你可以启动更多节点。它们可以在没有任何中断的情况下开始从队列中提取，并在没有干扰的情况下开始添加到结果队列。因为队列可以给你带来一些很酷的功能，所以同样的事情。如果你销毁一个节点，即使在工作进行中，你也不会失去那个进度。
- en: able to re-request that resource。 So once the scraper is done， you send it back
    to the controller。 which doesn't have to be， in the same place as the first phase
    and it stores into result queue。 This queue keeps track of where you've progressed
    and allows you to keep track of your total。 progress。 So right away， this speeds
    up your scraping much faster。
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 能够重新请求该资源。所以一旦抓取器完成，你将其发送回控制器。控制器不必与第一阶段在同一位置，并将其存储到结果队列中。这个队列跟踪你已进展的情况，并允许你跟踪你的总体进度。因此，立刻，这加快了你的抓取速度。
- en: So when I first started doing this， I was working at the volume of about 15，000
    pages。 I always said 25 earlier， but five years ago， about 15，000 pages。 That
    would take me about 26 hours with the second iteration before the distributed
    scraper。 When I went up to a distributed scraper using about 8 or 10 nodes， that
    would reduce it。
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 所以当我第一次开始做这个时，我的工作量大约是15,000个页面。我之前总是说25，但五年前大约是15,000个页面。这需要我大约26小时，使用第二次迭代之前的分布式抓取器。当我使用大约8或10个节点的分布式抓取器时，这个时间就减少了。
- en: down to four or five hours。 That's still a lot of time because you're still
    looking at a lot of resources。 but something， you can run overnight without any
    issues。 You're now no longer wasting resources。 so instead of having proxies in
    there doing nothing。 you can actually employ all the proxies to do something to
    help you with overall job。
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 降低到四到五个小时。这仍然是很多时间，因为你仍然在查看大量资源。但是，有些东西可以在夜间运行而不会出现任何问题。你现在不再浪费资源。因此，替代在那儿无所事事的代理，你可以实际上雇用所有代理来做一些事情，帮助你完成整体工作。
- en: With the introduction of queues， we're improving our fidelity of the process。
    We're ensuring that all the work has been done and again， the results from all
    the resources。 And of course， you can automate the whole process。 If you employ
    a distributed scraper on a platform such as AWS， Azure， Lino， DigitalOcean， they。
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 随着队列的引入，我们正在提高流程的准确性。我们确保所有工作都已完成，并再次从所有资源中获得结果。当然，你可以自动化整个过程。如果你在AWS、Azure、Lino、DigitalOcean等平台上使用分布式抓取器，它们。
- en: all have their own APIs， so you can automate almost this entire process。 Of
    course。 the disadvantages with distributed scraper is now there's much higher
    cost。 Again。 these platforms are fairly cheap， so you're not talking large dollars，
    but there。 is still a cost to actually employ each of these nodes。
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 所有平台都有自己的API，因此你几乎可以自动化整个过程。当然，分布式抓取器的缺点是成本大幅增加。再次强调，这些平台相对便宜，所以你不需要花费大量资金，但实际上雇佣每个节点仍然会有成本。
- en: Read-a-play ability of code has been a bit tricky， but I have a few ideas on
    that and。 I'll talk about that in a minute。 And the other one is assumption of
    trust。 So you're assuming that this entire system is a closed system。 You assume
    that all your nodes are only talking with your queues and you're assuming that。
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 代码的可读性有些棘手，但我有一些想法，稍后会谈到。另一个是信任的假设。因此你假设整个系统是一个封闭系统。你假设所有节点仅与队列进行通信，并且你假设。
- en: no data is being introduced from another source that could either be malicious
    or just be not。 useful。 So that is generally a distributed scraper in general，
    but I want to talk about a few other。 more specific technical things on these
    different phases。 The first is that the introduction of queues made that distributed
    scraping job really easy。
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 没有数据被引入自另一个源，这可能是恶意的，或者根本没有用。因此，这通常就是分布式抓取器的整体情况，但我想谈谈这几个不同阶段中的一些更具体的技术内容。首先是引入队列使得分布式抓取工作变得非常简单。
- en: to manage。 Cues themselves are often useful dispatch methods， so you can ensure
    that the resources。 are being spread out across the different nodes， whether you
    want fair or round robin。 You get ACK methods， so you can guarantee that your
    resource has been sent to a scraping。 node and then you acknowledge and confirm
    that results have been sent back before deleting。
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 进行管理。队列本身通常是有用的调度方法，因此你可以确保资源在不同节点之间的分配，无论你是想要公平分配还是轮询分配。你可以获得ACK方法，以确保你的资源已被发送到抓取节点，然后在删除之前确认结果已被发送回来。
- en: anything。 So you have that acknowledgement before you to ensure that you've
    completed the job。 They can also be wrapped。 You're also sending messages across
    them。 so typically you're sending them as text。 So you can wrap them in JSON and
    make it easy to send across and use it with any type of。 platform or language
    or tool。 These message brokers， again， are really agnostic。
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 因此你需要确认你已经完成了工作。它们也可以被包装。你还在它们之间发送消息，因此通常你是以文本形式发送。你可以将它们包装在JSON中，这样便于发送，并可以与任何类型的平台、语言或工具一起使用。这些消息代理同样是非常通用的。
- en: so you're not stuck to any specific one。 I typically use RabbitMQ or Redis in
    my own work。 My day job is using Kafka a lot， but， again， you're not stuck in
    the so you can use what。 you're used to， whether it's AWS data streams， Azure
    vent hubs or SQL or no SQL databases。 Python offers a lot of really useful and
    easy packages for you to actually start working。
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，你不必被限制于任何特定的工具。我通常在自己的工作中使用RabbitMQ或Redis。我的日常工作是大量使用Kafka，但同样，你并不受限，因此可以使用你习惯的工具，无论是AWS数据流、Azure事件中心还是SQL或非SQL数据库。Python为你提供了许多非常有用且易于使用的包，让你可以实际开始工作。
- en: with these queues。 Pika and Puka are the ones that I've used most。 I haven't
    used Celery in this context， but I've used it for offloading jobs off websites。
    It's very easy to manage your queues。 And one thing I've really found tricky over
    the last few years was code management。 So as you employ your scraping code to
    each of the nodes， eventually something's going。
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这些队列。Pika和Puka是我使用最多的库。在这个上下文中我没有使用Celery，但我曾经用于从网站上卸载工作。管理队列非常简单。过去几年，我发现代码管理非常棘手。因此，当你将抓取代码应用到每个节点时，最终总会出现一些问题。
- en: to happen that's going to break it。 Either there'll be an exception that you
    didn't think of， say。 for example， you pulled， a birthday off of a website and
    they said that person was born on the 13th month。 the， 45th day and the year 1045。
    That can easily break your Python code if you don't have any exceptions to it。
    So， or for example， if resources doesn't exist or it throws weird errors or even
    commonly。
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 会发生某些事情导致程序崩溃。可能会出现你没有考虑到的异常，例如，你从网站上提取一个生日，结果它显示这个人出生在第13个月，第45天，年份1045。如果你没有对它进行异常处理，这可能会轻易破坏你的Python代码。或者，例如，如果资源不存在，或者抛出奇怪的错误，甚至是常见的错误。
- en: at the website just changes， the layout changes and your code no longer works，
    you need to。 manage that code onto the scraping node itself。 The couple options
    I've looked at over the years was containers which work。 I am not a fan of them
    personally in this context， partly because I'm not super comfortable。 with them，
    but also I find on the nodes themselves， for myself， they're often a bunch of
    servers。
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 当网站布局发生变化，你的代码不再有效时，你需要管理抓取节点上的代码。多年来，我考虑过的几个选项是容器，但在这个上下文中我个人并不喜欢它们，部分原因是我对此并不十分熟悉，但我发现对于我来说，节点上往往有一堆服务器。
- en: So I have a lot of bash scripts that I have outside of the container context
    that I also。 need to manage。 One thing I like is a VCS hook。 Every time you start
    up your scraping node。 it just pulls the latest version of the code， off of a
    repo。 I've looked into serverless computing such as AWS Lambda， but again， a lot
    of the time。
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我有很多在容器外部的 bash 脚本需要管理。我喜欢 VCS 钩子。每次启动抓取节点时，它会从仓库中拉取最新版本的代码。我研究过无服务器计算，比如 AWS
    Lambda，但实际上很多时候。
- en: in scraping is sitting there waiting for resources。 So when you're using that。
    you're paying for Lambda when you're not actually using it。 The other thing I'm
    using is manual deployment of images。 I'll create a node FTP into it。 update the
    code， shut down it， create a new image of it。
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在抓取中，等待资源的时间是个问题。所以当你使用它时，实际上并没有在使用 Lambda。另一件我使用的事是手动部署镜像。我会创建一个节点，FTP 进去，更新代码，关闭它，创建一个新镜像。
- en: and deploy all the nodes off that new image。 It's not ideal。 but it ends up
    being lazy and going to that approach。 So I do want to highlight a couple of useful
    Python packages that are useful in scraping。 both in the distributed scraper and
    in non-distributed scraping， request beautiful soups， lenium。
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 然后基于那个新镜像部署所有节点。这并不理想，但最终还是懒得去采取这种方法。所以我想强调几个在抓取中有用的 Python 包，既适用于分布式抓取，也适用于非分布式抓取，包括
    requests、Beautiful Soup 和 Selenium。
- en: or your go-tos。 If you've ever done a tutorial， you'll have seen them。 So I
    won't go too much into them。 Click is really awesome。 Comes in a lot of other
    packages so you might already have installed。 It makes it really easy for you
    to create command line arguments for your scripts。 Retry back off。
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 或者你的常用工具。如果你做过教程，你肯定见过它们。我就不详细讲了。Click 非常棒，包含很多其他包，你可能已经安装了。它让你为脚本创建命令行参数变得非常简单。Retry
    和 backoff。
- en: Retry and back off retry are two different packages that make it easy。 They're
    essentially function wrappers that handle the retrying of failed functions。 So
    for example。 if you request a website and it gives you a 404 error because it
    doesn't， exist。 it would retry it and you can give a number of parameters such
    as the number of。
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: Retry 和 backoff retry 是两个不同的包，使得这一过程变得简单。它们本质上是处理失败函数重试的函数包装器。例如，如果你请求一个网站，它返回
    404 错误因为它不存在，它会重试，你可以设置多个参数，例如重试次数。
- en: retimes you want to try before you give up。 You can add back off so you exponentially
    slow down your request between every time。 So that's a really useful package。
    Request cache is a really great one for development。 Request is the package to
    request a resource on website， but request cache actually caches。 that result。
    So every time when you're developing and you're testing and you're building a
    script for。
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 你想在放弃之前尝试多次。你可以添加 backoff，这样在每次请求之间会指数级减慢速度。所以这是一个非常有用的包。requests-cache 是一个非常适合开发的包。requests
    是用于请求网站资源的包，但 requests-cache 实际上会缓存这个结果。因此每次你开发、测试和构建脚本时。
- en: the first time， you don't have to re-grab it new every single request。 You can
    just grab it at your cache and it helps you up your development time。 And fake
    user agent was a really good one that I found recently。 I learned about it from
    one of the Python podcasts， I just can't remember which one。 But it。
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 第一次，你不必每个请求都重新获取。你可以直接从缓存中获取，这样可以提高开发时间。而伪造用户代理是我最近发现的一个非常好的工具。我是从一个 Python
    播客了解到的，只是记不起是哪一个。但它。
- en: so what I mentioned earlier before introducing random headers for your requests。
    it uses a real world database and it gives you a random header that's based on
    distribution。 of what they actually see in real life。 So instead of just adding
    a random header。 you're adding something that you're likely， to see or the server
    is likely to see in real time。
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我之前提到的在请求中引入随机头部的内容，使用了一个现实世界的数据库，它为你提供一个基于他们在现实生活中实际看到的分布的随机头部。所以与其仅仅添加一个随机头部，不如添加一个你很可能会看到的，或者服务器在实时中很可能会看到的东西。
- en: So just some other considerations， partly for myself， as far as you think about，
    you definitely。 want to add。 Full automation， I haven't fully gotten there yet
    to fully creating this。 partly because， I'm lazy and busy just managing these
    servers。 processes and getting my team working。 But you can fully automate them。
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 所以还有一些其他考虑，部分是为了我自己，考虑到你绝对想要添加。全面自动化，我还没有完全做到这一点，部分原因是我懒得去管理这些服务器的流程和让我的团队工作。但你可以完全自动化这些。
- en: something I want to investigate more into。 Alerts are something that you definitely
    need on all of your web scraping scripts。 whether， it's an SMS or an email。 You
    need to， it makes your life so much easier if you can be aware as soon as it happens。
    if there's a failure。 So for example， there's a network issue or you got a 503
    error because you got banned。 or because the website no longer exists or the page
    doesn't exist or the layout changed。
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我想更深入调查的一个点是。警报是你在所有网页抓取脚本中绝对需要的，无论是短信还是电子邮件。如果你能在发生故障时立即得到通知，会让你的生活轻松很多。例如，网络问题或者你因为被禁止而收到503错误，或者网站不再存在，或者页面不存在，或者布局发生了变化。
- en: And also for completions and success， you also want to know when it's succeeded
    so you。 can actually go back and continue the next phase of whatever you're working
    on。 And of course。 all the previous things I've talked about still need to be
    addressed。 So I kind of went a little bit quicker than I hope， but yeah， same
    conclusions。
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 而且对于完成和成功，你还想知道何时成功，这样你才能回过头来继续进行你正在进行的下一阶段。当然，我之前提到的所有内容仍然需要解决。所以我有点比我希望的快，但结论是一样的。
- en: Python itself makes scraping really easy。 Large jobs require a lot of work to
    ensure the fidelity and ensure that you can actually。 scrape all the resources，
    especially in a reasonable amount of time。 Distributed web scraping is a approach
    you can use to improve your fidelity and speed。 up the time to actually request
    all these resources。 However。
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: Python本身使得抓取变得非常简单。大型任务需要大量工作以确保保真度，并确保你能实际抓取所有资源，尤其是在合理的时间内。分布式网页抓取是一种可以用来提高保真度和加快实际请求所有这些资源时间的方法。然而。
- en: it does come at the cost of a higher actual resource cost。 So thank you for
    that。 I don't believe we're doing Q&A， but I'm here all weekend。 So if you ever
    see me this weekend。 just come up and chat。 So thank you。 [APPLAUSE]， [BLANK_AUDIO]，
    [ Silence ]。
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 这确实需要付出更高的实际资源成本。所以谢谢你。我不认为我们要进行问答，但我这周末都会在这里。所以如果你这周末看到我，就来聊聊吧。谢谢。[掌声]，[空白音频]，[沉默]。
- en: '![](img/d0fd66aaacea9ae07aa87bfadc637056_11.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d0fd66aaacea9ae07aa87bfadc637056_11.png)'
