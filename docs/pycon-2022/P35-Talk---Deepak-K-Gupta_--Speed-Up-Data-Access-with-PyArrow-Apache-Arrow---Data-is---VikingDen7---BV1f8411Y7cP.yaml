- en: P35：Talk - Deepak K Gupta_  Speed Up Data Access with PyArrow Apache Arrow   Data
    is - VikingDen7 - BV1f8411Y7cP
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: P35：演讲 - Deepak K Gupta_ 加速数据访问与 PyArrow Apache Arrow 数据是 - VikingDen7 - BV1f8411Y7cP
- en: Good afternoon everyone。 Now the next talk is about speeding up data access
    with PyArrow。
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 大家下午好。接下来的演讲是关于加速数据访问与 PyArrow。
- en: '![](img/1bb5d948cb3320391d7315a340e72f5b_1.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1bb5d948cb3320391d7315a340e72f5b_1.png)'
- en: '![](img/1bb5d948cb3320391d7315a340e72f5b_2.png)'
  id: totrans-3
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1bb5d948cb3320391d7315a340e72f5b_2.png)'
- en: The speaker is Deepakutta。 As a reminder， we're not taking questions at the
    end of the talk。 You can reach out to the speaker after the talk。 Thank you。 1，
    2， 3， ABC。 Hello everyone。 Good afternoon。 Thank you for joining this talk。 Where
    we are going to talk about speeding up data access with Apache arrow。 the Python
    library， PyArrow。 And what is the meaning of data is the new API？ So the golden
    question。
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 演讲者是 Deepakutta。提醒一下，演讲结束后我们不接受提问。你可以在演讲结束后与演讲者联系。谢谢。1，2，3，ABC。大家好，下午好。感谢大家参加这次演讲。我们将讨论如何加速数据访问与
    Apache Arrow，Python 库 PyArrow。数据是新的 API 的意思是什么？这是一个重要的问题。
- en: all of you can see my screen。 Right？ Okay。 So this is the third time I am presenting
    in， PyCon US。 I thank PyCon staff and members for this opportunity。 Before I go
    ahead some， introduction about me。 my name is Deepakkigupta。 I'm also known by
    the name of Dutch。 I am a tech。 consultant and CTO to a company called Hotel Hub，
    which looks after business travel logistics and。
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 大家可以看到我的屏幕，对吗？好的。这是我在 PyCon US 的第三次演讲。感谢 PyCon 的工作人员和成员给我这个机会。在继续之前，先简单介绍一下我自己。我叫
    Deepakkigupta，大家也可以叫我 Dutch。我是一名技术顾问，也是名为 Hotel Hub 的公司的首席技术官，该公司专注于商务旅行物流。
- en: based out of India， UK and France。 Since we are not going to take any question。
    I have come up with a， trick。 My Twitter DM will be open for today。 If you have
    any question。 you can just， you know， write the question without following。 Okay。
    So let's start。 I believe all of you have heard， about this term called data is
    new oil。 Right？ Yes， it is。
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 我在印度、英国和法国工作。因为我们不会接受任何问题，我想了一个办法。我的 Twitter 私信今天会开放。如果你有任何问题，可以随意写问题，无需关注。好的，让我们开始。我相信大家都听说过“数据是新的石油”这个词，对吧？是的，确实如此。
- en: Data is the new oil， but if you cannot， access the data when it needed。 you
    cannot extract the information from the data。 This oil will not， be as valuable
    as it is there。 Okay。 So since we are talking about data， we need to talk about。
    some of the very basic aspects of software development， which is what is a software
    program made of？
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 数据是新的石油，但如果你无法在需要时访问数据，你就无法从数据中提取信息。这种石油的价值就不会像它所蕴含的那样。好的。既然我们在讨论数据，就需要谈谈软件开发的一些基本方面，也就是软件程序是由什么构成的？
- en: Irrespective whether you are writing programming， irrespective of whether you
    are writing coding。 any programming language， whether you are creating front end
    or back end， any program you write。 each and every software program consists of
    only two things。 These two things are data and。 functions which acts on that data。
    There is nothing else in a software program。 Unfortunately。
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 不论你是编写程序，无论你是在写代码，任何编程语言，无论你是在创建前端还是后端，任何你写的程序。每一个软件程序仅由两个部分组成。这两部分是数据和作用于这些数据的函数。软件程序中没有其他东西。不幸的是。
- en: in last four to five decades， whenever we are learning programming， we are focusing。
    more on functions， not data。 Remember， main， print， hello world。 This is a function
    and whenever we。 are talking about data， we just create one class， few variables
    and we learn complete programming。 language without understanding how to take
    care of large amount of data within the program。
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去的四到五十年里，每当我们学习编程时，我们更多地关注函数，而不是数据。记住，main，print，hello world。这是一个函数，每当我们谈论数据时，我们只是创建一个类，几个变量，就这样学习完整的编程语言，而没有理解如何在程序中处理大量数据。
- en: Remember， within the program and this is what we do in our software today。 Fortunately。
    things have changed， and we are focusing more on data today and to all the programmers
    out there and the next generation。 programmers out there， think about data as
    much as you are thinking about functions。 With this。 I am going to talk about
    Apache Arrow。 Apache Arrow is not a framework or library。 It's。
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，在程序内部，这就是我们今天软件中的工作。幸运的是，事情发生了变化，我们今天更关注数据，对所有程序员和下一代程序员来说，想想数据，和你思考函数时一样。基于此，我将讨论
    Apache Arrow。Apache Arrow 不是一个框架或库。它是。
- en: a specification。 It's a specification for representing data in memory in a particular
    format which is。 called columnar format。 And it is language independent but since
    it is a specification， it。 doesn't mean that we need to implement that specification。
    It has libraries in 12 language。 either direct implementation or binding which
    we can use to use this particular Apache Arrow。
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 一个规范。这是一个用于以特定格式在内存中表示数据的规范，称为列式格式。它是语言无关的，但既然这是一个规范，并不意味着我们需要实现这个规范。它在12种语言中有库，或者直接实现或绑定，我们可以使用这个Apache
    Arrow。
- en: specification。 Pi Arrow is a binding which we can use and we are going to see
    some of the。 examples of that。 So， anyway， we are talking about data and all of
    you must have seen these。 kind of pictures where you are querying things with
    REST APIs or even with GraphQL。 So， when you。 say that REST API give me employee
    information or PyCon attendees information， how do you know。
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 规范。Pi Arrow是我们可以使用的绑定，我们将看到一些例子。所以，无论如何，我们在讨论数据，大家一定见过这些。图片，您在用REST API或甚至GraphQL查询东西。所以，当您说REST
    API给我员工信息或PyCon参与者信息时，您怎么知道。
- en: that the information which is coming is actually the same？ It's actually the，
    you know， attend。 the information or employee information what you are asking。
    There is no way to know but we trust。 that since I have called this particular
    API， this API will give me what I have asked for。 Okay。 So。 this is the way data
    communication happens but the problem is that if there are。
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 传来的信息实际上是相同的吗？实际上就是你知道的，参加者的信息，或者说是员工信息。我们无法得知，但我们信任。既然我调用了这个特定的API，这个API会给我我所请求的内容。好的。所以，数据通信就是这样发生的，但问题是如果有。
- en: two different entities talking to each other at the different ends， you are
    seeing couple of codes。 snippets over here。 One is in Java， one is in Python。
    Both are binary compatible but still if。 we need to talk to each other， they need
    to go ahead with something called serialization and。 deserialization。 Okay。 So，
    that is one cost that needs to be paid even if the data is， you know。
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 两个不同的实体在不同的端之间相互交流，您看到这里有几段代码。一个是Java，一个是Python。两者都是二进制兼容的，但如果我们需要彼此沟通，它们必须进行所谓的序列化和反序列化。好的。这是一个即使数据是，您知道的，仍需支付的成本。
- en: binary compatible。 And this problem is not only between two different programming
    language systems。 This is also between your program and databases。 When you fetch
    data from a database or you send。 data to a database， this problem of serialization
    and deserialization is there and it's real。 Okay。 So， I stumbled across one of
    a very nice paper online written by Marx and Hens。 It says that。
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 二进制兼容。而这个问题不仅存在于两种不同的编程语言系统之间。这也是在您的程序和数据库之间。当您从数据库获取数据或将数据发送到数据库时，序列化和反序列化的问题是现实存在的。好的。所以，我在网上偶然发现了由Marx和Hens撰写的一篇非常好的论文。它说。
- en: don't hold my data hostage。 And this is the picture in second picture in that
    particular paper。 You can， just Google it online。 I do not believe that anyone
    here can dispute this picture。 There might be， some disagreement with， you know，
    what is the exact time that it will take。 you know， serialization， will be less
    or deserialization will be more but there can't be any dispute on the output of
    this。
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 不要把我的数据扣为人质。这是那篇论文中第二张图片。你可以在网上搜索一下。我不相信在座的任何人能反驳这张图片。可能会有一些关于，您知道的，所需时间的分歧。比如说，序列化会少，还是反序列化会多，但对于这个输出不会有任何争议。
- en: particular picture。 So， how to solve this problem？ One way of solving this problem
    is to do what we。 call it as data is the new API。 So， what is the meaning of data
    is the new API？ The meaning of data。 is the new API is that， you know， when I
    am getting some data， if it is possible for data。 for itself to tell me that，
    okay， what am I？ So， I am not sure I am expecting a data but I am not。
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 特定的图片。那么，如何解决这个问题？解决这个问题的一种方法是我们称之为数据就是新的API。那么，数据就是新的API是什么意思呢？数据就是新的API的意思是，当我获取某些数据时，如果数据能够告诉我“好吧，我是什么”，那就太好了。我不确定我期待的数据，但我不是。
- en: sure which data it is coming， whether it is implied data， identity data or some
    other data。 But if data can itself tell me that， okay， I am employed data and
    these are the places where I am。 and you can directly use mean， that is the idea
    behind data is the new API。 And this is one of the。 ways to remove what we call
    it as the cost associated with serialization and deserialization。
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 确保知道数据的来源，无论是隐式数据、身份数据还是其他数据。如果数据能够告诉我，“好的，我是被雇佣的数据，这些是我的位置”，你就可以直接使用这个信息。这就是数据作为新
    API 背后的理念。这是消除我们称之为序列化和反序列化成本的一种方式。
- en: And this is where the concept of the in-memory columnar representation of Apache
    arrow comes into。 picture。 So， I have created this picture to make you understand
    that if things are speaking same。 language， it is very similar to， you know， I
    am speaking English， you are listening in English。 But if this is not the case，
    there will be some interpreter in between and the communication will。
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是 Apache Arrow 的内存列表示概念所涉及的地方。因此，我创建了这个图示来帮助你理解，如果事情在使用相同的语言，这非常类似于，我在说英语，而你在听英语。但如果情况不是这样，中间就会有某个解释者，沟通将会受到影响。
- en: slow because of that interpreter， okay。 The same thing happens with serialization
    and deserialization。 So， if you are using Apache arrow compliant entities on both
    side， they do not need to serialize。 and deserialize because both understand the
    format of memory data that is coming。 Now。 if you are interested in Apache arrow
    or Pi arrow and if you want to try it out。
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 由于解释器的原因，这个过程会很慢。序列化和反序列化也会出现同样的问题。因此，如果你在两边都使用兼容 Apache Arrow 的实体，它们就不需要进行序列化和反序列化，因为两者都理解传输的数据内存格式。如果你对
    Apache Arrow 或 Pi Arrow 感兴趣，想要尝试一下。
- en: this is a homework for all of you。 If you want to try it out， this is the first
    thing you should do。 Take up Apache Spark and Pandas right now without Apache
    arrow， if you want to take。 Spark data frame to a Pandas data frame， you need
    to go through complete conversion of a Spark。 data frame to Pandas data frame
    from JVM to Python memory。 You can enable Apache arrow in。
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这是留给你们的作业。如果你想尝试一下，这就是你应该做的第一件事。现在立即使用 Apache Spark 和 Pandas，而不使用 Apache Arrow。如果你想将
    Spark 数据框转换为 Pandas 数据框，你需要将 Spark 数据框从 JVM 完整转换为 Pandas 数据框，进入 Python 内存。你可以启用
    Apache Arrow。
- en: Spark with simple parameter and just try it out。 This will give you good enough
    indication of what。 Apache arrow or Pi arrow can do for you or whether it is worth
    for you to invest some time in it。 You can get some open source code for this
    but this is something I would like you to try it out。 first if you are interested
    in this。 That is why I have shown Apache Spark and Pandas。 So。
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 使用简单的参数进行 Spark 测试。这将给你一个良好的指示，了解 Apache Arrow 或 Pi Arrow 可以为你做什么，或者是否值得你在这方面投入一些时间。你可以找到一些开源代码，但这是我希望你们首先尝试的。如果你对此感兴趣，这就是为什么我展示了
    Apache Spark 和 Pandas。
- en: in general what Apache arrow is all about。 So， it says that no matter whether
    your data is。 in some blob storage or in some database， when you want to access
    that data， you put that data in。 memory using the in memory columnal format of
    Apache arrow using Pi arrow or for other programming。 language using their respective
    libraries。 And you can do lots of things what you could do with。
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，Apache Arrow 是关于什么的。它指出，无论你的数据是在某个二进制大对象存储中，还是在某个数据库中，当你想访问这些数据时，你可以使用
    Apache Arrow 的内存列格式，通过 Pi Arrow 或其他编程语言使用各自的库，将数据放入内存中。你可以在 Apache Arrow 的内存中做很多事情，这些事情是你可以用
    Pandas 或 R 完成的。
- en: Pandas or R in Apache arrow in memory itself。 But still if you cannot do that，
    you can take。 the Apache arrow in memory directly to Pandas， NumPy， R or in case
    of other languages in their。 respective data structures。 You can take this directly
    and the biggest thing is that it is。 possible to have a zero copy access to the
    memory that is there with Apache arrow to Pandas。
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 但是如果你无法做到这一点，你可以直接将 Apache Arrow 内存数据传递给 Pandas、NumPy、R，或者在其他语言中传递到它们各自的数据结构中。你可以直接使用这些，最重要的是，Apache
    Arrow 允许对内存进行零拷贝访问，这使得与 Pandas 的集成变得更加高效。
- en: NumPy or， R。 I have written it limited because there are some limitations that
    it can happen。 Zero copy， happen only for purely numeric values and the values
    which are not null but it is possible to。 do a zero copy。 So， you can have a Pandas
    data frame without occupying even a byte in memory。 As well as you can create
    memory mapped files for the format that is there in the Apache arrow。
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: NumPy或者R。我这样写是有限的，因为它可能会有一些限制。零复制仅适用于纯数字值和非空值，但确实可以做到零复制。所以，你可以拥有一个Pandas数据框，而不占用内存中的一个字节。同时，你可以为Apache
    Arrow格式创建内存映射文件。
- en: And you can save that file and use it across processes in a memory mapped way。
    So。 this is the complete idea。 The idea is that we should get rid of serialization
    and。 deserialization as much as possible。 And to do that we need to make sure
    that data itself is。 capable of telling that what this data is all about。 And
    Apache arrow is a specification。
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以保存那个文件，并以内存映射的方式在不同进程中使用它。所以，这就是完整的概念。这个想法是我们应该尽量摆脱序列化和反序列化。为了做到这一点，我们需要确保数据本身能够说明这些数据的内容。而Apache
    Arrow就是一个规范。
- en: Pi arrow is a Python library and these are the things you can do with it。 There
    is a similar picture that is there in Apache arrow website which talks about things
    in a similar。 way that you don't do copy and convert because it's a very costly
    operation。 And in case if you。 are worried about that dark blue color and light
    blue color it is all about whatever is in the。
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: Py Arrow是一个Python库，你可以用它做这些事情。Apache Arrow网站上有一幅类似的图片，以类似的方式说明了你不应该进行复制和转换，因为这是一项非常耗费资源的操作。如果你担心深蓝色和浅蓝色，它们分别代表磁盘中的内容和你希望在内存中进行的操作。
- en: disk and whatever you want to do in memory。 So， it is all about whenever you
    are taking your data。 in memory to do something to do some computation to get
    some data。 If you can do it in the arrow format， you don't need to copy and convert。
    That's one part。 Let's talk about this Apache arrow， columnar in memory format。
    So。
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，这一切都与当你将数据放入内存中进行某些计算时有关。如果你能以Arrow格式处理数据，就不需要复制和转换。这是其中的一部分。让我们谈谈Apache
    Arrow的列式内存格式。
- en: in a table we have rows of data even for Python attendees we will have， rows
    of data。 Traditionally this is being stored as row wise like one row is stored
    in memory。 the next row is stored in memory and third and fourth row is stored
    in memory。 But the Apache arrow。 columnar representation said that we have to
    store data in a column format。 Why？ Because during。
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个表中，我们有数据行，即使对于Python用户，我们也会有数据行。传统上，这些数据是以行的方式存储的，比如第一行存储在内存中，第二行存储在内存中，第三行和第四行也如此。但是Apache
    Arrow的列式表示法则说我们必须以列的格式存储数据。为什么？因为在处理过程中。
- en: in-memory computations vectorization can happen easily。 The SIMD of the CPU
    architecture can help。 fetching lots of data in the easiest possible way and faster
    way。 That's one of the reason why this。 is fast。 So， this is the columnar format，
    columnar way of representing data in memory。 Now you might， be having a question
    stating that you know what？
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在内存计算中，向量化可以轻松实现。CPU架构的SIMD可以帮助以最简单和最快的方式提取大量数据。这就是它快速的原因之一。所以，这就是列式格式，以列的方式在内存中表示数据。现在你可能会问，你知道吗？
- en: I understand it but it looks very similar to you， know optimized row column
    or parquet files。 Yes。 you are right but the difference is Apache arrow is。 in
    memory and directly computable which means that you can compute on Apache arrow
    in memory data。 directly which you cannot do in parquet file or as your for that
    matter any other database。
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我理解，但它看起来很像你知道的优化行列或Parquet文件。是的，你说得对，但区别在于Apache Arrow是在内存中并且可以直接计算，这意味着你可以直接在Apache
    Arrow内存数据上进行计算，而这在Parquet文件或任何其他数据库中是无法做到的。
- en: You need to extract data to compute something and when we are talking about
    computation I would。 like to show you one example of how the you know py arrow
    computation looks like but before that。 let me tell you the kind of computation
    functions that are available with Apache arrow which you。 can use with py arrow。
    There are vector functions where you can take lots of data and do some。
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要提取数据来计算某些东西，当我们谈论计算时，我想给你展示一个py arrow计算的示例，但在那之前，让我告诉你Apache Arrow中与py arrow一起使用的计算函数类型。有向量函数，你可以处理大量数据并进行某些操作。
- en: calculation。 There are scalar function where you can you know take a single
    value and do some。 calculation as well as aggregate functions which are similar
    to reduce where you can take lots of。 vectorized data and come up with a result
    for example sum of elements。 So， here is one example。 which I want to show you
    about using compute。 In line number one I have created a numpy array。
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 计算。有标量函数，你可以知道取一个单一的值并进行一些计算，以及类似于reduce的聚合函数，你可以取大量的向量化数据并得出一个结果，例如元素的总和。所以，这里有一个例子，我想向你展示关于使用计算。在第一行中，我创建了一个numpy数组。
- en: In the line number two I have created a py arrow array from the numpy array
    and I am using py。 arrow compute function to get min and max of the array。 In
    the line number two since it is purely。 numeric data it is zero copy。 It can be
    zero copy in the reverse way also just like I have shown in。 the pictures a few
    minutes back。 If you are having a apache you want to take a numpy it could be
    zero。
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二行中，我从numpy数组创建了一个py arrow数组，并使用py arrow计算函数来获取数组的最小值和最大值。在第二行中，由于它是纯数值数据，所以是零拷贝。它也可以以相反的方式进行零拷贝，就像我几分钟前在图片中展示的那样。如果你正在使用Apache，想要读取numpy，它可以是零拷贝。
- en: copy you want to take a pandas it could be zero copy provided some conditions
    are met。 So。 let us talk about the main aspect of this talk is speeding up data
    access because I said。 data is new oil but unless until you can access the data
    when you need it the oil is not as valuable。 as it should be。 So， I am going to
    show you three examples and how you can speed up data access and。
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想要进行pandas拷贝，前提是满足一些条件，它可以是零拷贝。所以，让我们谈谈这个演讲的主要方面，即加速数据访问，因为我说过数据是新石油，但除非你可以在需要时访问数据，否则这石油就没有那么有价值。所以，我将向你展示三个例子，以及如何加速数据访问。
- en: how py arrow comes into picture。 The first example is trivial of reading a CSV
    file。 So。 the file which I am reading seems to be a very famous file it is a yellow
    taxi trip data。 file January 2021 file which is approximately 126 MB of size。
    So， in line number one I am reading。 the CSV file using pandas。 Line number two
    I am reading the same CSV file using py arrow。
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: py arrow是如何介入的。第一个例子是读取CSV文件的一个简单示例。所以，我正在读取的文件似乎是一个非常著名的文件，它是2021年1月的黄色出租车行程数据文件，大小约为126
    MB。所以，在第一行中，我使用pandas读取CSV文件。在第二行中，我使用py arrow读取同样的CSV文件。
- en: Line number three I am converting py arrow to pandas remember that picture we
    can do that。 and line number four is all about making sure that both data frames
    are same。 Look at the wall timings。 Of course these timings are from my system
    but there is no reason for these timings not to。 proportionate in any system you
    run。 So， in line number one you get read CSV which is wall time 1。7。
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 第三行我正在将py arrow转换为pandas，记住那个图片，我们可以这样做，而第四行则完全是确保两个数据框是相同的。看看墙上的时间。当然，这些时间来自我的系统，但没有理由这些时间在你运行的任何系统中不成比例。所以，在第一行中，你得到读取CSV的墙上时间为1.7秒。
- en: seconds huge。 The same CSV file you can read using py arrow in 129 millisecond
    and you can convert。 that py arrow to pandas in 65 millisecond。 So， if you sum
    up second and third it is around 200。 millisecond few numbers here and there which
    is still faster than 1。7 seconds it takes。 But again I want to remind you that
    you can convert arrows to pandas but you still have to。
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 秒数巨大。使用py arrow读取同一个CSV文件只需129毫秒，而将py arrow转换为pandas仅需65毫秒。所以，如果你把第二个和第三个相加，大约是200毫秒，这里有一些数字的变化，但仍然比1.7秒快。不过，我想再次提醒你，虽然你可以将arrow转换为pandas，但你仍然必须。
- en: make sure that there are numerical values and there are some values like date
    and time which is。 having a different way of representing in arrow tables。 So。
    this is what you can do in approximately， 200 second and again the output of fourth
    is true both data frames are same。 So， this is about reading a CSV file you can
    read CSV file fast even if you consider converting it。
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 确保其中有数值，并且有一些像日期和时间这样的值在arrow表中有不同的表示方式。所以，这大约是你可以在200秒内完成的，再次验证第四个输出，两个数据框是相同的。所以，这就是关于读取CSV文件的内容，你可以快速读取CSV文件，即使考虑到转换。
- en: to data frame at later point of time。 Second thing is reading a parquet file。
    So。 for parquet files and this apache arrow this is you know kind of match made
    in heaven both are。 columnar representation but there are some compression thing
    that happens with parquet file。 and when arrow loads it needs to decompress that。
    So， here is the wall time between these two。 So。
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在稍后的时间将数据框架（data frame）转化。第二件事是读取一个parquet文件。因此，对于parquet文件和Apache Arrow，这是一种天作之合，它们都是列式表示，但parquet文件会有一些压缩的处理，当Arrow加载时需要解压缩。因此，这里是这两者之间的总耗时。
- en: if you read it through pandas it is 600 millisecond approximately and if you
    read it through。 py arrow it is approximately 100 millisecond。 And the most important
    example which I am going to。 show you now is using memory mapped file。 So， in
    pandas write what happens that if you have a。 16 gigs of RAM you will make sure
    that your pandas memory does not go beyond 8 or something 10 good。
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 如果通过pandas读取，时间大约为600毫秒，而如果通过PyArrow读取，则大约为100毫秒。最重要的例子我现在要给你展示的是使用内存映射文件。因此，在pandas中写入时，如果你有16GB的RAM，你会确保你的pandas内存不会超过8GB或10GB左右。
- en: will be 5。 But with this particular memory mapped file which you can create
    for apache arrow you。 can read you know 50 gigs of data frame using pandas。 Now
    there are some limitation with a memory。 mapped file what I would like to do to
    get the best possible performance is that the。 I write a memory mapped file or
    apache arrow file directly onto the this and then I do a memory mapping。
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 将是5。但使用这个特定的内存映射文件，你可以为Apache Arrow创建，并使用pandas读取50GB的数据框。现在，内存映射文件有一些限制，我希望为了获得最佳性能，直接将内存映射文件或Apache
    Arrow文件写入，然后进行内存映射。
- en: So， I have converted the same yellow taxi trip data into a arrow file。 So， I
    have taken that。 into a memory and just written that raw file。 So， line number
    one I have created a memory mapped。 file which means no memory is allocated。 Line
    number two I am reading the complete 50 gigs。 of data if it is 50 gigs read all
    columns no memory is allocated。 Okay。
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我将同样的黄色出租车行程数据转换成了一个Arrow文件。所以，我将其放入内存中，并且只写入了原始文件。因此，第一行我创建了一个内存映射文件，这意味着没有内存被分配。第二行我正在读取完整的50GB数据，如果是50GB就读取所有列，没有内存被分配。好的。
- en: Line number three I am reading， just one column of this 50 gig data it may be
    5 gig data。 So。 no memory is allocated and I can， run some computations on that。
    The interesting thing is line number four。 Okay。 We can convert。 table to pandas
    all 50 gigs if it is compatible with arrow format which means it is numerical
    data。
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 第三行我正在读取这50GB数据中的一个列，它可能是5GB的数据。因此，没有内存被分配，我可以对其进行一些计算。有趣的是第四行。好的，我们可以将表转换为pandas，如果它与Arrow格式兼容，这意味着它是数值数据。
- en: there is no null data we can create a pandas data frame without allocating any
    memory for you know。 50 gigs of data。 But if that is not the case and you want
    to do some calculations on some of the。 columns of the data set you can take only
    those columns and put that into the data frame。 What will。 happen is that even
    if the memory is allocated with pandas data frame the memory will be limited。
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 没有空值数据，我们可以在不为50GB数据分配任何内存的情况下创建一个pandas数据框。但如果不是这种情况，而你想对数据集的某些列进行一些计算，你可以只提取那些列并将其放入数据框中。发生的情况是，即使为pandas数据框分配了内存，内存也会被限制。
- en: in nature。 So， for a 50 gig file if you want to calculate only for column trip
    distance and that。 column is just you know five gigs of size then you know even
    if the pandas data frame allocates。 memory it will be five gig it will never overrun
    your RAM。 So， this is the benefit of memory mapped。 file reading memory mapped
    file using pi arrow。 Now this is all about you know reading data within。
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在本质上。因此，对于一个50GB的文件，如果你只想计算列“行程距离”，而该列的大小只有5GB，那么即使pandas数据框架分配了内存，它也只会是5GB，不会超出你的RAM。因此，这是内存映射文件读取的好处，使用PyArrow进行内存映射文件读取。这一切都是关于你知道如何在内部读取数据。
- en: your program within your process but what about reading data across network。
    So， when Apache arrow。 came into picture this was not the case but later something
    got added and which is called Apache arrow。 flight。 This flight is mainly for
    transferring arrow buffer pi arrow buffer or Apache arrow buffers。 over grpc。
    Okay， given you can transfer anything over grpc but in this particular case if
    you want to。
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在你的进程中你的程序，但读取跨网络的数据怎么样呢？所以，当Apache Arrow出现时，情况并非如此，但后来添加了一些东西，称为Apache Arrow
    Flight。这个Flight主要用于通过gRPC传输Arrow缓冲区、Pi Arrow缓冲区或Apache Arrow缓冲区。好的，虽然你可以通过gRPC传输任何东西，但在这种特定情况下，如果你想要。
- en: you know take care of the performance what Apache arrow has to offer you can
    transmit only arrow。 buffers over grpc in case of arrow flight。 So， it is the
    on the via representation of the arrow data。 Now in case of inter-process communication
    some kind of serialization and deserialization is。 mandatory。 We can't avoid that
    some kind of serialization and deserialization。 So。
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 关注性能，Apache Arrow提供的，你只能在Arrow Flight的情况下通过gRPC传输Arrow缓冲区。因此，这是Arrow数据的传递表示。在进程间通信的情况下，某种序列化和反序列化是必要的。我们无法避免某种序列化和反序列化。
- en: what Apache arrow does is that is not deserialization they call it rehydration
    of data which is much。 cheaper than deserialization which means that rehydration
    means that I know which memory buffer。 corresponds to which data and I can just
    do a memcopy on that。 As far as serialization goes。 there is no data serialization
    but there is a meta data that is created using Google flat。
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Arrow所做的是不进行反序列化，他们称之为数据的再水合，这比反序列化便宜得多。再水合意味着我知道哪个内存缓冲区对应于哪些数据，我可以直接进行内存复制。至于序列化，并没有数据序列化，但创建了一个使用Google
    Flat的元数据。
- en: buffer and that is serialized but that is a small part of the data。 So， that
    is the additional cost。 you give if you use Apache arrow flight using pi arrow
    or any other respective language libraries。 So， this is a purely client server
    architecture where you can create a flight server and everything。 is touched by
    client。 Client gets the flight info it is very similar to your flight， your whatever。
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 缓冲区被序列化，但那只是数据的一小部分。因此，这是额外的成本。如果你使用Apache Arrow Flight，使用Pi Arrow或任何其他相关语言库，就会产生这个成本。所以，这是一个纯粹的客户端-服务器架构，你可以创建一个Flight服务器，一切都由客户端触及。客户端获取航班信息，这与你的航班非常相似。
- en: airport flight info you get。 It puts the data it gets back the data and this
    can also be distributed。 People are you know tending too much to use it as a in-memory
    cache but the creators of Apache。 arrow said that this is not meant for that but
    you know something works for you works for you。 So。 let's talk about some of the
    statistics of flight。 Again， I am reading the same New York。
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 飞机场航班信息你获得。它将获得的数据返回并且可以分发。人们知道，倾向于将其作为内存缓存，但Apache的创建者表示这并不意味着如此，但如果某样东西对你有效，那就有效。所以，让我们谈谈一些航班统计数据。再一次，我在读同样的纽约。
- en: trip data 126 MB file 1。83 seconds this time fine。 Now with Apache arrow flight
    we can only transfer， arrow buffers not any other buffers。 So。 I worry about wall
    time so I have converted that data。 frame into arrow table and it took me approximately
    194 to 100 milliseconds and when I transfer that。
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 行程数据126 MB文件耗时1.83秒，这次很好。现在通过Apache Arrow Flight，我们只能传输Arrow缓冲区，而不是其他缓冲区。因此，我担心实际时间，所以我把那个数据框转换成Arrow表，花了大约194到100毫秒，当我传输时。
- en: particular 126 MB data to a server it took me approximately you know again within
    the range of。 200 millisecond。 That's a huge I mean that's fast considering that
    much data transfer over， network。 Of course these these were you know fastest
    land possible and in the ideal condition。 but again if this is going to be proportionate
    with all the other loads your other things is also。
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 特定的126 MB数据传输到服务器，大约花了我200毫秒。这是个巨大的速度，考虑到如此多的数据通过网络传输。当然，这些都是在最快的条件下进行的。但如果这与你的其他负载成比例，那也是如此。
- en: going to be proportionate and when we talk about fetching the data back same
    126 MB odd data fetching。 back again it's approximately 158 or in that range only
    within 200 millisecond。 Now why I am talking about this particular thing you know
    we have managed to transfer 126 MB data。 and get back 126 MB data within you know
    400 odd millisecond that's fast。 In contrast if you are。
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们谈论提取数据时，获取的126 MB数据大约在200毫秒内返回，大约是158或那个范围。为什么我要谈论这个特别的事情呢？因为我们成功地在400毫秒内传输126
    MB数据并返回126 MB数据，这很快。相比之下，如果你是。
- en: trying to store a data frame somewhere and you want to serialize that data frame
    one of the most。 often used technique is pickling。 Pickling takes time so does
    unpickling that will also take time。 Here you can use the data as soon as you
    get it so the table which you are getting it over here。 in the top of the screen
    you can use that table then in there itself you don't need to unpickle it。
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想在某处存储数据框并希望序列化该数据框，最常用的技术之一就是打包。打包需要时间，解包也需要时间。在这里，你可以在获取数据后立即使用数据，所以在屏幕顶部你获取的表格可以直接使用，而无需解包。
- en: So arrow flight in my opinion has interesting possibilities it is very fast
    and for the purpose。 you want to use it it's up to you。 So the opportunities with
    this particular new format new way of。 representing columnar in memory data is
    limitless but always evaluate the cost even if something is。 very fast performant
    there is a cost associated with it so nothing is free in this world and I。
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在我看来，Arrow Flight 具有有趣的可能性，它非常快速，你想如何使用它由你决定。对于这种新的格式和表示内存中列式数据的方法，机会是无限的，但总是要评估成本，即使某些东西非常快速高效，仍然会有相应的成本，所以这个世界上没有什么是免费的。
- en: guess I forget to give a disclaimer in the beginning that I am not related to
    Apache arrow I am just a。 user and maybe a potential future contributor but at
    this moment of time I am a user。 So with this I complete my talk I thank you all
    of you I hope that I knew spent here was worth it。 Thank you very much。 Thank
    you。
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我想我忘了在开始时说明一下，我与 Apache Arrow 没有关系，我只是一个用户，也许将来会成为贡献者，但此刻我还是用户。所以到此我结束我的发言，感谢大家，希望我在这里的分享是值得的。非常感谢。谢谢。
- en: '![](img/1bb5d948cb3320391d7315a340e72f5b_4.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1bb5d948cb3320391d7315a340e72f5b_4.png)'
