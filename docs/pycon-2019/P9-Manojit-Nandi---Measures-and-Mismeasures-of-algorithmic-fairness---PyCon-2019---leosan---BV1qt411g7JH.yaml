- en: P9：Manojit Nandi - Measures and Mismeasures of algorithmic fairness - PyCon
    2019 - leosan - BV1qt411g7JH
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Hi everyone welcome on that speaker would be Manajit Nandi talking to us。 about
    measures and mismeasures of algorithmic fairness。
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/19dc4c4a9b64c2297f4044cba9885c3c_1.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
- en: '![](img/19dc4c4a9b64c2297f4044cba9885c3c_2.png)'
  id: totrans-3
  prefs: []
  type: TYPE_IMG
- en: '![](img/19dc4c4a9b64c2297f4044cba9885c3c_3.png)'
  id: totrans-4
  prefs: []
  type: TYPE_IMG
- en: How do I do things？ There we go that's how things happen。 Okay ah sorry for
    the bit。
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/19dc4c4a9b64c2297f4044cba9885c3c_5.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
- en: like hello everyone thank you all for coming to my talk this afternoon on。 measures
    and mismeasures of algorithmic fairness。 I'm Manajit Nandi and。
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/19dc4c4a9b64c2297f4044cba9885c3c_7.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
- en: I'm a data scientist so a quick about me according to the Google Cloud Computer，
    Vision API。 So I ran a photo myself through the Google Vision API and it produced。
    some labels for me so let's go through some of them and see how right they are。
    Am I a dancer？
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: Yes I'm an aerial dancer in a circus acrobat I just work as a。 data scientist
    for 40 hours a week because I got a pay rent somehow。 Am I， entertaining？
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: I hope so you're stuck with me for the next 40 minutes。 Am I fun？
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: Ah most of the time and lastly am I a girl huh so that one is not true I am
    a。 male presenting cis man so why did Google Vision think I am a girl and the。
    thing is it's not just Google Vision if I were to run this through Microsoft's。
    Vision API it produces similar results it says like oh this is a picture of a。
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: lady and so the thing is like what are these really looking for when they say。
    this is a girl are they saying how similar am I to what they've understood to
    be a。 girl is it because of oh this is a dancer and all the dancers in my training。
    set were girls so therefore this is a girl as well or is there some physical。
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: feature about me that makes it think that oh this is probably a girl and this。
    seems like a fun innocuous example but these types of systems of automatic。 gender
    recognition are used everywhere for example to basically show you better。
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/19dc4c4a9b64c2297f4044cba9885c3c_9.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
- en: ads when you're in a taxi cab for some reason because after all is not the true。
    purpose of data size to show you better ads and think about this this is it sounds。
    really dumb it's actually quite dangerous because if you basically have。 some
    prototypical understanding of like what this is what a man looks like or。
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: this is what a woman should look like you could actually misgender people whom。
    that is actually a huge issue to and so there's a researcher at University of，
    Washington osu！
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: Keys and in who's done a survey of how queerness and gender are。 thought out
    in human computer systems and their overall findings is that the way。 we think
    about gender in human computer systems is inherently trans exclusionary。 so now
    as we try to be like data driven leaders and we use these types of。
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: systems we are sort of using systems that are inherently trans exclusionary
    we。 inherently make decisions that ignore the existence and lived experience of
    the。 trans community and I hope this example sort of motivates why we should care
    about。 fairness and ethics when we do data science machine learning so what is。
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们所使用的系统本质上是排除跨性别者的，我们做出的决策本质上忽视了跨性别社区的存在和生活经验，我希望这个例子能激励我们关注公平和伦理，尤其是在进行数据科学和机器学习时。那么，什么是。
- en: '![](img/19dc4c4a9b64c2297f4044cba9885c3c_11.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](img/19dc4c4a9b64c2297f4044cba9885c3c_11.png)'
- en: algorithmic fairness so algorithmic fairness is a field of research that's。
    aimed at trying to mitigate the effects of unwarranted bias on discrimination
    on。 people on machine learning and so this is a kind of weird thing because like
    bias。 is a very overlord term in data science and machine learning it's probably
    a。
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 算法公平性是一个旨在缓解不当偏见对人们在机器学习中歧视影响的研究领域。这是一个有点奇怪的事情，因为偏见在数据科学和机器学习中是一个非常广泛的术语，这可能是。
- en: second most overlord term after kernel which has a bunch of different definitions。
    in machine learning and right now sort of like the focus on this type of。 research
    has been like coming up with mathematical definitions of fairness we。 have this
    mathematical definition of fairness we're gonna find some solution。
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，除了内核之外的第二个最重要的术语有许多不同的定义。目前，这类研究的重点是提出公平性的数学定义。我们有这个公平性的数学定义，我们将找到一些解决方案。
- en: to it and hope that maps back well to the original problem but over the last。
    year or so we've seen some pushback against this we've seen that okay in this。
    paper for example called fairness abstraction and social technical。 systems that
    there's sort of an over a formalism trap that we care too much。
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 但我们希望这能很好地映射回原始问题，但在过去的一年左右，我们看到了一些反对意见，例如在一篇名为“公平抽象与社会技术系统”的论文中，存在一种形式主义陷阱，我们对此过于关注。
- en: about mathematical formalisms that even if you solve the mathematical。 formalism
    it doesn't really translate well to a real world solution and the。 key thing to
    keep them out this is like Ferris is inherently a social and ethical。 concept
    it cannot be perfectly captured or represented through mathematical。
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 关于数学形式化的问题，即使你解决了数学形式化，它在现实世界中的解决方案也并不容易转化，而关键在于，像费里斯这样本质上是一个社会和伦理的概念，无法通过数学完美捕捉或表现。
- en: definitions or statistical metrics and we'll talk about the research and I guess。
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 定义或统计指标，我们将讨论研究，想必。
- en: '![](img/19dc4c4a9b64c2297f4044cba9885c3c_13.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](img/19dc4c4a9b64c2297f4044cba9885c3c_13.png)'
- en: I want to take a slight like a side about like this is an idea of like okay
    well。 we're using algorithms and math and math can't be racist that's silly and
    it's。 actually came up in January when this like alt-right journalist basically。
    criticized the New York Congresswoman Alex Ocasio Cortez when she said that okay。
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我想稍微谈一下这个问题，也就是说，我们在使用算法和数学，而数学本身不能是种族歧视，这太荒谬了。实际上，这个问题是在一月份提出的，当时一位极右翼记者批评了纽约国会女议员亚历克斯·奥卡西奥·科特斯，她表示这样。
- en: algorithms have been shown to discriminate against blacks and Hispanics。 in
    different cases and so the thing is like when we say that an algorithm is。 racist
    or sexist we're not saying that like math is inherently racist or。 sexist we're
    not saying that like the prime numbers are inherently racist or。
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 研究表明，算法在不同情况下对黑人和西班牙裔人士存在歧视。因此，当我们说一个算法是种族歧视或性别歧视时，我们并不是说数学本身是种族歧视或性别歧视，我们并不是说质数本身就是种族歧视的。
- en: logistic regression isn't like inherently sexist we're talking about the ways
    we。 use math the ways we use algorithms can reinforce societal inequalities you。
    know prime numbers are not racist but the way we can use prime numbers to break。
    cryptographic systems has real world implications the way we use logistic。
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归本身并不是性别歧视的，我们谈论的是我们使用数学、使用算法的方式如何加强社会不平等。你知道，质数本身不是种族歧视的，但我们可以使用质数破解密码系统的方式在现实世界中具有实际影响。
- en: regression to decide who gets a credit loan or bank loan or not has real world。
    implications and that's what we need to think about when we design these。 algorithms
    they're not sort of just existing in the void by themselves there's。
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 回归分析来决定谁能获得信用贷款或银行贷款，这在现实世界中有实际影响，这是我们在设计这些算法时需要考虑的，它们并不是孤立存在的。
- en: '![](img/19dc4c4a9b64c2297f4044cba9885c3c_15.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](img/19dc4c4a9b64c2297f4044cba9885c3c_15.png)'
- en: embedded in human systems and on the research side of this algorithmic fairness。
    so now let's be revealed to called fairness accountability transparency。 fat star
    ML and this is like this interdisciplinary research area about。 how we get machine
    learning and technical systems to care about ideas like fairness。
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入在人类系统中的算法公平性研究领域。现在让我们揭示所谓的公平性、问责制和透明度。这个跨学科的研究领域探讨如何使机器学习和技术系统关注公平的理念。
- en: and justice and equality has really exploded over the last few years you。 seeing
    a dedicated conference to it ACM fast-star which was held in Lanta we。 have seen
    lots of open-source libraries about this and even at this conference。 we actually
    have another talk about model fairness later today and I do for。
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去几年中，公平、正义和平等的话题得到了迅速发展，你可以看到专门的会议，比如在兰塔举办的ACM Fast-STAR。我们看到许多关于这方面的开源库，今天我们在会议上还有关于模型公平性的另一个讲座。
- en: anyone who's also interested in attending that I have looked at that other talk
    I。 do believe there's enough differentiation between our two talks that you would。
    get something about worthwhile by attending both talks for anyone who has a slight。
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 如果有兴趣参加的人，我已查看过那场讲座，我相信我们的两场讲座之间有足够的差异，让你参加两场都会有所收获。
- en: '![](img/19dc4c4a9b64c2297f4044cba9885c3c_17.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](img/19dc4c4a9b64c2297f4044cba9885c3c_17.png)'
- en: concern and it's not just researchers who are talking about this we talk about。
    this in popular media too oh so weapons and math destruction by Kathy O'Neill。
    automating quality by Virginia U-Banks and algorithms of repression by Safya。
    Nobel and so you know the big three and I think they take this interesting。
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 这不仅仅是研究人员在讨论，我们在流行媒体中也谈论这一点，凯西·奥尼尔的《武器与数学毁灭》，维吉尼亚·尤班克斯的《自动化质量》，以及萨法·诺贝尔的《压迫算法》，这三本书都很有趣。
- en: approach of like okay here's how the algorithms work on the people like we as。
    the data scientists we as the machine learning researchers or sort of。 separated
    from the downstream effects of how our work impacts the general。 humanity and
    a general society and thing is like as data scientists and like。
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法是，算法如何影响人们，我们作为数据科学家，作为机器学习研究者，与我们工作的下游影响似乎是分开的。
- en: computer scientists we're sort of trained like math and statistics we're not。
    trained to think about ethics we're not think a trained to think about a public。
    policy but we do need to understand that like our work has downstream。 implications
    I think deep down like we may not be comfortable like taking the。
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机科学家训练时主要是数学和统计，我们并未被训练去思考伦理或公共政策，但我们需要理解我们的工作有下游影响。
- en: role of an ethicist or taking the role of a public policy but I think deep down。
    none of us want to cause the apocalypse none of us wanted to harm to others like。
    no one wants to be the people written about in these books and I think the way。
    that they attacked this problem of like talking about the dangers of these。
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 伦理学家的角色或公共政策的角色，但我认为深层次上。我们都不想引发 apocalypse，我们都不想对他人造成伤害。没有人想成为这些书中所写的人，我认为他们处理这个问题的方式很有意思。
- en: outcomes like how these hurt people is an interesting perspective from just。
    research which is okay here's the cool things we could do whereas this these。
    popular media talk about like how could this hurt people and talk about how could。
    this hurt people we have to think about like okay maybe you don't really care。
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 像这些结果如何伤害人们的视角很有趣，研究中的观点是，好的事情我们可以做，而流行媒体则讨论这些可能如何伤害人们。
- en: about the moral side of like I don't want to hurt people but I can maybe encourage。
    you to think about these types of problems by legal regulations of you。 maybe
    don't care about hurting people but you do care about keeping your job。 because
    if you break some of these things well you're violating legal laws。
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 关于道德的一面，我不想伤害人，但我可以鼓励你考虑这些问题，通过法律法规。也许你不在乎伤害他人，但你在乎保住工作，因为如果你违反了某些规定，就会触犯法律。
- en: and then you can lose your job because your company will be fined millions of。
    dollars and so in the US for example we have disparate impact laws which prevent。
    again which basically regulate how you can give out loans how you can hire for。
    people such that you don't harm them and also in the EU we now have GDPR which
    is。
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 然后你可能会失去工作，因为你的公司将被罚款数百万美元，因此在美国，例如，我们有差异影响法，这些法则基本上规范了如何发放贷款，如何雇佣人，以免伤害他们。此外，在欧盟，我们现在有GDPR。
- en: an overarching holistic set of laws and rules of like how we should think about。
    the way we use algorithms to process people's data and we have to think about。
    okay can you explain why you're all with the media to this it can you explain。
    why your neural network didn't give someone alone and can they opt out of。
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 一套总体的整体法律和规则，关于我们如何思考使用算法处理个人数据的方式，我们必须考虑。好吧，你能解释一下为什么你的媒体会这样吗？你能解释一下为什么你的神经网络没有给某人贷款，他们能否选择退出。
- en: that as well and so let's talk about the different types of。
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这也是，咱们来谈谈不同类型的。
- en: '![](img/19dc4c4a9b64c2297f4044cba9885c3c_19.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](img/19dc4c4a9b64c2297f4044cba9885c3c_19.png)'
- en: algorithmic biases and so this is based off some work by researchers at Microsoft。
    and research and Cornell and this was presented at the conference that is now。
    near or up any you are IPS someone told me it's pronounced like Europe and I'm。
    not actually sure if that was a joke or not but really these research is sort。
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 算法偏见，这基于微软和康奈尔大学研究人员的一些工作。这在现在的会议上被展示出来了。有人告诉我，它的发音像欧洲，我不确定这是不是玩笑，但这些研究确实是种。
- en: '![](img/19dc4c4a9b64c2297f4044cba9885c3c_21.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](img/19dc4c4a9b64c2297f4044cba9885c3c_21.png)'
- en: of identified three big categories of way we think about like all with devices。
    of like where we are now where we're heading and where we need to be so first。
    let's talk about where we are now so a lot of problems in all with vices focus
    on。 this problem of allocation who gets a loan or not who gets a job and these
    are。
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们确定了思考所有设备的三大类方式，包括我们现在的位置、我们所走的方向以及我们需要达到的目标，首先，让我们谈谈我们现在的位置，很多关于设备的问题集中在贷款或就业分配的问题上。
- en: really just like binary classification problems of like do men get software。
    engineering jobs more than women do whites get credit loans more than non。 whites
    kind of thing this is a binary yes or no and this is really what most of。 the
    research has been on now because it's sort of easy to formalize as a。
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这些实际上就是二元分类问题，例如男性是否比女性更容易获得软件工程职位，白人是否比非白人更容易获得信用贷款，这实际上是一个二元的“是”或“否”，而这正是大多数研究所集中于的，因为这相对容易形式化。
- en: mathematics problem and easy to solve and so this was in the new big story last。
    year was that Amazon had to scrape their AI recruiting tool because they found。
    out that it was just women against women it was basically looking at things of。
    like oh hey if you have a name like Jared or you were on the lacrosse team at。
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 数学问题且容易解决，因此去年新闻报道中提到，亚马逊不得不停止使用其AI招聘工具，因为他们发现它只是对女性有偏见，基本上是关注像“贾里德”这样的名字，或者你曾在长曲棍球队。
- en: back in college you're probably more likely to be hired as a software engineer。
    which really makes no sense and so that's a scrapeness because they realize oh
    hey。 we were harming or choosing our model was selecting against women but if
    we use。 this next we'll talk about where we're sort of heading and so next is
    this idea。
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 回到大学时，你可能更有可能被聘为软件工程师，这真的没有意义，因此这是一种偏见，因为他们意识到，哦，我们在伤害或选择的模型是对女性有偏见的，但如果我们使用。
- en: of bias representation and so this is focused on looking at like how harmful。
    stereotypes or harmful labels are propagated through machine learning。 systems
    and these are often related to like language problems or computer。 vision problems
    which are like the neural network problems of it just picks out。
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 偏见的表现形式，因此关注的是如何通过机器学习系统传播有害的刻板印象或标签，这通常与语言问题或计算机视觉问题有关，这些都是神经网络的问题，它只是挑选出。
- en: a solution and do some blackbock neural network magic but we don't really care。
    about why the inner mechanics of what it's doing and these are sort of hard to。
    quantify the errors compared to those previous bias and allocation problem。 which
    is why there's less research on this now but like we started to think。
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 一个解决方案并进行一些黑箱神经网络的魔法，但我们并不关心它所做的内部机制，这些错误与之前的偏见和分配问题相比是比较难以量化的，这也是为什么现在对此的研究较少，但我们开始思考。
- en: about it more and so this is famous example that when Google photos was。 deployed
    like 2015 that it actually labeled a group of black kids at a。 graduation party
    as gorillas and so for those of you who are not familiar it was。 like history
    of US I don't know if it all applies to other Western countries but。
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 这有一个著名的例子，就是当谷歌照片在2015年发布时，竟然将一群在毕业派对上的黑人孩子标记为大猩猩。对于那些不熟悉的人来说，这就像美国的历史，我不知道这是否适用于其他西方国家，但。
- en: gorillas or apes has been historically used as a derogatory slur against。 African
    Americans in this nation so this label like what is wrong but you it also。 invokes
    harmful stereotypes against African Americans and thing is like it's。 harder to
    quantify as an error because yeah this is a wrong label but at the same。
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 大猩猩或猿类在历史上曾被用作对美国非裔人士的贬义词，因此这种标签是错误的，但它也。 引发了对非裔美国人的有害刻板印象，而这一点更难量化，因为是的，这确实是一个错误的标签，但与此同时。
- en: time Google photos is never going to label a group of white kids as gorillas。
    this is specifically a label it gives to these kids here and it promotes this。
    harmful stereotype and other examples that sort of hard to quantify are snap。
    chat filters for example so when you apply snapchat filter flower crown it sort
    of。
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，谷歌照片永远不会将一群白人孩子标记为大猩猩。这是一个专门给这些孩子的标签，助长了这一有害的刻板印象，而其他一些难以量化的例子就是社交媒体滤镜，例如，当你应用社交媒体的花冠滤镜时。
- en: brightens the skin of the individual why because a snapchat filters of sort
    of。 learn okay brighter skin is associated with prettiness therefore when I apply。
    the affiliate filter brighten their skin and so everything about that's like a。
    really hard to quantify what's wrong it's doing the optimization process it's。
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 个体的肤色变亮，为什么呢？因为一些社交媒体滤镜会学习到更亮的肤色与美丽相关，因此当我应用这个滤镜时，会让他们的肤色变亮，所有这些都像是一个。 很难量化的问题，它正在优化过程中。
- en: supposed to do but there's just something about that solution just sits on easy。
    with you and another example is Google translate so when you translate sentences。
    from like Malay which is a language that doesn't have gendered pronouns so you。
    can have like they are a doctor they are a soldier they are a professor they are。
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 应该做的事情，但这个解决方案总让人感到不安，另一个例子是谷歌翻译，因此当你翻译一些来自马来语的句子时，马来语没有性别代词，你可以说他们是医生，他们是士兵，他们是教授，他们是。
- en: a prostitute they are a nurse but you translate to English it's sort of。 automatically
    genders them he is a doctor he's a soldier he's a professor。 she is a proton prostitute
    she is a maid she's a nurse and at the same time like。 Google translate is doing
    this mathematical observation it's like hard to。
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 一名妓女，她是一名护士，但你翻译成英语时，它就会自动赋予性别，他是一名医生，他是一名士兵，他是一名教授。她是一名妓女，她是一名女仆，她是一名护士，同时，谷歌翻译在进行这种数学观察时，这很难。
- en: say why this is wrong this is a valid translation of those sentences but it's。
    just something about it that just sits uneasy with you but and to give credit。
    to where's due Google has actually made a sort of hot fix where when you translate。
    from a non-gendered language to a gendered language it will actually show。
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么这是错误的，这对这些句子的翻译是有效的，但总有一些东西让你感到不安，不过值得一提的是，谷歌实际上已经做出了一种临时修复，当你从一种无性别语言翻译成性别语言时，它实际上会显示。
- en: both gendered ways so translating from they are a doctor translates to she is
    a。 doctor he is a doctor now so this is like a quick fix for this it's still not。
    available on the iOS and Android version of Google translate and you still see。
    like some of these biases when you do Google autocomplete so if you type in。
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 这在性别上是有区别的，因此从“他们是医生”翻译成“她是医生”或“他是医生”，这对这个问题的快速修正来说仍然不可用。在谷歌翻译的iOS和Android版本中，你仍然会看到一些偏见，当你进行谷歌自动补全时，比如说。
- en: the search of he is a blah you'll probably get doctor or soldier more than。
    if you do she is a blah in Google autocomplete， finally where do we need to be
    and so this last version is called the。 weaponization of machine learning and
    so the key idea of this is like as data。 scientists we're taught to train models
    we produce some metrics we don't really。
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你搜索“他是个……”你可能会得到“医生”或“士兵”的结果。如果你搜索“她是个……”在谷歌自动完成中，最后我们需要达到的目标是，最后这个版本被称为机器学习的武器化，因此关键思想是，作为数据科学家，我们被教导去训练模型，产生一些指标，但我们并不真正。
- en: think about how are more can harm people or how could be misused and so this
    is。 big story back in 2017 a group of Stanford researchers tried to create a。
    cost-fiction algorithm that tries to predict people's sexuality or sexual。 preferences
    based on images of their faces and so effectively said they created。
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到我们可能如何伤害人们或如何被误用，2017年，斯坦福大学的一组研究人员试图创建一个基于人脸图像预测人们性取向或性偏好的算法。
- en: a gay-dahr of trying to predict if someone is heterosexual or homosexual。 and
    the thing about this is their countries in this world where homosexuality is。
    criminalized by the state if you are out as a homosexual you will be executed
    by。 the state and there are plenty more words like it's not criminalized but you。
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 有一个尝试预测某人是异性恋还是同性恋的工具。值得注意的是，世界上有些国家将同性恋视为犯罪，如果你作为同性恋者公开出柜，你可能会被国家处决，还有许多地方虽然没有被犯罪化，但。
- en: used to really dangerous to be out as a homosexual so if someone takes this
    a。 model they could use it as a weapon against homosexuals in those countries。
    and think about this is there's no mathematical measure no unit test you can。
    write that can tell you this is a bad idea you shouldn't do this this is really。
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个公开的同性恋者是非常危险的，因此如果某人使用这个模型，他们可能会把它作为武器对抗那些国家的同性恋者。要考虑的是，没有任何数学衡量标准或单元测试可以告诉你这主意不好，你不应该这样做。
- en: gonna require you to have cultural anthropologists and historians in the。 room
    to tell you like hey this can harm people and that's why I'm really great。 that
    there is sort of the starting discussion of like having data scientists。 and machine
    learning technologists really think about ethics training like。
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 这将要求你在场中有文化人类学家和历史学家来告诉你，“嘿，这可能会伤害人们。”这就是为什么我真的很高兴有这样的讨论开始，数据科学家和机器学习技术专家开始思考伦理培训。
- en: we need to do degree programs that like UC Berkeley or Carnegie Mellon they're。
    starting to include an ethics component well I don't think that we're really fully。
    solid this I do think it's the start of developing a different tech culture。 that
    really cares about the downstream effects of our work on people who are。
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要进行像加州大学伯克利分校或卡内基梅隆大学这样的学位项目，它们开始纳入伦理成分。我认为这确实是培养一种更加关注我们工作对人们下游影响的技术文化的开始。
- en: not like us so let's talk about the different types of fairness measures so。
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 不像我们那么简单，所以让我们谈谈不同类型的公平性衡量标准。
- en: '![](img/19dc4c4a9b64c2297f4044cba9885c3c_23.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](img/19dc4c4a9b64c2297f4044cba9885c3c_23.png)'
- en: these are like the mathematical definitions and I will really talk about。 like
    what they are and some their big weaknesses of them and so this is based。 off
    of work by two Stanford researchers Sam Corbett Davies and Sherrod Gull and。 they're
    based on this behavior measures and mismeasures of fairness which this。
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 这些就像是数学定义，我将真正讨论它们是什么以及它们的一些主要缺陷，因此这基于斯坦福大学两位研究人员山姆·科贝特·戴维斯和谢罗德·古尔的工作。他们基于这些行为衡量标准和不当衡量标准。
- en: talk is the title this talk is based on and so we'll talk about difference。
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这次演讲的标题就是基于此，因此我们将讨论差异。
- en: '![](img/19dc4c4a9b64c2297f4044cba9885c3c_25.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](img/19dc4c4a9b64c2297f4044cba9885c3c_25.png)'
- en: definitions of algorithmic fairness so there was a talk a year ago by this。
    principal professor Arvind Narayan we really showed that okay there were 21。 definitions
    of algorithmic fairness at the time of the time when he gave that。 talk now there's
    more than 30 definitions of like algorithmic fairness now and。
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 算法公平性的定义，去年阿文德·纳拉扬教授的演讲真正展示了在他演讲时有21个算法公平性的定义，现在已经有超过30个。
- en: think about this like as computer scientists and like statisticians we think。
    okay there's three definitions there's got to be like some that are better than。
    all that it's got to be like this one true definition of fairness we can use。
    and that's not really true why because fairness is a social and cultural。
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 把这件事看作计算机科学家和统计学家，我们会想，好的，有三个定义，肯定有一些比其他的更好。必须有一个真正的公平定义可以使用，但这其实并不真实，为什么？因为公平是社会和文化的产物。
- en: concept like what was considered fair 50 years ago is not going to be considered。
    what we consider fair now and what we consider fair 50 years from now is。 different
    from what we consider fair today but these sort of different 30 so。 definitions
    of algorithmic fres can be broken down to three big groups anti。
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 而且，50年后我们认为公平的事情，与今天认为的公平是不同的，但这些不同的定义可以归纳为三大类反分类。
- en: classification cost of question parity and finally calibration and I'll just
    give a。 quick warning this is where the talk story gets a bit more technical and。
    mathematical so first up there's these anti classification measures and so what。
    these are is that okay you have these protective features such as like race。
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 分类成本、问题平衡，最后是校准，我要给个快速警告，这里谈话的故事变得有些技术性和数学性。首先，这些反分类措施是什么？也就是说，你有这些保护特征，比如种族。
- en: gender religion in place of origin and you have unprotected features for your。
    model and whether these anti-placement works ideally is that okay when you make。
    an overly decision it should effectively ignore those demographic features。 ignore
    race it's ignore gender should ignore religion when it makes this。
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 性别、宗教和出生地等特征，你有未保护特征用于模型，而这些反置特征理想情况下是，当你做出决策时，它应该有效地忽略那些人口特征。忽略种族、忽略性别、忽略宗教。
- en: decision or behave in a way it does so it's sort of this idea of individual。
    fairness you judge a person by the content of their character rather than the。
    color of skin kind of thing that two people should be treated equally if they。
    have the same unprotected features but the thing about this is that like you。
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 决策或行为的方式，就是这个个体公平的概念，你根据人的品格判断一个人，而不是肤色。两个人如果拥有相同的未保护特征，应该受到平等对待，但这里的事情在于，像是50年前被认为公平的概念现在就不再被认为公平。
- en: can't just like throw out race and gender from your model or from your data。
    sense like okay I'm done because we're now starting to realize that there are。
    proxy features that we can worry about like features that are not directly。 rate
    encode race or gender but are highly correlated with them and so for。
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 你不能仅仅从模型或数据中抛弃种族和性别，感觉好像我完成了，因为我们现在开始意识到有一些代理特征，我们需要担心，比如那些并不直接编码种族或性别但与之高度相关的特征。
- en: example there's a story back in like 2012 when the office supply company。 Staples
    data promotion that if you live within 20 miles of a competing。 competing store
    we will give you a special coupon discount so that you'll。 come to our store instead
    the thing is the people who live in those like。
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 举个例子，2012年有个故事，办公用品公司Staples推出促销，如果你住在竞争商店20英里内，我们将给你特殊优惠券折扣，促使你光顾我们的商店。问题是，那些住在离竞争对手20英里的社区，通常是富裕的郊区社区。
- en: neighborhoods at 20 miles from the competitors there's a sort of like。 wealthier
    suburban neighborhoods and the people who live in wealthy suburban。 neighborhoods
    have certain racial demographics for example so what do you。 it's when your model
    discriminates or makes a decision based on location it's。
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '住在富裕郊区的人群有特定的种族人口特征，所以，当你的模型基于位置进行歧视或做出决定时，它就是这样的。 '
- en: inadvertently acting as if it was making decisions based on race and so。 Staples
    was hit huge fine with that for disparate impact and as a result these。 types
    of features are very useful for designing fairness aware models so what。 is a
    fairness aware model so it's sort of like a traditional machine learning。
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 无意中表现得好像是在基于种族做出决定，因此Staples因为不同影响而遭到巨额罚款，因此这些特征对于设计关注公平的模型非常有用。那么什么是关注公平的模型呢？这有点像传统的机器学习。
- en: supervised model except now we have this added component we need to think about。
    those protected features so when you have these standard machine learning you。
    have your features X and your labels Y and you basically want to map the。 features
    X to the labels Y well now with fairness aware algorithms you have X you。
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 监督模型现在有了这个需要考虑的附加组件，即那些受保护的特征，因此在标准机器学习中，你有特征 X 和标签 Y，你基本上是想将特征 X 映射到标签 Y，而现在使用公平性算法时，你有
    X。
- en: have Y and you have your protected attributes so race gender religion。 place
    of origin as a reputed class Z and what you want to do is like you want to。 like
    learn features that learn your label so you want to learn who to give a。 loan
    to we don't want to accidentally learn race or gender and so one really。
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 你有 Y 和受保护属性，例如种族、性别、宗教、出生地作为声誉类别 Z，你想要的就是学习能够学习标签的特征，所以你想学习给谁贷款，我们不想意外地学习种族或性别。
- en: cool algorithm that does this is fairness aware GAN so I think this is a。 really
    clever idea so how this works with GANs is that you sort of have linked。 these
    two sub models one is sort of your standard machine learning classifier it's。
    trying to like take the features and learn the labels the other one is trying。
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 一个很酷的算法是公平性意识的 GAN，我认为这是一个非常聪明的想法，它与 GAN 的工作原理是将这两个子模型链接在一起，一个是你的标准机器学习分类器，它试图学习特征并预测标签，另一个则在尝试。
- en: to take the labels and learn and protect the class and so what you can think
    about。 is a sort of like this dual like I'm trying to learn a good classifier
    you're。 trying to break me by saying oh that cost fire accidentally learns race
    or。 gender and so this is what how that really works and like you have to look。
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 处理标签并学习并保护类别，因此你可以考虑一种双重的思维方式：我试图学习一个好的分类器，而你试图通过说哦那个分类器偶然学习了种族或性别来打破我，这就是它真正运作的方式。
- en: at this law function so like the one in the red box is sort of saying I'm I'm
    a。 good classifier not am I learning features that predict who gets a loan or
    not the。 one in the blue box is saying okay you learn who to give a credit loan
    to are。 you accidentally learning race or gender is your basically who gets a
    loan or。
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个红框中的损失函数是说我是一种好的分类器，不是我在学习哪些特征来预测谁能获得贷款，而蓝框中的则是在说，好的，你学习了谁可以获得信用贷款，你是否意外地学习了种族或性别，基本上是谁能获得贷款。
- en: not basically just figuring out is this person white or non-white and sort of。
    this trade-off if you really squint your eyes at this if you study machine。 learning
    and regularization so like originally regression or last regression。 you squint
    your eyes at this and stare at it deeply it kind of looks like a。
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 其实只是弄清楚这个人是白人还是非白人，如果你仔细观察，如果你研究机器学习和正规化，比如最初的回归或逻辑回归，如果你仔细看着它并深深注视它，它看起来有点像一个。
- en: regularization of I want to be a good costifier but I don't want to be overly。
    complex in this case complex it's sort of like I don't want to accidentally be。
    discriminatory and this is sometimes referred to as this accuracy fairness。 trade-off
    and I'll be really honest I don't like that this is referred to as a。
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 正规化，我想成为一个好的分类器，但我不想在这种情况下过于复杂，这种复杂性有点像我不想意外地具有歧视性，这有时被称为准确性与公平性之间的权衡，老实说，我不喜欢这样称呼。
- en: trade-off because if you really think about like why are we doing all this。
    stuff in the first place because our training set is biased because our。 testing
    set is biased and that means our testing labels are biased like our。 answer key
    is wrong we don't want models that perfectly learn the the testing。
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 权衡，因为如果你真的思考我们为什么要做这一切，原因在于我们的训练集是有偏的，因为我们的测试集是有偏的，这意味着我们的测试标签是有偏的，就像我们的答案键是错误的，我们不想要完美学习测试的模型。
- en: set labels because then it's also learning those biases as well so we。 expect
    the accuracy to go down and I also really don't like calling this a trade-off。
    because it sort of ties into this like very harmful idea that somehow。 promoting
    diversity is requires you to partake in sub-optimality and so you see。
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 设置标签，因为它也在学习这些偏见，所以我们预计准确性会下降，我也不喜欢称之为权衡，因为这会与一种有害的想法相关联，即促进多样性需要你参与亚最优性，因此你看到。
- en: this is like tech hiring where okay if a software company says like oh hey we
    have。 ours 50% of our software engineers a woman that's a good thing but then
    there's。 always like some loser on Twitter it's like hey I got a brilliant idea
    instead。 of hiring people based on their gender what if we hire the most talented
    person。
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 这就像科技招聘一样，假如一家软件公司说“嘿，我们的50%的软件工程师是女性”，这是好事，但总会有人在推特上说：“嘿，我有个绝妙的主意，为什么不根据人才而不是性别来招聘最优秀的人呢？”
- en: for their job and this is sort of like ties into harmful idea like in order
    to。 and get diversity you have to pick less optimal candidates or less skilled。
    candidates and that's not true at all like diversity is very good for tech。 having
    people with different lived experiences and different ways of。
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 这是与一种有害观念有关的，认为为了实现多样性，你必须选择较不理想的候选人或技能较低的候选人，这完全不正确，多样性对科技领域是非常有益的，拥有不同生活经历和不同思维方式的人。
- en: thinking in the room is important because they can call you out when you're
    being。 stupid and that's very important data science and so what are the dangers
    of。 using these anti-classification measures like okay that sounds like a good
    idea。 like we should judge people based on their important characteristics and
    not。
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在房间里进行思考是重要的，因为他们可以在你愚蠢的时候指出你，这对于数据科学非常重要。那么，使用这些反分类措施有什么危险呢？听起来好像是个好主意，我们应该根据人们的重要特征来评判他们，而不是。
- en: accidentally learn those protective features well by removing those。 protective
    features we're sort of under ignoring the underlying process that。 acts on different
    demographic groups like these metrics sort of look on making。 the outcome equal
    but really ferris is making the process equal and so one way。
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 不小心学习了那些保护特征，通过移除这些保护特征，我们在某种程度上忽视了作用于不同人口群体的潜在过程。这些指标似乎是在努力使结果平等，但实际上，公平是使过程平等，因此有一种方法。
- en: this is a danger is sort of prison recidivism so who gets when you're a。 prisoner
    and you're defending like you get released on bail or not and so how。 prison of
    our citizens works is like this is all the compass it gives you。 score between
    one and ten which is sort of like how risky is it to let you out。
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一种危险，类似于监狱复犯率，因此，当你是一名囚犯并且在辩护时，你是否可以获得保释，监狱如何运作就是这样，它给你一个分数，范围在一到十之间，这大致表明放你出去的风险有多大。
- en: like what's likelihood you will come in on a crime if we release you and the。
    thing is that male defendants are more likely for the same score male。 defense
    are more likely to reoffend than female dependence so if you ignore。 gender in
    this case and sort of average of two lines you sort of get to the。
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们释放你，你再犯罪的可能性是什么？问题在于男性被告在同一分数下比女性被告更容易再犯。因此，如果你在这种情况下忽略性别，平均两条线，你就会得到。
- en: average line that's higher than the female defendant rate and so if you cut。
    your threshold up off like a little below 60 percent whereas for me if you just。
    looked at gender exclusively you would say okay we don't release a female。 defendant
    when they have a score of eight or above whereas if you take the average。
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 平均线高于女性被告的比率，因此如果你把阈值设定在略低于60%时，而对我来说，如果你仅仅看性别，你会说好吧，我们不会释放女性被告，当她们的分数是八分或以上，而如果你取平均值。
- en: and you now have that threshold we don't release anyone if it's like seven or。
    above and thus you're now detaining more women who then you were generally done。
    if you had looked at gender separately as a feature in this so next is。 classification
    parity so classification parity is sort of extension of like。
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在有了那个阈值，如果是七分或以上，我们就不会释放任何人，因此你现在拘留了更多女性，这与之前的情况不同。如果你单独将性别视为一个特征，接下来是分类公平，因此分类公平可以视为一种扩展。
- en: traditional machine learning when machine learning you evaluate your。 model
    according to a metric like accuracy or precision or recall or AUC。 whereas now
    you take those metrics and sort of do it across different。 demographic groups
    for example this is famous example gender studies by an。
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在传统机器学习中，你根据准确率、精确度、召回率或AUC等指标来评估你的模型，而现在你需要在不同的人口群体中进行这些指标的评估。例如，这是著名的性别研究案例。
- en: MIT student Joy Bullolani and what she shows is that like these commercial face。
    detection algorithms Microsoft IBM and then face plus plus they're really good。
    detecting white male faces they're really pretty good at detecting white。 female
    faces detecting black male faces but the intersection of race and gender。
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 麻省理工学院的学生**乔伊·布洛拉尼**展示了这些商业人脸识别算法（如微软、IBM以及Face++）在识别白人男性面孔方面非常优秀，对白人女性面孔也很擅长，对黑人男性面孔的识别也不错，但在种族与性别的交集上。
- en: they're really bad detecting of black women faces and so this is still okay。
    across these different demographic groups the accuracy is significantly lower
    for。 black women and joy also has like a very cool video on YouTube that's like
    a。 spoken word poem about talking about okay how commercial computer vision。
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 对于黑人女性面孔的识别效果极差，因此在这些不同的人口群体中，黑人女性的准确性显著低下。乔伊还在YouTube上有一段非常酷的视频，作为一首口语诗，讲述了商业计算机视觉的现状。
- en: systems are constantly misgender of famous black women such as Michelle Obama
    or。 Oprah and these types of metrics or these type of cost-pation parity metrics
    or。 was common used to enforce those legal regulations so it's like the equal。
    opportunity equal employment opportunity act or things of that nature or sort
    of。
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 系统经常错误地识别著名的黑人女性，如**米歇尔·奥巴马**或**奥普拉**，而这些类型的指标或这些成本平衡指标通常用于执行法律法规，因此就像**平等机会**、**平等就业机会法**或类似的东西。
- en: using these types of classification metrics behind the scenes so what's the。
    most common one so most common one of these types of metrics is demographic。 parity
    so demographic parity is like how often do we get positive outcomes by。 different
    demographic groups or different protected groups and this is。
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在幕后使用这些分类指标，那么最常见的指标是什么呢？这些指标中最常见的是**人口平衡**，即不同人口群体或受保护群体获得积极结果的频率。
- en: what's really used to the audit models for disparate impact so like who gets
    a。 loan or not you look at different demographic groups and say that okay the。
    the positive rate like okay the percentage of loans of given to white men。 versus
    the percentage of loans given to non-white non-mails should be at most。
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 审计模型用于评估不同影响，像是贷款的发放，查看不同人口群体，然后说，白人男性获得贷款的比例与非白人非男性获得贷款的比例之间的差异最多应为**20%**。
- en: 20% difference that's what that 80% rule means they have to be within 80% of。
    each other and the thing about this is like that sounds nice okay if we're a。
    bank you have to sort of balance out the number of loans you give to different。
    populations but the thing is about you can satisfy that immediate thing but you。
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是**80%**规则的含义，他们的比例必须在彼此的**80%**范围内，而这听起来不错。如果我们是一家银行，你需要平衡向不同人群发放的贷款数量，但问题是你可以满足这一直接要求，但。
- en: could not really think about like the long-term consequences of your actions。
    and so this is a paper by researchers of UC Berkeley the late impact of fair。
    machine learning where they show that okay if you do this demographic balance。
    for demographic parity so you give the same percentage of loans to people in the。
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 可能没有真正考虑到你行为的长期后果。这是加州大学伯克利分校研究人员的一篇论文，讨论了公平机器学习的深远影响，他们展示了如果你进行这种人口平衡。
- en: orange population and people in the blue population you're giving a lot more。
    loans to people in the blue population who would then default on that loan and。
    a result their credit score goes down and the overall credit score distribution。
    for the blue population goes down as well so if you just think about satisfying。
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 橙色人群和蓝色人群之间的对比，你向蓝色人群提供了更多贷款，这些人随后可能违约，因此他们的信用评分下降，整体信用评分分布也随之下降，所以如果你考虑满足。
- en: that immediate constraint of like we just have to make the outcomes equal positive。
    outcomes equal then you can harm them in the long run and so another way to also。
    think about this is that like when it comes to tax hiring of like okay we just。
    need to hire make sure the percentage of women we hire is equal to the number。
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们必须使积极结果平等的直接约束，然后你可能会在长期内伤害他们，另一种考虑方式是，在招聘时，比如我们需要确保招聘女性的比例与人数相等。
- en: percentage of men we hire for technical positions well if those women leave
    in。 like five six months because of a toxic workplace environment do you really。
    deserve your gold star for me that hiring quota I don't think so next is。 parity
    of false positive right so this is really like as the name suggests we're。
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在技术职位上雇佣男性的百分比，如果那些女性因为有毒的工作环境在五六个月内离开，你真的值得这个金星吗？我不这么认为，接下来是错误的积极率的平等，这正如名字所示。
- en: looking instead of like the positive outcomes we're looking at the false。 positives
    so really false positives like the model says yes but the。 actualities no and
    this is sometimes called equal opportunity and think about。 like if you think
    about like false positive right is it's false positive。
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们不是在关注积极结果，而是在关注错误的积极率，错误的积极率就像模型说“是”，但实际上“不是”，这有时被称为平等机会，想想看，错误的积极率其实就是错误的积极。
- en: over false positive plus true negative so if you want to drop that false positive。
    right now ideally you're focusing on dropping the number of false positives。 but
    if you think about it I could just drop the false positive right down by。 increasing
    the number of true negatives in the system and so I say you're a。
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在错误的积极率加上真正的负面率中，如果你想降低错误的积极率，理想情况下你应该专注于减少错误的积极数量，但如果你想想，我可以通过增加系统中的真正负面数量来降低错误的积极率。
- en: police chief and someone comes to you and says oh hey you are overly detaining。
    too many black men that the number of black men who you've arrested and not。 denying
    bail to or denying parole to who'd otherwise not go on to a refund is too。 high
    so what can you do about this how can you drop that false positive right。
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 警察局长，有人来找你说，哦，你的拘留过于严格，逮捕的黑人男性人数太多，拒绝保释或假释的人数过高，那么你能做些什么来降低这个错误的积极率呢？
- en: down well ideally you're sort of dropping the false positive you're sort of。
    granting more bail to black men who would otherwise not go on to。 refund but you
    can also just increase the number of true negatives what's the true。 negative
    so true negative is a person who is arrested they are released and they。
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 理想情况下，你是减少错误的积极率，给予更多本应不被保释的黑人男性保释，但你也可以增加真正负面的数量，真正的负面是什么？真正的负面是一个被逮捕的人，他们被释放，并且没有再次犯罪。
- en: don't go on to create a crime so who are those those are people who've been。
    at misdemeanors so I can increase the number of true negatives in the system。
    you arrest more people for misdemeanors and let them go boom that false positive。
    right goes down you're taught on you've achieved equality aren't isn't that great。
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 那些人是轻罪的犯人，所以我可以增加系统中的真正负面数量，你逮捕更多轻罪的人并释放他们，哇，错误的积极率就下降了，你被教导过，你达成了平等，这不是很好吗？
- en: you just have to arrest more black men for misdemeanors that oh that's terrible。
    and if you don't think about you just optimize that metric and not think about。
    like the social side of factors that generate these numbers you end up。 harming
    that vulnerable population we want to help。
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 你只需逮捕更多的黑人男性，针对轻罪，哦，这太糟糕了。如果你不考虑这一点，只是优化那个指标，而不考虑导致这些数字的社会因素，你最终会伤害我们想要帮助的脆弱群体。
- en: and finally last one is calibration so calibration is like really hard thing。
    to explain in layman's terms like it comes to statistical calibration and。 really
    the way you think about this is like for any event like who wins a state。 of a
    political election there's some true outcome like we know someone who wins。
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一个是校准，校准是一个非常难以用通俗语言解释的事情，它涉及统计校准，真正的思考方式是，任何事件的发生，比如谁赢得了一场政治选举，有一个真实的结果，我们知道谁赢了。
- en: someone doesn't lose and we're trying to figure like how well are we actually。
    predicting that outcome with our model and so when this is using let's say。 recidivism
    for example where people are given scores come to scores from one。 to ten or child
    protective services where you give a child is given a score of。
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 有人没有失去，我们正在弄清楚我们的模型在多大程度上实际预测了这个结果，例如使用再犯率，给人们的评分从1到10，或者儿童保护服务，孩子被给予一个评分。
- en: one to twenty representing how much dangerous child is in so like one being。
    the child not really in danger in this household or twenty being like that。 child
    is in serious danger get them out the household now and so how this works。 is
    okay CPS goes in they evaluate the household and they give this child a。
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 从一到二十表示孩子在危险中的程度，比如一分表示这个孩子在家庭中并没有真正的危险，而二十分则意味着这个孩子处于严重危险中，必须立即将他们带离家庭。
- en: score between one to twenty and if that score is high enough then of。 those
    some threshold t or which is say 15 for example they take the child out。 of the
    household because the child is in danger if they stay there and think about。 statistical
    calibration is that like really if two people get the same score。
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 每当你谈论算法公平性时，必须提到这个Compass例子，所以这是不可避免的Compass分数，从一到二十，如果得分足够高，那么在某个阈值t，例如15时，他们会把孩子从家庭中带走，因为孩子在那里的危险。
- en: that should be all that matters if you have a white child who gets a score 15。
    and a black child who gets a score 15 they should be considered to be in the。
    same danger level they should both have like a 70% chance of being harmed or。
    75% chance of being harmed if they stay in that household like you shouldn't。
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有一个白人孩子和一个黑人孩子，他们的得分都是15，那么这应该是最重要的，他们应该被认为处于同样的危险水平，他们在那户家庭中受到伤害的机会应该都是70%或75%。
- en: matter on like racial demographics for example it should just matter like okay。
    we value the score and that that score is like the final thing like we take that。
    score and make decision solely based on that score and if its models well。 calibrated
    if like okay 15 is to cut off for both and we sort of get the same。
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑统计校准，如果两个人得到了相同的分数，那么种族人口统计学应该没关系，应该只在乎，我们重视这个分数，而这个分数就是最终的依据，我们仅基于这个分数做决定，如果模型校准良好，那么15对于两者来说都是界限。
- en: percentage of children being removed from households from white households。
    versus black households and I guess the key idea is that like okay the score。
    is to mean the same thing for same people and so I guess like before from my。
    previous slides when talking about these metrics it's usually like a little。
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 这如何运作呢？CPS会进入家庭进行评估，并给这个孩子一个从白人家庭到黑人家庭被移除的儿童百分比，我想关键在于得分对于相同的人意味着相同的事情。
- en: bullet point at the bottom that says danger here's why this is bad but I don't。
    have this here and why because I have a whole dedicated slide dedicated to WUSTI。
    problem with calibration and so anytime you give a talk on algorithmic fairness。
    you have to bring up this compass example so here's the obligatory compass。
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 我想在我之前的幻灯片中讨论这些指标时，通常底部有一个小要点说危险，这就是为什么这是不好的，但我在这里没有，因为我有一个完整的幻灯片专门讨论校准的问题。
- en: example I think this is an important case study because so what is the issue。
    with compass so this is that prison and recidivism algorithm so in 2016 the。 publication
    ProPublica says that oh hey when you use compass to in order to。 decide who gets
    bail or not you're accidentally defending you're retaining。
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我认为这是一个重要的案例研究，因为关于Compass的问题是什么？这就是监狱和再犯算法，2016年出版的ProPublica表示，使用Compass来决定谁可以保释时，实际上是在保留。
- en: black defense at a higher rate you're detaining black defendants who if they。
    were released they would go on to not reoffend again and so ProPublica。 Northpoint
    which is the group that makes this compass algorithm their argument。 was that
    no our risk scores are well calibrated if you get a score of eight。
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 黑人被拘留的比例更高，拘留那些如果被释放就不会再次犯罪的黑人被告，因此ProPublica的Northpoint（该Compass算法的开发小组）争辩说，我们的风险分数是良好校准的，如果你得分8。
- en: or above we detain you so a score models are well calibrated but the issue is。
    that the underlying distribution of the scores are not so if we give scores of。
    eight or above a lot to black men and we really give it to white males for。 example
    then okay then chance to defend detaining a white male because scores of。
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 或者是我们拘留你，因此分数模型是良好校准的，但问题在于得分的基础分布并不是，如果我们给黑人男性的分数是8或以上，而我们给白人男性的分数则很少，那么很明显，拘留白人男性的机会更高。
- en: eight or above a rare for them is low but the chance of detaining a black male。
    who the score of eight or above is high because that they're more likely to have。
    those higher scores then well your model is not kind of racist and the thing about。
    this is I think it also shows an important another aspect of using these。
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 评分为八分或以上的黑人男性被拘留的机会很高，因为他们更可能获得更高的分数，因此你的模型并不是种族歧视的。而这一点也展示了使用这些模型的另一个重要方面。
- en: metrics is that you can do really well according to one of these metrics。 statistical
    calibration but do horribly compared to another metrics such as。 false positive
    rates and so just trying to like aimlessly optimize for these。 metrics will result
    in solutions that really don't actually generate well to。
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 这些指标中，你可能在某些指标上表现很好，例如统计校准，但在另一些指标上（如假阳性率）表现很糟，因此，仅仅盲目优化这些指标将导致实际上并不能产生良好结果的解决方案。
- en: the real world so what can we do and so I'm gonna propose sort of a low。
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 那么我们该怎么办？我打算提出一种低成本的解决方案。
- en: '![](img/19dc4c4a9b64c2297f4044cba9885c3c_27.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![](img/19dc4c4a9b64c2297f4044cba9885c3c_27.png)'
- en: quantitative low-tech solution which is really let's write better documentation。
    and that must be silly but I think part of the issues we had now with AI ethics
    is。 that Silicon Valley is sort of insular Silicon Valley believes it's filled
    with。 like tech geniuses who have like believed they have achieved the 13th。
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 一种定量的低技术解决方案就是撰写更好的文档，这听起来可能很傻，但我认为我们现在在人工智能伦理方面所面临的一部分问题是硅谷有些封闭，硅谷认为这里充满了技术天才，他们相信自己达到了“第十三”的境界。
- en: level of cognition by drinking their salt juice and like going in their cryobaths。
    and they don't need plebeian mortals to tell them how to do their job they will。
    take care of them by themselves they don't need government regulations they。 don't
    need outsiders to do their job and that's not true at all like I think。
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 通过饮用盐水和进入冷冻浴来提高认知水平，他们不需要平民来告诉他们如何做他们的工作，他们会自己处理，不需要政府法规，也不需要外部人士来帮助他们，这完全不是真的。
- en: better documentation allows for better communication of like how these data。
    sets are spelled how these models work and so even if you're not trained as like
    a。 cultural anthropologist or an ethicist if you can better communicate the outcomes。
    of your work better come at you like how this is trained how this is tested to。
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以想象，像汽车测试这样的例子，历史上更好的文档能够更好地传达数据集的构成以及这些模型的工作原理，因此即使你不是文化人类学家或伦理学家，如果你能更好地沟通你工作的结果，也能让人更清楚地理解你的训练和测试过程。
- en: '![](img/19dc4c4a9b64c2297f4044cba9885c3c_29.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![](img/19dc4c4a9b64c2297f4044cba9885c3c_29.png)'
- en: those people they can do the evaluation of like okay well this harm people in。
    the long run Lewis accidentally harm these groups of people you're not。 thinking
    about and so this is really cool idea of data seats for data sets and。 this is
    one by Timnet Gebru TIM for the sonographer and she basically。
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 这些人可以评估，比如这会在长期内伤害到某些人群，路易斯可能无意中伤害了那些你没有考虑到的群体。因此，关于数据集的这一很酷的想法来自于蒂姆内特·盖布鲁，她基本上。
- en: proposes like well other industries like an automobile industry or clinical。
    testing they sort of have standardizations of how they evaluate their。 models
    and we should adopt something similar for data science data sets and so。 if you
    think of the two like automobile testing for example like okay historically。
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 提出的建议是，像汽车行业或临床测试等其他行业通常会有标准化的评估模型的方法，我们应该为数据科学数据集采用类似的标准。
- en: those automobile tests were tested on like pro dummy crash has dummies were。
    pro typically adult male features and as a result in the real world because when。
    actual car automobile collisions happen women and children were extremely harmed。
    were more likely be severely injured in those and so now we have legal。
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 这些汽车测试是针对像专业假人碰撞测试而进行的，专业假人通常具备成年男性的特征，因此在现实世界中，当实际的汽车碰撞发生时，女性和儿童受到的伤害极其严重。
- en: regulations saying like okay we need to make sure you also test on a pro。 typically
    adult female dummies and children child dummies and this。 documentation I think
    will answer like a lot of collections like how is the data。 collected like is
    this data collected in a way that doesn't exclude individuals。
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有法律法规规定，必须确保也对专业的成年女性假人和儿童假人进行测试。这份文档我认为能够回答许多问题，比如数据是如何收集的，是否以不排斥个体的方式收集。
- en: and I think this is very important because I do think a lot of the ways we think。
    about machine learning is sort of focused on the model and not so much on the。
    training set we don't think about like what makes a good training set a good。
    training set and you may think okay what makes a good training set like no。
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为这非常重要，因为我确实认为我们关于机器学习的思维方式。是比较集中在模型上，而不是那么关注于。训练集，我们没有考虑到是什么使得一个好的训练集是一个好的。训练集，你可能会想“好吧，是什么使得一个好的训练集”，但不仅仅是。
- en: missing data no outliers and that really answers the question of like can I
    do。 statistical analysis on this or not it doesn't answer like did I exclude groups。
    of individuals that I should care about did I exclude trans communities I exclude。
    non-heterosexual individuals from my data set but just by the way I collected
    the。
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 缺失数据没有异常值，这确实回答了我是否可以进行。统计分析的问题，但并没有回答“我是否排除了我应该关注的个体群体，是否排除了跨性别社区，是否排除了非异性恋个体”。因为我收集数据的方式。
- en: data I do think this shifts the conversation from like the model to like。 how
    do we actually collect good data that we really not thought about before。 because
    we just sort of assume okay the data set it lives in the ether I got。 from Kaggle
    or I got from some other website but we don't really think about。
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 我确实认为这将讨论的重点从模型转向了。我们如何实际收集好的数据，这之前我们真的没有考虑过。因为我们只是有点假设“好吧，这个数据集存在于虚空中，我是从Kaggle上获得的，或者从其他网站上获得的”，但我们并没有真正考虑。
- en: like is that data set actually suitable for my problem or is it just。 conveniently
    available and next something similar for models and so this is done。 out of Google
    research by Meg Mitchell's group the Google AI fairness group and。 so what they
    propose is sort of like something similar so standardized。
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 比如，那个数据集是否真正适合我的问题，还是仅仅是。方便可用的。接下来，模型也是类似的，这项工作是由谷歌研究的Meg Mitchell团队和。谷歌AI公平性小组完成的。因此，他们提出的方案有点类似于标准化。
- en: documentation for machine learning models you document okay how was this。 model
    when you produce a model and train it before you deploy let's talk。 about how
    is it meant to be used like what are the intended use cases of this。 how was it
    evaluated and not just like okay is looking at accuracy calibration。
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习模型的文档，你记录好这个。模型是如何产生的，以及在部署之前如何训练它，让我们讨论。它应该如何使用，预期的使用场景是什么。它是如何被评估的，而不仅仅是“好吧，是看准确性校准”。
- en: or like precision but also how is it evaluated gets different demographic。 groups
    how is it evaluated gets the intersection of those demographic groups。 we just
    let's bring in intersectional analysis into how we do data science。 and machine
    learning and I think also the key thing is like what are those。
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 或者是准确度，还包括它如何在不同的人口群体中进行评估。它如何在这些人口群体的交集上进行评估。我们只是把交叉分析引入到我们进行数据科学和机器学习的方式中。我认为关键点还在于，这些。
- en: ethical concerns so if you do design a sexuality classifier what's the ethical。
    concerns with that like how could this be used to harm people or be weaponized。
    against people in countries where homosexuality is illegal and the key thing。
    about this is like more transparent model reporting will allow us to better。
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 伦理问题是什么，如果你确实设计了一个性取向分类器，伦理。问题是什么？这可能会如何被用于伤害他人，或者在同性恋是非法的国家被作为武器。关键在于，更多透明的模型报告将使我们能够更好地。
- en: communicate like how people should use our models because right now we sort
    of。 just like okay I want to do computer vision let's grab like YOLO net or。 Alex
    net and just like apply to all our problems even though like those models。 are
    not really designed for your specific problem and finally there's also。
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 沟通应该如何使用我们的模型，因为现在我们有点像是。就像“好吧，我想做计算机视觉，我们就抓取YOLO网或。Alex网，然后把它应用到所有问题上”，尽管那些模型。并不是专门为你的特定问题设计的，最后还有。
- en: this tool Deion which is an ethical checklist for data science projects so。
    this is Deion is produced by an organization called Driven Data they do。 like
    consulting with like nonprofit government organizations and they also。 run data
    science for good competition so think like Kaggle competition but with。
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 这个工具Deion是一个数据科学项目的伦理检查表。Deion由一个叫Driven Data的组织制作，他们为非营利性政府组织提供咨询，同时也。举办数据科学为善的竞赛，所以想象一下Kaggle竞赛，但带有。
- en: it with the definite emphasis on helping like some social good aspect of。 like
    helping teachers helping education helping drug treatment for example and。 what
    this tool does it creates a markdown file in your repo with a checklist。 that
    checks for different things like how the data collected was it collected in a。
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 以明确强调帮助某种社会公益的方式，例如帮助教师、帮助教育、帮助药物治疗等。这项工具可以在你的代码库中创建一个Markdown文件，包含一个检查清单，用于检查不同的事项，比如数据是如何收集的，是否以公正的方式收集，是否覆盖了所有人群，是否排除了某个群体，以及人们如何被告知他们的数据集是如何收集的。
- en: fair way did it actually cover all the groups of people in the cover did you。
    exclude a group of people and also where people in form of how their data set
    was。 collected like in four percent is important in the social sciences like you。
    can't get research approval unless you have informed consent for your。
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在社会科学中，知情同意是非常重要的，你无法获得研究批准，除非你获得了知情同意。
- en: participants but on the other hand in data science we're just able to run A/B。
    tests on our users without telling them what we're doing so and you can run some。
    pretty horrible A/B tests on people that like why the hell did you do this without。
    having to inform them and also think about data storage and I think this is。
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 参与者另一方面，在数据科学中，我们能够在不告知用户的情况下对他们进行A/B测试，因此你可以对人们进行一些相当糟糕的A/B测试，为什么你要这样做而不告知他们，同时也要考虑数据存储，我认为这是。
- en: important because we now sort of have this idea of like right to be forgotten
    so。 that if someone doesn't want their data set to be a data to be used in an。
    algorithm they should be able to say I don't want my data to be used here。 delete
    it from your records and you should be able to do that easily you。
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 这很重要，因为我们现在有了被遗忘的权利的概念，如果有人不想让他们的数据集被用于算法中，他们应该能够说我不想让我的数据被使用，从你的记录中删除它，并且你应该能够轻松做到这一点。
- en: should be able to comply with that that's part of GDPR and also when you deploy。
    the model how do you think about taking down if it does come out of the okay。
    there's a new story that says like oh you're models harming these groups of。 people
    how do you take your model out of production I think it's important to。
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 应该能够遵守这一点，这也是GDPR的一部分，当你部署模型时，你如何看待如果模型出现问题该如何下架？有个新闻说你的模型正在伤害这些群体的人，你如何将模型从生产环境中移除，我认为这很重要。
- en: think about those beforehand and I do want to know that like these things like
    the。 model cars the data sheets and this checklist they're not foolproof like
    it's。 not a foolproof way to prevent you from harming groups of people that you
    don't。 want to harm but I do think it's like a sort of good first step it sort
    of helps。
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 提前考虑这些问题，我想说这些模型、数据表和检查清单并不是万无一失的，这并不能完全防止你伤害那些你不想伤害的群体，但我认为这算是一个好的第一步，有助于。
- en: us be a little less stupid when we do data science and finally I want to give。
    it like a special shout out to the AI now Institute so this is a NYU research。
    Institute as looks out like sort of like the cultural and societal impact of AI。
    and so they hold this annual symposium on ethics organizing accountability and。
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在进行数据科学时稍微聪明一些，最后我想特别提到AI Now研究所，这是纽约大学的一个研究机构，关注AI的文化和社会影响，因此他们每年举办关于伦理、组织和问责的研讨会。
- en: they also recently produced this paper talking about the diversity crisis in
    AI。 so a lot of the things of all the part issues with AI now sort of like we。
    don't really have diversity we don't have different voices or different。 opinions
    in the room and this makes a lot of ad hoc solutions to like。
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 他们最近也发表了一篇关于AI多样性危机的论文，所以与AI相关的许多问题，如我们缺乏多样性、缺乏不同的声音或意见在场，这导致很多临时解决方案。
- en: algorithmic bareness like kind of mood okay if you talk about okay instead of。
    just having an awkward an automated algorithm it just have a human in loop。 to
    check decision well if most of your humans in the loops are like straight。 white
    men they don't really understand the lived experience of non-straight white。
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你讨论的是算法的偏见，而不是仅仅依赖于一个自动化算法，而是让一个人来参与检查决策，那么如果你参与检查的大多数人都是白人男性，他们并不了解非白人群体的生活经历。
- en: with men and they won't be able to check for decisions that your algorithm makes。
    that they don't realize that this is an issue for other people who are different。
    from them and so I think it's important because it also talks about like some。
    of the more technical long-term technical consequences of not having。
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 与男性在一起，他们将无法检查你的算法所做的决策。他们没有意识到这是其他不同人群的问题。因此，我认为这很重要，因为它也涉及到一些。没有多样性的长期技术后果。
- en: diversity in AI and machine learning and data science and so that's really an。
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在人工智能、机器学习和数据科学中缺乏多样性，这真的很。
- en: '![](img/19dc4c4a9b64c2297f4044cba9885c3c_31.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![](img/19dc4c4a9b64c2297f4044cba9885c3c_31.png)'
- en: talk here are all the papers I referenced throughout the talk if you want to。
    check them out afterwards and yeah I hope this talk was engaging I hope you all。
    learn something and if you have the time I hope you check out the other talk on。
    measuring model fairness， you， (applause)， [APPLAUSE]。
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是我在演讲中提到的所有论文，如果你想要的话，可以在之后查看。是的，我希望这次演讲能引人入胜，希望你们都能学到一些东西。如果有时间，我希望你们去看看关于衡量模型公平性的另一场演讲，你们，（掌声），[掌声]。
