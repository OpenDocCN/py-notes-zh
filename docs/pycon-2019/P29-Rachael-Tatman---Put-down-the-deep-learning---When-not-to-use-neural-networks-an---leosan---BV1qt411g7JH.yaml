- en: P29：Rachael Tatman - Put down the deep learning - When not to use neural networks
    an - leosan - BV1qt411g7JH
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: P29：Rachael Tatman - 放下深度学习 - 什么时候不该使用神经网络 an - leosan - BV1qt411g7JH
- en: Welcome everyone。 Just as a reminder， please make sure to put all of your devices
    in silence。 Otherwise you will have a glare from me。 We are welcoming Rachel Tappman。
    She will be。
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 欢迎大家。请记得将所有设备调成静音。否则我会对你们有个瞪眼。我们欢迎 Rachel Tappman。她将会。
- en: '![](img/fce9dc1b3b018e80cfb5fd237a9adbf0_1.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fce9dc1b3b018e80cfb5fd237a9adbf0_1.png)'
- en: presenting put down your deep learning when not to use neural networks and what
    to do， instead。 [Applause]， Hello。 I just wanted to make sure my phones run silent
    essentially since they are right。 next to that microphone。 Good afternoon everybody。
    Thank you for coming。 I am Rachel Tappman。 I am a data scientist， advocate。 I
    am trying to say that I have a job title that says what I do。
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 介绍什么时候不该使用深度学习，应该用什么来替代。[掌声]，你好。我只是想确保我的手机调成静音，基本上因为它们就在麦克风旁边。大家下午好。谢谢你们的到来。我是
    Rachel Tappman。我是一名数据科学家，也是倡导者。我想说我有一个工作头衔来描述我所做的事情。
- en: I am trying， this one out。 Let me know if it is helpful for you guys at Kaggle。
    I am very happy to。 talk about Kaggle but that is not what I am here to talk about
    today。 Today I am here。 to talk about deep learning and more importantly alternatives
    to deep learning。 Just to get。 a sense of the room before I start out， who has
    heard of deep learning？ Yay。 Pretty much， everybody。
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我正在尝试这个。请告诉我这对你们在 Kaggle 上是否有帮助。我很乐意谈论 Kaggle，但今天我不是来谈这个的。今天我来这里是为了讨论深度学习，更重要的是深度学习的替代方案。在开始之前，我想先了解一下在场的各位，谁听说过深度学习？耶，几乎所有人。
- en: Presumably that is why you are here。 Who has done some deep learning？ Anything。
    from tens or flow to poets to it is part of your job？ Okay。 I would say about
    half the， people。 Some folks are like kinders。 I probably don't need to tell you
    guys this but I think。 it is worth going over。 Deep learning is a fantastic set
    of technologies and we have。
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 可能这就是你们在这里的原因。谁做过一些深度学习？从 tens 或 flow 到 poets，这是你工作的一部分吗？好吧，我认为大约一半的人。一些人可能还像幼儿园小朋友。我可能不需要告诉你们这些，但我认为值得一提。深度学习是一套极好的技术，我们有。
- en: seen some amazing wins lately。 I have just picked a couple of examples of things
    that。 I think are particularly impressive。 OpenAI 5 is the model that just beat，
    I say， Dota 2， mods。 Professional video game players at the video game that they
    play professionally。 At。 a live sporting event for the first time。 It is very
    exciting and that happened last week。
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 最近见到了一些令人惊叹的胜利。我挑选了几个我认为特别令人印象深刻的例子。OpenAI 5 是一个刚刚战胜专业玩家的模型，我说的是 Dota 2。在这款他们职业比赛的电子游戏中，首次在现场体育活动中出现。这非常令人兴奋，这发生在上周。
- en: Previously， AlphaGo Zero was a Google model that beat the best Go players。 It
    is a board。 game in the world at the board game that they play professionally。
    Another really exciting。 instance of computers being better than humans at a very
    narrow task。 There is a robot that。 came out relatively recently or the paper
    came out recently and it looks in a bin of。
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 之前，AlphaGo Zero 是一个谷歌模型，打败了最好的围棋玩家。围棋是他们职业比赛的棋盘游戏。又一个令人兴奋的实例，计算机在一个非常狭窄的任务上优于人类。最近出现了一个机器人，或者说论文最近发表，它可以在一个箱子里。
- en: things and it picks out things and it tosses them into other bins。 It sorts
    by throwing。 which seems really dumb and like a toddler could do it but it is
    actually very hard。 computationally。 That is an impressive finding。 This particular
    newspaper is a little bit。 hypey and the actual paper title is towards human parity
    and it has got juiced up in the。
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 东西，它能挑出东西并将其扔到其他箱子里。它通过投掷来分类，这看起来很傻，像幼儿能做到的事，但实际上在计算上非常困难。这是一个令人印象深刻的发现。这个特定的新闻稿有点夸张，实际论文标题是“朝着人类平等迈进”，它在。
- en: PR rounds but speech recognition has gotten much better。 It is not quite at
    the point where。 we can replace human transcivers。 Thank you。 If you want high
    quality transcriptions。 I probably would not use it for everything but we are
    getting to the point where it is。 fairly usable for most things。 It seems like
    if you are outside the field and learning。
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: PR 轮次，但语音识别已经有了很大的改善。它还没有达到可以替代人类转录员的程度。谢谢你。如果你想要高质量的转录，我可能不会把它用于所有事情，但我们正在接近可以在大多数事情上相当可用的水平。似乎如果你是外行并在学习。
- en: about it through headlines， machine learning can do anything these days and
    especially。 deep learning which means we can use deep learning to do everything。
    Not so much。 I think that being in the deep learning field right now is a little
    bit like being a paleontologist。 You tell people you are a paleontologist， I am
    assuming， I am not a paleontologist。 People。
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 关于这个问题的标题是，机器学习如今可以做任何事情，尤其是深度学习，这意味着我们可以用深度学习做一切。其实并非如此。我认为现在身处深度学习领域有点像是一名古生物学家。你告诉人们你是古生物学家，我假设，我并不是古生物学家。人们。
- en: are like， oh cool。 I think you have dinosaurs。 They are so powerful。 They are
    so big。 They。 are so exciting。 Maybe small children are impressed。 Small children
    generally aren't impressed。 when I tell them I do machine learning but maybe someday
    they will be。 You have the glamorous。 experience of what people think deep learning
    is like and then you have the less glamorous， reality。
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 像是，哦，酷。我想你有恐龙。它们太强大了。它们太大了。它们太令人兴奋了。也许小孩子会印象深刻。小孩子通常对我说我做机器学习时并不印象深刻，但也许有一天他们会。你体验到的那种光鲜亮丽是人们认为深度学习的样子，而现实却没有那么光鲜。
- en: Building deep learning model， even if you are not doing research， is slow。 It，
    is tedious。 It is painstaking。 It takes a lot of time。 It is really expensive。
    We will。 talk about how expensive and a little bit。 It is frustrating because
    there is this sense。 of uncertainty。 When you set out working on a project and
    you are building a specific， model。
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 构建深度学习模型，即使你不进行研究，也是缓慢的。这是乏味的。这是费力的。需要很多时间。这真的很昂贵。我们会谈谈有多昂贵，还有一点。这让人沮丧，因为有一种不确定感。当你开始一个项目并构建一个特定模型时。
- en: you do not know that it is going to work。 You do not know that the specific
    parameters。 you are using and the specific hyper parameters you are using are
    going to work for your problem。 You do not know what architecture is going to
    work best。 The state of the art right now。 is guess and test。 I am in no way exaggerating。
    There is a lot of deep learning models that。
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 你不知道它是否会有效。你不知道你所使用的具体参数和超参数是否适合你的问题。你也不知道哪种架构效果最好。目前的前沿状态是猜测和测试。我一点也不夸张。确实有很多深度学习模型。
- en: we do not have a deep theoretical understanding of。 We just have a lot of empirical
    results。 that show that they are good for a certain set of problems。 Which is
    really frustrating。 I have a really good example of this。 This is a passage from
    the BERT paper。 BERT is a large。 neural network based language model。 That is
    what I am looking for。 It is unsupervised。
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对这方面并没有深入的理论理解。我们只是有很多实证结果，显示它们在某些问题上是有效的。这真的让人沮丧。我有一个很好的例子。这是一段来自BERT论文的内容。BERT是一个大型的基于神经网络的语言模型。这正是我所寻找的。它是无监督的。
- en: It is basically trained on a bunch of text data。 It notices patterns more or
    less。 The。 idea is that you have your big language model。 On top of it， you have
    a little bit of fine。 tuning for your specific task where you do have labeled
    data。 It is a lot of unsupervised， learning。 It seems to be pretty transferable
    between tasks。 That is exciting。 Unfortunately。
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 它基本上是基于一堆文本数据进行训练的。它或多或少会注意到模式。其想法是你有一个大型语言模型。在它的基础上，你为你的特定任务进行了一些微调，而你确实有标注数据。这是很多无监督学习。它在任务之间似乎相当可转移。这令人兴奋。不幸的是。
- en: this is a really good paper。 It just won best paper at NACL which is one of
    the premier。 NLP natural language processing conferences。 This is state of the
    art research。 Something。 in the paper that they said that really stuck with me
    is that as they were doing this fine。 tuning building the layer on top， sometimes
    it just didn't work。 When that happened， they。
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一篇非常好的论文。它刚刚在NACL获得了最佳论文奖，这是顶尖的自然语言处理会议之一。这是前沿的研究。论文中有一句话让我印象深刻的是，他们在进行微调时，构建上层时，有时就是不奏效。当这种情况发生时，他们。
- en: just restarted it randomly。 Sometimes it did work。 You just have to run a bunch
    of models。 and figure out which one works well and use that one。 That does not
    feel super satisfying。 as a working methodology。 I like to know what works ahead
    of time。 I mentioned expense。 I know these are hard to read。 I will give you a
    link to the slides at the end。 People don't。
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我刚刚随机重启了它。有时候确实有效。你只需运行一堆模型，找出哪个效果好，然后使用那个。作为一种工作方法，这感觉并不是特别令人满意。我喜欢提前知道什么是有效的。我提到过费用。我知道这些内容很难阅读。我会在最后给你一个幻灯片的链接。人们并不。
- en: generally tell you how much it costs to train their model。 Folks are also willing
    to go on。 Twitter and do back of the envelope math。 Here is one estimate for big
    GAN which is an。 image generation model。 I think it came out in January。 You might
    have seen headlines like。 this dog doesn't exist。 A computer imagined it。 The
    estimate for back of the envelope math。
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 通常会告诉你训练他们的模型需要多少费用。人们也愿意在Twitter上进行一些粗略的计算。这是一个关于大GAN的估算，它是一个图像生成模型。我想它是在一月发布的。你可能见过这样的头条新闻：这只狗并不存在。是电脑想象出来的。粗略计算的估算是。
- en: is around $60，000 just of compute。 That does not count what you might need to
    pay to license。 the images to train it。 GPT2 is another big neural language model
    like BERT。 This one was。 bigger and trained for longer。 Again， back of the envelope
    math， maybe around $43，000 worth。 of compute。 These are by no means the biggest
    models。 I mentioned open AI5 which is the model。
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 大约$60,000，仅仅是计算费用。这不包括你可能需要支付的用于训练它的图像许可费用。GPT2是另一个像BERT一样的大型神经语言模型。这个模型更大，训练时间更长。同样，粗略估算可能在$43,000左右的计算费用。这些绝不是最大的模型。我提到的open
    AI5就是这个模型。
- en: that won Dota 2。 That trained continuously for 10 months。 I cannot tell you
    how much it costs。 I feel comfortable saying many millions of dollars。 I don't
    know about you guys in your。 relationships with your managers but if I go to my
    manager and ask for $50，000 worth。 of compute for something that might work， I
    would be encouraged to explore other approaches。
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 赢得Dota 2的那个模型，连续训练了10个月。我无法告诉你它的成本。我觉得可以说是几百万美元。我不知道你们和经理的关系如何，但如果我去找我的经理要求$50,000的计算资源用于可能有效的东西，我会被鼓励去探索其他方法。
- en: to that problem。 So when would I actually use deep learning？ I'm going to keep
    this talk。 very high level and talk about my experiences。 I'm not going to go
    into the math。 There are。 many resources out there and feel free to hit me up
    afterwards if you want recommendations。 I would use deep learning for a problem。
    If one， a human can do it in less than a second。
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 那么我到底什么时候会使用深度学习？我将保持这个演讲在一个非常高的层面，谈谈我的经历。我不会深入数学部分。外面有很多资源，如果你想要推荐，可以随时找我。我会在一个问题上使用深度学习，如果一个人能在不到一秒钟内完成。
- en: The longer it takes you to do something， especially something cognitive。 the
    more complex it probably， is， the more data I'm going to need and the bigger the
    model I'm going to need to train。 in order to approximate it。 So one second is
    just sort of my personal， litmus test right now。 I have to have a high tolerance
    for weird errors。 Neural networks， have neural in the name。
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 你花费在某件事情上的时间越长，尤其是一些认知上的事情，它可能越复杂，我将需要更多的数据以及更大的模型来训练，以便进行近似。所以现在我个人的试金石是“一秒钟”。我必须对奇怪的错误有很高的容忍度。神经网络的名字中有“神经”二字。
- en: They are not cognitive models。 They are not telling you what a human， would
    do in this situation。 So a really good example of this is humans use silhouettes，
    when identifying objects。 That's just sort of a cognitive bias that we have。 Most
    computer。 vision models do not use silhouettes。 They use collection of pixels。
    So are they much。
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 它们不是认知模型。它们并不告诉你人在这种情况下会怎么做。一个很好的例子是人类在识别物体时使用轮廓。这只是我们的一种认知偏见。大多数计算机视觉模型不使用轮廓。它们使用的是像素集合。那么它们在这个问题上能有多大作用呢？
- en: easier to fool with textures than they are with silhouettes？ So humans have
    a very gestalt。 view if you have taken much psychology。 Neural networks don't。
    I would also only use a neural。 network if I didn't need to explain myself。 So
    if I were say working in finance and needed。 to show with a certain degree of
    confidence that I did not use race or any correlative。
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 比起轮廓，使用纹理更容易欺骗他们吗？所以人类有一种非常整体的视角，如果你学过心理学的话。神经网络则没有。我也只会在不需要解释自己的情况下使用神经网络。如果我在金融行业工作，并需要以某种程度的信心展示我没有使用种族或任何相关因素。
- en: race to decide whether or not to give you a mortgage， I can't do that with a
    neural network。 And we've been doing as a field， not me personally。 a lot of work
    on interpretability and explainability， and it is getting much better。 but we're
    not to the same point that we are already with， regression models。
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 种族因素决定是否给你按揭贷款，我无法用神经网络做到这一点。作为一个领域，我们正在进行大量关于可解释性和可解释性的工作，虽然进展显著，但我们还没有达到与回归模型同样的水平。
- en: I would also only use neural networks if I had a large quantity of labeled，
    data。 I say more than 5。000 labeled items per class that is very much a low ball。
    You。 can get away with less if you already have a trained model and you can do
    transfer learning。 But still， you need a lot of data and all of the research so
    far has shown the more data。
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我有大量标记数据，我也只会使用神经网络。我说每个类别超过5000个标记项，这个数字实际上是偏低的。如果你已经有一个训练好的模型，可以进行迁移学习，那可以少一些。但是，依然需要大量数据，迄今为止所有的研究都表明，数据越多。
- en: the bigger the model， the more compute， the better the results。 And finally，
    I'd only。 do it if I had a lot of time and money。 So training models takes time，
    especially if。 I'm doing model comparison， I'm doing any sort of hyperparameter
    search that takes much。 more time because I have to train multiple models and
    see which one is better。 And money。
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 模型越大，计算越多，结果越好。最后，我只有在有大量时间和金钱的情况下才会这样做。所以训练模型需要时间，尤其是如果我在进行模型比较，做任何形式的超参数搜索，这会花费更多的时间，因为我必须训练多个模型，看哪个更好。而且金钱。
- en: So for data annotation， if I need additional labeled data， if I need someone
    to go through。 and maybe do some sort of human in the loop tuning， that costs
    money。 And compute， of， course。 also costs money。 And we talked about the orders
    of magnitude that we're talking。 about there for a really large model。 So deep
    learning。 You need a lot of time。 You。
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 所以对于数据标注，如果我需要额外的标记数据，需要有人来审核，也许还需要进行某种人机交互的调整，这会花费金钱。当然，计算也需要花费金钱。我们讨论过，对于一个真正大的模型，所需的量级是相当可观的。所以深度学习需要大量的时间。你。
- en: need a lot of money。 You need a lot of data。 Some of you may be in that position。
    Fantastic。 go forth and deep learn。 If you are not， you have lots of options。
    I'm not going to cover。 the entire fields of statistics and machine learning today，
    but I am going to talk about。 three different types of models。 So in particular
    regression， tree-based models， tree-based models。
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 需要大量的资金。你需要大量的数据。有些人可能处于这种情况。太棒了，继续深入学习。如果你不是，还有很多选择。我今天不会覆盖整个统计学和机器学习的领域，但我会谈论三种不同类型的模型。特别是回归，基于树的模型，基于树的模型。
- en: didn't fit well in my matrix， and then distance-based models。 Let's start with
    regression。 So if you， have done much machine learning， I'm hoping you are already
    familiar with regression。 Quick， show of hands。 Who here has never learned about
    regression， either in a blog post？
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在我的矩阵中没有很好地拟合，然后是基于距离的模型。让我们从回归开始。所以如果你对机器学习有一定了解，我希望你已经熟悉回归。快速举手。这里谁从未在博客文章中学习过回归？
- en: I'm really， really， really happy to hear that。 I talk to a lot of aspiring data
    scientists and people。 who want to get into the field who started with deep learning
    and aren't familiar with。 other methods like regression。 That's part of the reason
    I put this talk together。 I。 love regression。 I'm a big regression fan girl。 In
    regression， you do need to do a little。
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我真的非常非常高兴听到这一点。我和很多有抱负的数据科学家交流，他们想进入这个领域，却从深度学习开始，对其他方法如回归不熟悉。这也是我组织这个演讲的部分原因。我喜欢回归，我是个大回归粉。在回归中，你确实需要做一些。
- en: bit of hands-on pointing of the model in the direction that you want it to go。
    So you need。 to pick the family of the function you'll use to model your data。
    In this case， we have。 the simplest case of linear regression where the thing
    on the x-axis， whatever it is， has。 a linear relationship with the thing on the
    y-axis， whatever that is。 This is just a picture。
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 需要一点动手操作，将模型指向你希望它发展的方向。所以你需要选择用于建模数据的函数家族。在这种情况下，我们有最简单的线性回归，x轴上的事物，无论是什么，和y轴上的事物，无论是什么，之间有线性关系。这只是一幅图。
- en: of a regression that's in the public domain。 And there's really a lot of kinds
    of regression。 models out there， and unlike with most types of neural networks，
    we have a really principled。 understanding of what types of problems these are
    going to be good for。 So if you're using。 regression， you can basically sit down
    and work through a flowchart to fit the specific。
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 有一个在公共领域的回归模型。实际上，有很多种回归模型，与大多数类型的神经网络不同，我们对这些模型适用的具体问题有相当原则性的理解。因此，如果你使用回归，你基本上可以坐下来通过流程图来拟合特定的。
- en: type of regression that's going to work best for your problem， which is really
    nice。 Some really big advantages of regression， fast-to-fit， much faster-to-fit
    than neural， networks。 especially if you're working with a well-optimized library。
    The Python regression。 libraries tend to vary wildly， so you might want to do
    a little bit of shopping around。
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 一种最适合您问题的回归类型，这真的很好。回归有一些非常大的优势，拟合速度快，比神经网络要快得多，尤其是当您使用一个经过良好优化的库时。Python的回归库差异很大，因此您可能需要多看看。
- en: It works well with small data。 I've done regression on 8，000 data points and
    learned something。 useful and interesting。 And it's also really easy to interpret，
    and I'll walk through an。 example of interpreting it a bit， but it looks like
    most of you are already familiar。 with regression， so that might be a little bit
    redundant。 There are some downsides。 You。
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 它在小数据集上表现良好。我在8,000个数据点上做了回归，并学到了有用和有趣的东西。而且它也很容易解释，我会举一个解释的例子，但看起来你们大多数人已经对回归很熟悉了，所以这可能有点多余。有一些缺点。你。
- en: need to do a little bit more data preparation than for some other methods。 If
    there are two。 variables that are strongly correlated， you might want to spend
    some time， either creating。 a feature based on those models or correcting because
    you're going to have a standard error。 a standard error， and a count of multi-colinearity，
    classic machine learning interview question。
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他一些方法相比，您需要做更多的数据准备。如果有两个变量强相关，您可能需要花一些时间，要么基于这些模型创建一个特征，要么进行修正，因为您会有一个标准误差，一个标准误差和多重共线性的计数，这是经典的机器学习面试问题。
- en: And they also require validation。 So regression models are based on strong assumptions
    about。 the distribution of the data points or the distribution of the errors，
    and if you are。 using a regression model， you need to check that those assumptions
    are true。 Otherwise。 you'll get numbers out。 They just won't mean anything。
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 它们还需要验证。因此，回归模型基于对数据点分布或误差分布的强假设，如果您使用回归模型，您需要检查这些假设是否成立。否则，您会得到一些数字，它们根本没有意义。
- en: My personal favorite type of regression model is mixed effects regression。 If
    I had to。 take a single machine learning model to use for the rest of my life，
    honestly， it would。 probably be mixed effects regression。 And this is a little
    example of Simpson's paradox。 So who's familiar with Simpson's paradox？ Oh， not
    everybody。 Excellent。 So the idea with。
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我个人最喜欢的回归模型是混合效应回归。如果我必须选择一个机器学习模型来用一辈子，老实说，可能就是混合效应回归。这是一个辛普森悖论的小例子。那么谁熟悉辛普森悖论？哦，不是每个人。很好。那么这个想法是。
- en: Simpson's paradox is if you look at all the points together as a group， you
    see one trend。 But if you divide the groups into subgroups that are meaningful
    for your specific problem。 you see the opposite trend。 So here， for example， let's
    say that the x-axis is a amount of time。 spent playing an educational video game，
    and the y-axis is test scores。 And we want to see。
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 辛普森悖论是，如果您将所有点作为一个组来看，您会看到一种趋势。但是如果将组划分为对您特定问题有意义的子组，您会看到相反的趋势。因此，在这里，例如，假设x轴是花费在玩教育视频游戏上的时间，y轴是测试分数。我们想看看。
- en: if playing this video game improves people's test scores。 If we look at all
    the students。 so let's see each point as a student， we will see a negative trend。
    So the more time you。 spend playing the video game， the worse your test scores
    are。 I'm asking you to imagine。 axes and axes labeled here。 But if we break things
    down by classroom where each of the， classes。
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 如果玩这个视频游戏能提高人们的测试分数。如果我们查看所有学生，让我们把每个点视为一个学生，我们将看到一个负趋势。因此，您花在玩视频游戏上的时间越多，测试分数就越低。我请您想象一下这里标记的坐标轴。但如果我们按教室来划分，每个班级。
- en: each color of students is a different classroom， we see the opposite trend。
    So it。 might be that different classes patterned together differently for other
    reasons。 So。 maybe some teachers were given an easier version of the test， or
    maybe the classes were。 divided by ability in the particular subject area that
    we're looking at。 So when we model。
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 每种颜色的学生是一个不同的课堂，我们看到相反的趋势。所以可能是不同的班级出于其他原因以不同的方式组合在一起。所以，也许一些老师给了一个更简单的测试版本，或者也许这些班级是按能力划分的，针对我们正在研究的特定学科区域。因此，当我们建模时。
- en: each class individually， we see that spending more time playing the video game
    actually improves。 our test scores。 And with mixed effects regression， we can
    really easily pull out these effects。 And that would be the random effect here
    would be which group you are assigned to， or which。 classroom you are in。 I'm
    not going to show you a whole bunch of code here。 The main thing。
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 单独的课堂，我们看到花更多时间玩视频游戏实际上改善了。我们的考试成绩。通过混合效应回归，我们可以非常轻松地提取这些效应。随机效应在这里是你被分配到哪个组，或者你在什么课堂上。我不会给你展示一堆代码。主要是。
- en: I want you to get from my code samples is that all of these models are very
    easy to， implement。 They don't take a lot of code， even a very minimal TensorFlow
    or Keras deep。 learning model is going to be more code than this。 So I'm using
    the stats model library， module。 sorry。 And I've used the mix L M method。 And
    here I am modeling the chance of being。
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望你从我的代码示例中获得的是，所有这些模型都非常容易。实现。它们不需要很多代码，甚至一个非常简单的TensorFlow或Keras深度。学习模型的代码量都会超过这个。所以我在使用统计模型库，模块。抱歉。我使用了混合线性模型方法。在这里，我正在建模被录取的机会。
- en: admitted to graduate school based on your GRE， which is a standardized test
    for people。 who are interested in going to graduate school。 And TOEFL score， which
    is a test of English。 is a foreign language。 And then I am grouping things， so
    I'm using as a random variable here。 university rating。 So I think that maybe
    really prestigious universities are going to pattern。
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 所以希望每个人都有。根据你的GRE被录取到研究生院，GRE是一个标准化考试，适合有意向上研究生院的人。TOEFL分数是英语考试。是外语测试。然后我在分组，所以我把大学评级作为随机变量。在这里。我认为也许真的有声望的大学会形成模式。
- en: together and maybe like smaller regional universities are likely to pattern
    together。 And then I。 fit my model。 And that's all there is to it。 I mentioned
    that regression models were particularly。 easy to interpret。 And here is the model
    output that we can use to answer specific questions。 So if a student came to me
    and was like， hey， I have a limited amount of time to study。
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 一起，或许像小型地区大学更有可能形成共同模式。然后我。适合我的模型。就是这样。我提到过回归模型特别。容易解释。这里是我们可以用来回答具体问题的模型输出。所以如果有学生来找我，像是，嘿，我有有限的学习时间。
- en: should I focus on my GRE or should I focus on my TOEFL exam？ By looking at the
    coefficients， here。 I can see that every one point increase in GRE score increases
    your chance to admit。 to graduate school by half a percent。 And every one point
    increase in TOEFL score increases。 your chances of admitted to graduate school
    by 7/10 of a percent。 So assuming that one。
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我应该专注于我的GRE，还是专注于我的TOEFL考试？通过查看这里的系数，我可以看到GRE每增加一分，进入研究生院的机会就增加了0.5%。而TOEFL每增加一分，进入研究生院的机会就增加了0.7%。所以假设每个。
- en: unit of studying is going to raise your score in either test by the same amount，
    I suggest。 focusing on the TOEFL score。 So that's a clear actionable recommendation
    I can make based。 on the model output that I have。 And if this was a business
    setting and people were like， hey。 should we focus on， you know， I can't come
    up with a good example right now， I could。
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 学习的单位将以相同的量提高你在任何一项考试中的分数，我建议。专注于TOEFL分数。所以这是一个明确可操作的建议，我可以根据我所拥有的模型输出做出。如果这是一个商业环境，人们像是，嘿。我们应该关注，嗯，我现在想不出一个好的例子，我可以。
- en: tell them based on the features that I had。 So regression modeling takes some
    time。 You。 need to do a little bit more hands on stuff。 You need to do your validation。
    You probably。 need to do some additional data cleaning。 Does not take that much
    money， doesn't need。 a lot of compute and can work with pretty small data sets。
    Trees。 So hopefully everyone has。
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我拥有的特征告诉他们。所以回归建模需要一些时间。你。需要做一点更动手的事情。你需要做验证。你可能需要进行一些额外的数据清理。并不需要花费太多资金，不需要。大量计算，可以处理相当小的数据集。树。
- en: seen something like a decision tree at some point。 Does anyone not run across
    tree structures。 at some point？ Okay， I see a simple hands。 So the idea with a
    tree structure and specifically。 a decision tree is at each node you look at one
    feature for each of your items and depending。 on the value of that feature you
    decide which path to go down。 And when you go down that。
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在某个时候见过决策树。有没有人没有遇到过树结构？好吧，我看到简单的举手。所以树结构的想法，特别是决策树是在每个节点查看你的每个项目的一个特征，并根据该特征的值决定走哪条路径。当你沿着那条路径走时。
- en: path the next node has another feature and you based on the value of that feature。
    So basically。 you are recursively cutting your decision region into smaller and
    smaller chunks。 Nobody。 uses trees。 Everybody uses forests。 So random forests
    are ensemble models。 They can bind。 a lot of different decision trees together
    into a single model。 They are extremely popular。
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 路径下一个节点有另一个特征，你根据该特征的值进行划分。所以基本上，你是在递归地将决策区域切分成越来越小的部分。没有人使用树。大家都使用森林。因此，随机森林是集成模型。它们可以将许多不同的决策树绑定在一起形成一个单一模型。它们非常受欢迎。
- en: on Kaggle。 If you're in the machine learning community you might associate random
    forests。 with Kaggle。 And from 2010 to 2016 about two thirds of all Kaggle competition
    winners used。 random forests less than half used some form of deep learning。 And
    random forests continue。 to do very well today。 They also tend to have better
    performance than logistic regression。
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在Kaggle上。如果你在机器学习社区，可能会将随机森林与Kaggle联系起来。从2010年到2016年，大约三分之二的Kaggle比赛获胜者使用了随机森林，而不到一半的人使用某种形式的深度学习。而且随机森林今天继续表现良好。它们的表现通常比逻辑回归更好。
- en: especially for classification。 There is a paper there you can read if you're
    interested。
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 特别是对于分类。这里有一篇论文，你可以阅读，如果你感兴趣的话。
- en: '![](img/fce9dc1b3b018e80cfb5fd237a9adbf0_3.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fce9dc1b3b018e80cfb5fd237a9adbf0_3.png)'
- en: And they work really well。 Some benefits and drawbacks。 You don't need to do
    a lot of data。 cleaning or model validation。 You can pretty much shove anything
    into a random forest。 You。 don't need to convert your categorical variables。 It
    will consume them and give you a model。 And there are a lot of really easy to
    use packages with a really fantastic developer。
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 它们的效果非常好。有一些优点和缺点。你不需要做大量的数据清理或模型验证。你几乎可以把任何东西放入随机森林。你不需要转换你的分类变量。它会处理这些变量并给你一个模型。而且有很多非常易于使用的包，开发者也非常出色。
- en: experience that are also very well optimized。 So XGBoost， which I like， light
    gbm， cat， boost。 There is going to be a new one in the next scikit， release candidates。
    You can。 check that out if you're interested in yet another very good option。
    I'm sure。 Some drawbacks。 They are very easy to overfit。 You can imagine if you're
    building a tree， the natural end。
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 这些经验也得到了很好的优化。所以XGBoost，我喜欢，light gbm，cat boost。在下一个scikit发布候选版本中将会有一个新模型。如果你对另一个非常好的选择感兴趣，可以去看看。我确信会有一些缺点。它们非常容易过拟合。你可以想象如果你在构建一棵树，自然的结束。
- en: case would be you had one data point per leaf and then you have perfectly described。
    the distribution of your training data， which is the definition of overfitting。
    So to get。 around that people will usually trim and remove the bottom couple layers
    from their tree。 They're also really sensitive to differences between data sets。
    And this is a little bit， blown out。
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 一个例子是你在每个叶子节点有一个数据点，这样你就完美地描述了训练数据的分布，这就是过拟合的定义。为了避免这种情况，人们通常会修剪并去掉树的底部几层。它们对数据集之间的差异非常敏感。这一点有点夸大了。
- en: But this is actually the decision region of a single decision tree。 And it sort。
    of looks like a plaid。 And you can imagine that if there's a point in one of those
    plaid stripes。 of the opposite color， then a tree trained on that data set would
    actually have that。 stripe be a different color， if that makes sense。 It's a little
    bit hard to see。 They're。
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 但这实际上是单棵决策树的决策区域。它看起来有点像格子。如果在那些格子条纹中有一个不同颜色的点，那么在该数据集上训练的树的那条条纹将会是不同的颜色，如果这样理解的话。这一点有点难以看清。
- en: also less interpretable than regression。 You can get information out。 You can
    get feature。 importance out， ranked feature importance pretty easily。 But it's
    harder to say， hey。 if I change this， what would change and tweak the knobs in
    quite the same way。 And they can。 also require a lot more compute and training
    time。 So I mentioned a lot of well optimized， packages。
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 也比回归模型更难解释。你可以获取信息。你可以得到特征的重要性，排名特征重要性也很简单。但很难说，如果我改变这个，会有什么变化，并以相同的方式调整参数。而且，它们可能需要更多的计算和训练时间。所以我提到了一些优化良好的包。
- en: But ensembles require building a lot of models。 And for every additional model。
    there's additional training time involved as a cost。 And again， a little code
    example。
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 但集成模型需要构建许多模型。每增加一个模型，所需的训练时间也会增加，这是一种成本。再次附上一个代码示例。
- en: '![](img/fce9dc1b3b018e80cfb5fd237a9adbf0_5.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fce9dc1b3b018e80cfb5fd237a9adbf0_5.png)'
- en: I happen to like XGBoost。 One thing that's really nice about XGBoost is that
    the default， parameter。 hyperparameter settings are very reasonable for the vast
    majority of problems。 So you can pretty much just use XGBoost out of the box and
    most of the time it will work。 well enough。 If you're hoping to do any kind of
    competition， you are going to need to do。
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我恰好喜欢XGBoost。XGBoost的一个很好的地方是，其默认参数和超参数设置对绝大多数问题来说非常合理。所以你几乎可以直接使用XGBoost，而且大多数时候它都能很好地工作。如果你希望参加任何形式的比赛，你需要进行。
- en: some tuning。 But it's a really good starting point。 So tree based methods take
    some time。 especially if you have a big ensemble， a big data set， and you want
    to do maybe some cross。 validation， some jack knifing， some bootstrapping to help
    avoid that overfitting to datasets。 Again。 not a lot of money。 They don't take
    that much compute。 And they do need some data。
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 进行一些调优。但这是一个非常好的起点。因此，基于树的方法需要一些时间，特别是如果你有一个大型集成、大数据集，并且你想进行一些交叉验证、切片、引导来帮助避免对数据集的过拟合。再说一次。这不会花很多钱。它们对计算的要求不高。而且确实需要一些数据。
- en: and a little bit more than for regression。 Again， because you're going to want
    to do more。 rigorous cross validation。 And finally， distance based methods。 So
    this is a very vague term。 that I've used to group together a large class of methods
    where the general idea is。 the closer some things are in a projection of feature
    space， whatever feature space you're， using。
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 并且比回归多一点。再说一次，因为你会想要做更严格的交叉验证。最后，基于距离的方法。所以这是一个非常模糊的术语，我用来将一大类方法归为一类，其中一般的想法是，在特征空间的投影中，某些事物越接近，无论你使用的是什么特征空间。
- en: the more likely they are to be in the same group or to be related to each other。
    So I have a little example of K nearest neighbors up there。 You decide the value
    of a point based。 on the whatever number nearest neighbors。 So majority vote。
    Gaussian mixture models。 which is more of a clustering thing， which suggests that
    any distribution of points。
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 它们越可能处于同一组或彼此相关。因此，我这里有一个关于K近邻的小例子。你根据最近邻的数量来决定一个点的值。所以采取多数投票。高斯混合模型，更多的是一种聚类方法，表明任何点的分布。
- en: is going to be a mixture of different Gaussians。 And support vector machines，
    which I like。 to think of as a cat that doesn't want to be petted。 So the line
    is trying to be as。 far away from all the data points as possible。 Or support
    vector regression， which is sort。 of the opposite where the cat wants to be as
    close to as many points as possible。 But。
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 将会是不同高斯的混合。而支持向量机，我喜欢将其想象成一只不想被抚摸的猫。所以这条线尽量远离所有数据点。或者支持向量回归，正好相反，猫想尽可能靠近尽可能多的点。但。
- en: the points are hands， I guess， in this metaphor。 So benefits and drawbacks。
    Support vector。 machines in particular work very well with small data sets。 You
    need a minimum of four， points。 They also tend to be very fast to train。 In my
    experience， at least in order。 of magnitude faster than a regression model on
    the same data。 Overall accuracy， there。
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 这些点在这个隐喻中是手。所以利益和缺陷。支持向量机特别适合小数据集。你至少需要四个点。它们的训练速度也相对较快。根据我的经验，至少在同样数据上，比回归模型快一个数量级。总体准确度。
- en: are other methods that tend to do better。 But for especially quick and dirty
    modeling， they're。 perfectly fine。 They're good at classification if you want
    to do estimation specifically with。 support vector machines。 It's a little bit
    slower。 So you might want to use regression， for that。 Mostly these days people
    tend to use them on sombles。 But it's a nice fast。
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 还有其他方法通常表现更好。但是对于特别快速和粗略的建模，它们完全可以。它们在分类方面表现良好，如果你希望具体使用支持向量机进行估计。这会慢一点。所以你可能想用回归来处理。大多数时候，现在人们倾向于在样本上使用它们。但它是一个很快的。
- en: first pass for a program， for a problem。 If you're sitting in a meeting and
    someone's like， hey。 is this predictable？ You can just， you know， open our pipe
    on or whatever you're， using quickly。 Run a few lines of code and tell them very
    quickly。 And again， here is。 just the code that you need to train this。 In the
    code sample I'll give you， I've done。
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个程序的第一次尝试，面对一个问题。如果你坐在会议上，有人问：“嘿，这可预测吗？”你可以快速打开你正在使用的工具，跑几行代码，迅速告诉他们。而且，这里就是你需要训练的代码。在我给你的代码示例中，我已经完成了。
- en: all the pre-processing beforehand and the same pre-processing was done for all
    three models。 And here I'm using support vector regression because I'm doing regression
    in all of my。 examples just so they're parallel。 All right， so distance mace measures。
    Very little time。 Very little money。 Very little data。 They're extremely lightweight。
    So what method should， you use？
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在此之前的所有预处理，对于这三种模型都是相同的。我在这里使用支持向量回归，因为在我的所有示例中我都在进行回归，这样可以保持平行。好吧，所以距离基础的度量。耗时极少。花费极少。数据也很少。它们非常轻量。那么你应该使用什么方法呢？
- en: And for some reason my shruggy emoji isn't showing up even though other emojis，
    have mysterious。 So it depends on what you have a lot of。 If you have a lot of
    time data， and money。 go ahead and use deplaying。 If you don't have any of those
    things， maybe， distance space learning。 But I have been a little bit disingenuous。
    There is a hidden。
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 不知何故，我的耸肩表情符号没有显示出来，尽管其他表情符号却神秘地存在。所以这取决于你拥有多少。如果你有大量的时间数据和资金，尽管去使用延迟。如果你没有这些，或许可以考虑远程学习。但我有点不太诚实。其实是隐藏的。
- en: fourth column that I haven't kind of been mentioning。 And that's performance。
    So what。 is sort of the ideal maximum？ How well can this model do in the best
    case？ And the fact。 of the matter is based on empirical evidence right now it
    looks like deep learning will。 perform the best on a given status at given sufficient
    time， money and compute。 Next。
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一个我之前没有提到的第四个维度，那就是性能。那么，什么才是理想的最大值？在最佳情况下，这个模型能表现得多好？根据当前的经验数据，事实是深度学习在给定足够的时间、资金和计算能力的情况下，看起来会表现得最好。接下来。
- en: at least again empirically， trees and specifically random forests。 After that
    regression and then。 finally distance base measures like SVM。 But again， there
    are things to consider other。 than whether or not you're going to get state of
    the results， state of the art results。 in NIRIPS unless you are like a Google
    research lab in which case you probably wouldn't be。
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 至少从经验上看，树结构，尤其是随机森林。在那之后是回归，最后是像支持向量机（SVM）这样的距离基础度量。但同样，还有其他因素需要考虑，而不是你是否能在NIRIPS中获得尖端结果，除非你像谷歌研究实验室那样，否则你可能做不到。
- en: here anyway。 And if I were going to give each of these model types a little
    high school。 superlative， I would vote deep learning the most powerful， the most
    flexible。 Regression。 the most interpretable。 If you need to know why， if you
    need to answer what if questions。 I'd go with regression。 Trees， the user friendliest，
    especially if you're just sitting down to。
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 不管怎样。如果我要给每种模型类型一个高中超级奖项，我会投票给深度学习，认为它是最强大、最灵活的。回归是最易解释的。如果你需要知道原因，或者需要回答“如果...会怎样”的问题，我会选择回归。树结构是用户最友好的，尤其是当你刚开始学习的时候。
- en: do classification。 And then distance base measures the most light weight because
    they're fast。 and you don't need a lot of data。 And just a final caveat， data
    science does not equal。 deep learning。 I know that outside of the field that's
    what we've been communicating。 out because it's the most exciting。 But you can
    do data science without deep learning。
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 进行分类。然后距离基础的度量是最轻量级的，因为它们速度快，并且你不需要很多数据。最后的警告是，数据科学不等于深度学习。我知道在这个领域之外我们一直在这样沟通，因为这是最令人兴奋的。但你可以在没有深度学习的情况下进行数据科学。
- en: It's really okay guys。 It's extremely powerful and it's not for everything。
    So I would like。 to encourage you all in all aspects of your life to avoid being
    a person with a hammer。 So there's a saying to a person with a hammer everything
    looks like a nail。 There are many。 tools out there and they're going to be better
    for different jobs。 So explore your options。
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 真的没关系，大家。这是非常强大的，但并不适用于所有情况。因此，我想鼓励你们在生活的各个方面避免成为一个只会用锤子的人。有人说，拿着锤子的人眼里所有东西都像钉子。外面有许多工具，它们在不同的工作中会表现得更好。所以，探索你的选项。
- en: and don't be wedded to your specific favorite tool。 I'm also saying that to
    myself。 There's。 places where you can't use mixed effects regression and it's
    fine。 And just a final note。 data deep learning isn't a core skill in professional
    data science。 And it's going to depend on your particular job that you're looking
    at。 But Dan Becker。
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 不要执着于你特定的最爱工具。我也对自己这样说。有些地方你不能使用混合效应回归，这是没问题的。最后一点，深度学习在专业数据科学中并不是核心技能，这将取决于你所关注的特定工作。但丹·贝克。
- en: who is in charge of learn at Kaggle posted the other day and I thought this
    was really， interesting。 He was looking at I think it was the hacker rank who's
    hiring thread and he。 set out a 400 job posts。 Only five of them called for PyTorch
    TensorFlow or Keras which。 are all deep learning frameworks or deep learning。
    So people aren't necessarily only looking。
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 负责Kaggle学习的人前几天发布了一个帖子，我觉得这很有趣。他在查看我想是Hacker Rank招聘帖的时候，列出了400个职位。只有五个职位要求使用PyTorch、TensorFlow或Keras，都是深度学习框架。所以人们并不一定只在寻找。
- en: to hire deep learning people。 All right。 And I'm going to open it up for questions。
    Thank， you。 [ Applause ]， \>\> We have time for about five minutes of questions。
    So use the mics on both the sides。 \>\> I also post a link to my slides。 \>\>
    Hi。 So I am a social scientist working in education。 And so I guess I'm going
    to go， opposite。
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 招聘深度学习人才。好的。我将开放提问环节。谢谢大家。[掌声]，我们有大约五分钟的提问时间。所以请使用两边的麦克风。>> 我也发布了我的幻灯片链接。>>
    嗨，我是一名在教育领域工作的社会科学家。因此，我想我将走向相反的方向。
- en: Are there opportunities for deep learning in education？ I think we have a tendency。
    to want to understand why we're finding out why kids aren't being successful。
    So just。 being told at the end of like a deep learning model that doesn't help
    us as much。 Can you。 think of some applications of deep learning within maybe
    the social science sphere？ \>\> Yeah。
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 教育中是否有深度学习的机会？我认为我们有一种倾向，想要了解为什么我们发现孩子们不成功。所以仅仅在一个深度学习模型的最后被告知这一点对我们帮助不大。你能想到一些在社会科学领域内的深度学习应用吗？>>
    是的。
- en: I can think of some bad ones。 So my big worry particularly in social science。
    and things with learning and things where you might like say be allocating education
    funding。 is you're going to end up modeling a latent variable that you don't necessarily
    want。 to model like how high property taxes are and how well that correlates with
    whether。
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我能想到一些不好的情况。所以我特别担心社会科学方面，和与学习有关的事物，比如你可能在分配教育资金时，最终会建模一个你不一定想要建模的潜在变量，比如财产税的高低及其与是否相关的程度。
- en: kids get a good lunch and then how well they do in school for that。 Yeah。 I
    think it's probably the most immediately useful applications of deep learning
    in social。 sciences would be for things like annotating large image datasets。
    So if you're looking。 at what's written on the board in the classroom I guess
    and you want to get that information。
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 孩子们吃好午餐，然后他们在学校的表现。是的，我认为深度学习在社会科学中最直接有用的应用可能是标注大型图像数据集。因此，如果你正在查看教室黑板上写的内容，我想你想获取该信息。
- en: into a way that's machine readable I think that could be a good application。
    \>\> Okay。 Thank you。 \>\> I think you could probably come out with better ones
    than I could。 \>\> So given enough time and data are there any reasons why you
    wouldn't want to try all。 of these for example or are there certain situations
    or context where you would really。
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 转换成机器可读的方式，我认为这可能是一个很好的应用。>> 好的，谢谢。>> 我想你可能会提出比我更好的想法。>> 所以如果有足够的时间和数据，是否有什么理由不想尝试所有这些，例如，或者是否有某些情况或背景，让你真的。
- en: say never do this as opposed to just doing a train test set and seeing what
    works best。 There's no reason not to do all of them if you have the resources
    and in fact it's very。 common to do all of them and then squish them into a single
    ensemble。 So yeah， no reason， not to。 \>\> Great talk。 I really enjoyed it。 I
    guess one question that comes to mind is it seems。
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我说永远不要这样做，而不是仅仅进行训练测试集并查看哪个效果最好。如果你有资源，实际上没有理由不尝试所有方法，并且确实很常见将它们合并为一个整体。所以，是的，没有理由不这样做。>>
    很棒的演讲。我非常喜欢。我想起的一个问题是，似乎。
- en: like there's a paradigm where there's those who have the resources and those
    who don't。 So I guess could you comment on that and what should plebs like me
    do and I don't have the。 resources。 \>\> Well I'm glad you asked that。 So there
    are a couple of places where you can get access。 to say large public datasets
    and a compute environment that's GPU enabled and that's， good。
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 像有一个范式，就是有资源的人和没有资源的人。所以我想你能评论一下这一点，像我这样的普通人该怎么办，而我没有资源。>> 嗯，我很高兴你问了这个。所以有几个地方可以获得访问权限，比如说大型公共数据集和一个GPU支持的计算环境，这很好。
- en: And there are also other places as well so co-lab is a hosted notebook that
    offers， GPU access。 Data， there are some areas where it's just going to be really
    hard to find。 data without paying for it's particularly financial data or image
    datasets where the。 images aren't all under a friendly copyright or friendly license。
    Yeah， no， that is something。
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 还有其他地方，所以 Co-lab 是一个托管的笔记本，提供 GPU 访问。有些数据区域，确实很难找到数据，尤其是财务数据或图像数据集，这些图像的版权或许可证都不友好。是的，这确实是个问题。
- en: that I stay up at night worrying about as well。 I mean， I can't spend $45 million
    training。 a game playing robot that's not on my capability list。 I have no good
    answers other than we're。 trying to help。 \>\> Hi， Cass Stor， Simon and Schuster。
    I'm curious。 do you have any good resources on interpreting， and validating models。
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我晚上担心的事情。我是说，我不能花4500万美元去训练一个不在我的能力范围内的游戏机器人。我除了说我们正在努力帮助外，没有好的答案。>> 嗨，Cass
    Stor，Simon 和 Schuster。我很好奇，你们是否有任何关于解释和验证模型的好资源。
- en: particularly regression models？ I think it's really important you。 can easily
    run a regression model but you might be using the wrong model in that case， right？
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 特别是回归模型？我认为这真的很重要，你可以轻松地运行一个回归模型，但在那种情况下你可能使用了错误的模型，对吧？
- en: \>\> Yeah， absolutely。 I have actually written a series of notebooks that are
    hosted on Kaggle。 that goes through it's in R。 If you're going to be doing a lot
    of regression， consider。 maybe looking at R that goes through model validation
    and selection for regression in。 particular that I am happy about。 So I'll tweet
    out a link to those and I am our seat。
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '>> 是的，绝对如此。我实际上写了一系列托管在 Kaggle 上的笔记本，用 R 编写。如果你要进行大量回归，考虑看看 R，尤其是它针对回归的模型验证和选择。这让我感到高兴。所以我会推特发一个链接，给你们。'
- en: catman on Twitter。 \>\> That'd be great。 Thank you。 \>\> One problem where we
    use a deep neural net to solve this problem but it turned out。 that actually all
    we really needed was regression。 And where would you start？ Should you start。
    with a regression or would you start with your list and start at the bottom and
    work， up？ \>\> Yeah。 I generally only use distance-based methods if I'm in a real-time
    crunch。 My use。
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: Twitter 上的 catman。>> 那太好了。谢谢。>> 有一个问题，我们使用深度神经网络来解决这个问题，但事实证明，我们真正需要的只是回归。你会从哪里开始？你应该从回归开始，还是从你的列表开始，从底部向上进行？>>
    是的。我通常只有在真正时间紧迫的情况下才会使用基于距离的方法。我的使用。
- en: usual step through if I'm doing a regression problem would probably be regression，
    random， forests。 deep learning if I have to。 If I was doing specifically classification
    and I wasn't。 as concerned with interpretability， I might do random forests regression
    and then deep， learning。 But every problem is different， right？ \>\> Sure， yeah。
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我在做回归问题，通常的步骤可能是回归、随机森林。如果我不得不这样做深度学习。如果我专门做分类，并且对可解释性不太担心，我可能会先做随机森林回归，然后再做深度学习。但每个问题都是不同的，对吧？>>
    当然，是的。
- en: But it encouraged me in our problem we started in the wrong place。 \>\> Yeah。
    don't start with deep learning。 Also in your personal learning journey， please。
    put deep learning after regressive。 It will make me happy。 \>\> I got one question。
    One of the issues that I see a lot is the issue dealing with extrapolation。
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 但这让我意识到我们在错误的地方开始了这个问题。>> 是的。不要从深度学习开始。此外，在你的个人学习旅程中，请将深度学习放在回归学习之后。这会让我感到高兴。>>
    我有一个问题。我看到的一个问题是处理外推的问题。
- en: '![](img/fce9dc1b3b018e80cfb5fd237a9adbf0_7.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fce9dc1b3b018e80cfb5fd237a9adbf0_7.png)'
- en: '![](img/fce9dc1b3b018e80cfb5fd237a9adbf0_8.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fce9dc1b3b018e80cfb5fd237a9adbf0_8.png)'
- en: versus interpolation。 One of the issues with neural networks is that outside
    of the domain。
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 与插值相对。神经网络的一个问题是，在领域之外。
- en: '![](img/fce9dc1b3b018e80cfb5fd237a9adbf0_10.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fce9dc1b3b018e80cfb5fd237a9adbf0_10.png)'
- en: you get completely undefined output。 Are there any models or any approaches
    that you would。 suggest to take when you want at least you have a reasonable expectation
    of what the。 value should be but not necessarily a data point there for the model
    to train on？ \>\> I mean。 if you have strong， high priority reason to believe
    that given X you get Y， out。
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 你会得到完全未定义的输出。当你希望至少有一个合理的预期值，但不一定有数据点供模型训练时，有没有什么模型或方法可以建议？>> 我的意思是，如果你有强烈、高优先级的理由相信在给定
    X 的情况下会得到 Y。
- en: you can use simulation for data augmentation。 That is kind of a dangerous path。
    If I were。 doing that， I would want to have a really strong reason to believe
    that there is data that exists。 here that I just don't have access to and I believe
    that these are its qualities。 \>\> So we are going to have to wrap up questions
    here but I am sure you are willing to answer。
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用模拟进行数据增强。这是一条有些危险的道路。如果我是这样做的话，我会想要有一个非常强的理由相信这里存在一些我无法访问的数据，并且我相信这些是它的特征。>>
    所以我们必须在这里结束提问，但我相信你愿意回答。
- en: some questions out in the hall if you have any other further questions。 Let's
    say thank。
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在大厅里有其他问题，请提出。让我们表示感谢。
- en: '![](img/fce9dc1b3b018e80cfb5fd237a9adbf0_12.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fce9dc1b3b018e80cfb5fd237a9adbf0_12.png)'
- en: you again。 [APPLAUSE]。
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 再次感谢你。 [掌声]。
