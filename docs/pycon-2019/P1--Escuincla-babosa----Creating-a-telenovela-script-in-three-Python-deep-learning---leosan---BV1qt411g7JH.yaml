- en: P1：¡Escuincla babosa! - Creating a telenovela script in three Python deep learning
    - leosan - BV1qt411g7JH
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: P1：¡Escuincla babosa! - 在三个 Python 深度学习中创建肥皂剧脚本 - leosan - BV1qt411g7JH
- en: It's opportunities。 So let's please welcome Lorraine Amesa。 She'll be giving
    the talk Esquincla Babosa， creating a telenovela， script， and three deep learning。
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 这是机会。所以请欢迎 Lorraine Amesa。她将做关于 Esquincla Babosa 的演讲，讲述如何创建一个肥皂剧脚本以及三个深度学习框架。
- en: '![](img/86caa82786cf9dc042e1ddb9efafe87d_1.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86caa82786cf9dc042e1ddb9efafe87d_1.png)'
- en: frameworks。 [APPLAUSE]， Excellent。
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 框架。[掌声]，非常好。
- en: '![](img/86caa82786cf9dc042e1ddb9efafe87d_3.png)'
  id: totrans-4
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86caa82786cf9dc042e1ddb9efafe87d_3.png)'
- en: OK， can you all hear me all right？ Very cool。 I naturally am a loud person and
    use a lot of hand motion。 So this will be great because I think it， goes with
    a topic pretty well。 So yes。 if you were staring at the name of this talk， and
    you're like， I don't think there's English。 in some of this。 You are correct。
    You're in the right place。 Esquincla Babosa。
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，大家能听到我吗？非常好。我本来就是一个声音很大的人，也喜欢用很多手势。所以这将会很棒，因为我觉得它与这个话题很契合。是的。如果你盯着这个演讲的名字，心想这里面好像没有英语。你是对的。你来对地方了。Esquincla
    Babosa。
- en: creating a telenovela script， and three Python deep learning frameworks。 Side
    note。 I will be actually on some of the slides。 There is a URL that you can actually。
    follow along with the slides。 And I do actually have all my content linked。 to
    the GitHub repository that has this code。 And there's a surprise of another GitHub
    repository。
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个肥皂剧脚本和三个 Python 深度学习框架。顺便说一下，我的一些幻灯片上会有一个网址，你们可以跟着幻灯片一起看。我所有的内容都链接到了包含这段代码的
    GitHub 仓库，还有一个惊喜的另一个 GitHub 仓库。
- en: at the tail end。 So who am I？ Ola， soy， la rena mesa， with our learning， glets
    or espanol。 But I will obviously be talking in English for this talk。 It is not
    pike， charlas。 So the topic。 I personally am just really， interested in deep learning。
    I hear the term deep learning。 And I'm like， what's so deep about this learning？
    So I'm really just kind of curious about this topic。
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在最后阶段。那么我是谁？你好，我是 Rena Mesa，正在学习我们的西班牙语。但是显然，我会用英语来做这个演讲。这不是皮克，聊天。所以话题是，我个人对深度学习非常感兴趣。我听到“深度学习”这个词时，我想，深度学习有什么深刻之处呢？所以我真的对这个话题很好奇。
- en: And I also really love telenovelas。 So when I was thinking about a topic。 it
    felt like a very big win-win for me。 So why not？ Why not？ But actually， more practically。
    I actually， work as a data engineer at GitHub。 And I sit between two teams in
    the data org。 My team's name is Software Intelligence Systems， which， is a little
    bit of a mouthful。
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我也非常喜欢肥皂剧。所以当我思考一个话题时，这对我来说感觉像是一个非常大的双赢。那么为什么不呢？但实际上，我确实在 GitHub 担任数据工程师。我在数据组织中处于两个团队之间。我的团队名为软件智能系统，这个名字有点拗口。
- en: S-I-S for short。 And the teams that I work with are our semantic code team。
    and our machine learning team。 Our machine learning engineers are increasingly。
    talking about different deep learning techniques， and using GPUs。 And a lot of
    these kind of techniques I hadn't used before。
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 简称 S-I-S。我工作的团队是我们的语义代码团队和我们的机器学习团队。我们的机器学习工程师越来越多地讨论不同的深度学习技术，并使用 GPU。这些技术中有很多我以前并没有使用过。
- en: So why not marry my interest and try a little toy session。 and see what I can
    learn about deep learning， and share with you all in a Python talk？
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 那么为什么不结合我的兴趣，试试一个小型的玩具会话，看看我能从深度学习中学到什么，并在 Python 演讲中与大家分享呢？
- en: I do some other work in the Python space as well。 I'm in Chicago。 I help run
    Pi Lady Chicago。 And I'm also on the Python Software Foundation board， of directors。
    Yay。 [LAUGHTER]。 If you do have any questions about those things， again， you can
    find me later。 So I'm a bit curious how many of you。
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我在 Python 领域也做一些其他工作。我在芝加哥，帮助管理 Pi Lady Chicago。同时我也是 Python 软件基金会的董事会成员。耶。[笑声]如果你对这些事情有任何问题，稍后可以找我。我有点好奇你们中有多少人。
- en: '![](img/86caa82786cf9dc042e1ddb9efafe87d_5.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86caa82786cf9dc042e1ddb9efafe87d_5.png)'
- en: may have seen some of these headlines floating around。 Oops。 Sorry about that。
    Let me go to the next slide。 OK， excellent。 Did anyone see this headline floating
    around about this AI。 generated script starring David Hasselhoff？ No？ OK， you
    should watch it。 It's this really interesting and absurdist eight minute short。
    And the creator of this。
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 你们可能见过一些这样的头条新闻。哦，抱歉。让我换到下一张幻灯片。好的，太好了。有人看到关于这个 AI 生成的剧本的头条新闻吗？主演是大卫·哈塞尔霍夫？没有？好的，你应该看看。这是一个非常有趣和荒诞的八分钟短片。这个创作者的。
- en: Ross Goodwin， who is a creative technologist in Google's Artist， and Machine
    Learning program。 actually used a RNN， a recurring， neural network to actually
    generate all the lines that。 were in the short for David Hasselhoff script。 The
    short is quite interesting。 It's very ephemeral。 It's not quite clear what's happening。
    But when I first saw this headline。
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 罗斯·古德温（Ross Goodwin），是一位创意技术专家，在谷歌的艺术与机器学习项目中，实际上使用了递归神经网络（RNN）来生成大卫·哈塞尔霍夫短片中的所有台词。这个短片非常有趣。它是非常短暂的，发生的事情并不十分明确。但当我第一次看到这个标题时。
- en: it actually got me thinking about the intersection of art， and technology。 And
    this was something I never really， have thought about before。 What happens if
    we can maybe automate the idea， the creativity process？
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这实际上让我思考艺术和技术的交集。这是我从未真正考虑过的事情。如果我们可以自动化创造过程，会发生什么？
- en: What does that actually look like？ Another place that I kind of started seeing
    this floating。 around， as one does， lots of conversations on Twitter。 Specifically。
    this is from a screenshot from a chat that， was March 2018， wherein someone alleged。
    that they took all of the scripts of the various saw movies， and no。
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 这实际上看起来是什么样子？我开始看到的另一个地方，像往常一样，在推特上有很多对话。具体来说，这是来自2018年3月的聊天截图，其中有人声称，他们拿走了各种《电锯惊魂》电影的所有剧本，但没有。
- en: I do not know how many there are。 And apparently developed a neural network。
    and allegedly trained it for 1，000 hours， whatever that means， to generate this
    script。 And this script is supposed to be a horror script。 And in the script，
    that first line， it says。 a sexy woman， Becky， sex woman， is covered by a bed。
    She's in a whale。
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我不知道有多少个。显然开发了一个神经网络，并且据说训练了它1,000小时，无论这意味着什么，来生成这个脚本。而这个脚本据说是一个恐怖片脚本。在脚本中，第一行写着，一个性感的女人，贝基，性女人，躺在床上。她在一条鲸鱼里。
- en: but doesn't know she's in a whale。 And it's a very confusing piece of text。
    The part on the right-hand side that kind of continues， to go into this a little
    bit starts。 talking about how this is a really interesting application。 of neural
    networks to try to create an on-meath idea， with text generation of movie scripts。
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 但不知道她在一条鲸鱼里。这是一段非常令人困惑的文字。右侧的部分继续谈论如何将神经网络应用于电影脚本的文本生成，这是一个非常有趣的应用。
- en: However， they do start asking questions， about why are the words in the vocabulary。
    used in this script， including such language as Trump and Whale。 The person then
    goes on to critique to say， actually。 nowhere in the saw movies are these two
    words appearing。
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，他们确实开始问，为什么这个脚本中使用的词汇，包括特朗普和鲸鱼这样的语言。然后这个人继续批评说，实际上，在《电锯惊魂》电影中，这两个词根本没有出现过。
- en: And if you know anything about neural networks， and by the end of this talk，
    you will， a little bit。 that can be a little problematic because as a neural network，
    is learning。 it learns from the corpus of text， that you provide it。 So if you're
    allegedly providing it the saw scripts。
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对神经网络有一点了解，到这次演讲结束时，你会有所了解。那可能有点问题，因为作为一个神经网络，它是通过你提供的文本语料库学习的。所以如果你据说提供了《电锯惊魂》的脚本。
- en: and it's coming up with words it had not ever seen before。 can we say this is
    actually an AI-generated script？ I don't know。 Either way。 I think this thread
    is very interesting， and it started poking a lot of questions。 and maybe you want
    to actually try my own example。 The Whale did that。 It became a meme。
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 它生成了以前从未见过的词汇。我们能说这是一个真正的人工智能生成的剧本吗？我不知道。无论如何，我认为这个话题非常有趣，并且开始引发很多问题，也许你想尝试我自己的例子。《鲸鱼》做到了。它成为了一个网络迷因。
- en: You can go find out more about this script。 But there's a lot of people that
    are starting。 to think about what is the intersectionality of creativity， and
    technology。
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以去了解更多关于这个脚本的信息。但有很多人开始思考创造力和技术的交集。
- en: '![](img/86caa82786cf9dc042e1ddb9efafe87d_7.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86caa82786cf9dc042e1ddb9efafe87d_7.png)'
- en: So you might be asking， why tel noveles？ Also， what in the world is a telenovela？
    Well。 you came to the right place。 I'm here to inform you about one of the most
    epic meltdowns。
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 所以你可能会问，为什么是肥皂剧？还有，什么是肥皂剧？好吧，你来对地方了。我在这里告诉你关于一次史诗般崩溃的事情。
- en: '![](img/86caa82786cf9dc042e1ddb9efafe87d_9.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86caa82786cf9dc042e1ddb9efafe87d_9.png)'
- en: in a telenovela that has ever happened。 So do I have any telenovela fans in
    here？ Yes。 OK。 that's exciting for me。 So maybe some of you remember this one，
    Maria La Del Barrio。 which is a beloved Mexican telenovela， from the 1990s。 I
    believe specifically 1995 to 1996 with like 195 episodes。
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在任何情感小说中都曾发生过。所以这里有情感小说迷吗？有。好吧。这让我很兴奋。或许你们中的一些人还记得这一部，《玛利亚·拉·德尔·巴里奥》，这是1990年代一部备受喜爱的墨西哥情感小说。我相信具体是1995到1996年，共有195集。
- en: But this scene-- and the images might be a little hard to see。 but this scene
    includes some of the characters， of where we get the beloved phrase。 Esquinga
    La Babosa， which is in the title of this talk。 So we have on the top two。 we've
    got a love-struck couple。 We have Alithia to the left， and we have Nambito to
    the right。
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 但是这个场景——虽然图片可能有点难看清，但这个场景包括了一些角色，正是我们获得那句著名短语的地方。“Esquinga La Babosa”，这是本次演讲的标题。所以在上面的两个角色中，我们有一对恋爱中的情侣。阿利西亚在左边，南比托在右边。
- en: And the main character who's shown on the right-hand side， and also that third
    panel at the bottom。 that is Soraya。 So she is our main character of this telenovela。
    And Soraya。 she's got some complex history going on。 So she is the-- she's a stepmother
    of Alithia。 And then Nambito is her ex-husband's son。 Right。 And the thing here
    is Soraya is still desperately。
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 而在右侧和底部第三个面板中展示的主要角色是索拉亚。所以她是这部情感小说的女主角。索拉亚有一些复杂的历史。她是阿利西亚的继母，而南比托是她前夫的儿子，对吧。问题在于索拉亚仍然绝望地。
- en: desperately in love with Nandito's father。 Like to the point of maybe there
    was like a restraining order。 So she's desperately-- she's desperately in love，
    with Nandito's father。 And she's stuck in a marriage， though， where she's， got
    this daughter， Alithia。 So she happens to walk into the room， where Alithia and
    Nandito are starting。
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 她对南迪托的父亲绝望地深深爱着，甚至可能到了一种限制令的地步。所以她绝望地……绝望地爱着南迪托的父亲。虽然她被困在一段婚姻中，拥有这个女儿阿利西亚。于是她恰好走进了阿利西亚和南迪托正在开始的房间。
- en: to express their love for one another。 Nandito very tenderly says， may Alithia
    please have a kiss。 And as they're leaning into this moment， in comes Soraya。
    And Soraya is like， what is going on？
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 表达他们彼此的爱。南迪托非常温柔地说，请阿利西亚给我一个吻。当他们俩即将靠近这一刻时，索拉亚进来了。索拉亚问，发生了什么？
- en: I don't understand， because for her， this shatters， I guess。 her master plan
    to get back with Nandito's father。 Essentially。 she then screams and calls Alithia
    Esquincalababoso。 which kind of means use world rotten bread in nice speak。 And
    what then ensues is the most epic fight。 I guess we can call it。 There's a lot
    of kind of air slaps。 You can very， very specifically tell。 that there's not any
    contact happening。 There's people like diving over beds to protect one another。
    some random gentleman in a suit walks in， and is like trying to break up the fight。
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我不明白，因为对她来说，这破坏了她的计划，试图与南迪托的父亲复合。基本上，她随即尖叫着叫阿利西亚“Esquincalababoso”，这在好听的说法中大致意为“用世界的腐烂面包”。接下来发生的是最史诗般的打斗，我想我们可以这么称呼。场面上有很多空气拍打。你可以非常清楚地看出并没有实际接触发生。有些人像是在床上跳来跳去保护彼此。一个穿着西装的陌生人走进来，试图制止打斗。
- en: His character is in no way， shape or form， explained， as to why he's there。
    And then the nanny。 the nanny here， she walks in。 And what that says in Spanish
    here is， this woman is， this woman is。 she's got problems。 She is possessed by
    the devil。 So this scene and all the kind of trope of love。 and complex family
    relationships。 And the idea that we all have this trauma。
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 他的角色并没有以任何方式解释他为什么在这里。然后保姆走了进来。她在西班牙语中说的就是，这个女人，她有问题。她被魔鬼附身。所以这个场景以及所有的爱情和复杂的家庭关系的老套，以及我们都有的这种创伤。
- en: that's near and dear to our heart， that leads us into what Eta de novella is。
    And so Jorge Gonzales in talking about tell novella， he's a sociologist kind of
    commenting。 on the impact and social phenomena of tell novella。 Basically says
    they are melodramas。 And these are serial melodramas。 You get increments of 25，
    30 minutes， maybe five times a week。
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这与我们内心密切相关，也引导我们进入《情感小说》的主题。因此，豪尔赫·冈萨雷斯在谈论情感小说时，他作为社会学家进行了一种评论，讨论情感小说的影响和社会现象。他基本上说这些都是情节剧，且是系列情节剧。你每次可以看到25到30分钟的片段，可能一周五次。
- en: But what happens here， because it is a stable relationship， that you're making
    with the audience。 people become very passionate and very connected， with these
    narratives。
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 但是这里发生的事情是，因为你与观众建立了一个稳定的关系，人们变得非常热情并与这些叙事紧密相连。
- en: '![](img/86caa82786cf9dc042e1ddb9efafe87d_11.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86caa82786cf9dc042e1ddb9efafe87d_11.png)'
- en: To the extent that when we talk about tell novella， there's about two billion
    people worldwide。 who consume tell novella that are from Latin America。 That's
    about like a third of the world population。 So that's a pretty sizable number。
    And if you want to know more about why tell novella， is our big deal。
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 就我们谈论肥皂剧而言，全球大约有20亿人观看来自拉丁美洲的肥皂剧。这大约占世界人口的三分之一。所以这是一个相当可观的数字。如果你想知道为什么肥皂剧是我们的重大事件。
- en: here's some stats of some of the viewing， and patterns of viewing of Latin American
    tell novella。 So Latin American tell novella have been viewed， in over 100 countries。
    They have generated over $800 million， in marketing on various tell novella in
    a five year period。 I believe that stat is 2010 to 2015。 And just generally speaking，
    this is a global phenomenon。
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一些关于拉丁美洲肥皂剧的观看统计和观看模式。所以拉丁美洲的肥皂剧在超过100个国家播放过。在五年内，它们在各类肥皂剧上的营销收益超过了8亿美元。我相信这个统计数据是2010到2015年。一般来说，这是一种全球现象。
- en: '![](img/86caa82786cf9dc042e1ddb9efafe87d_13.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86caa82786cf9dc042e1ddb9efafe87d_13.png)'
- en: So in breaking down the arc of a tell novella， because if we want to try to
    emulate。 and create a tell novella with a neural network， we need to understand
    what it is we're trying to create。
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 所以在分析肥皂剧的情节弧时，因为如果我们想尝试模仿并用神经网络创建一部肥皂剧，我们需要理解我们要创造的是什么。
- en: '![](img/86caa82786cf9dc042e1ddb9efafe87d_15.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86caa82786cf9dc042e1ddb9efafe87d_15.png)'
- en: So back to kind of thinking about that kind of relationship， and things that
    pop up。 One of the things that's interesting about tell novella。 is this idea
    that there's not really any cliffhangers， or kind of open ended stories。 Dr。 Rios
    who talks about tell novella， and kind of understanding their social impact。
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 回到那种关系及其带来的事情。关于肥皂剧，有趣的是，几乎没有悬念或开放式的故事。Rios博士谈到了肥皂剧，并探讨了它们的社会影响。
- en: specifically says that at the end of the tell novella， things have to be cleared
    up。 So in our Maria de la Barrio， for example， if we have the character Maria
    and we want to know。 what's happening， she would hypothesize and say， our audience
    doesn't want to worry about Maria。 We don't want to know if she didn't find her
    true love。
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 她特别提到，在肥皂剧的结尾，事情必须得到解决。所以在我们的《玛丽亚·德拉·巴里奥》中，例如，如果我们有角色玛丽亚，我们想知道发生了什么，她会假设并说，我们的观众不想为玛丽亚担心。我们不想知道她是否找到了真爱。
- en: '![](img/86caa82786cf9dc042e1ddb9efafe87d_17.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86caa82786cf9dc042e1ddb9efafe87d_17.png)'
- en: her true mother， her true father， we like want all those answers。 So unlike
    English soap operas。 which I think some have been going for more than like 15，
    20 years。 what happens with that on novella， is you have a fixed melodramatic
    plot line。 And yes。 it's going to be lost loves。 It's going to be mothers and
    daughters fighting。
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 她的亲生母亲，她的亲生父亲，我们都想知道这些答案。所以与英语肥皂剧不同，我认为有些剧已经播放了超过15年、20年。肥皂剧的情节往往是固定的戏剧性剧情。没错，里面会有失去的爱，会有母女之间的争斗。
- en: It's going to be long lost relatives。 It's going to be love found。 It may be
    as quickly as you found that love。 Something happens where that person is shipped
    away。 and we don't know what happened to them。 Maybe the guy in the suit， he's
    from an ela de la novella。 I don't know。 But our telenovelas have a finite beginning
    and end， and they're generally tied up。
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 它会有久未谋面的亲人，也会有重新找到的爱情。可能你一找到爱情，某些事情就发生了，那个人被送走了，我们不知道发生了什么。也许那个西装男是来自某个肥皂剧的人物，我不知道。但我们的肥皂剧有明确的开头和结尾，通常都会得到解决。
- en: with a very big happy element at the end。 So for example， like the world's largest
    wedding。 If you watch Casa de las velores， like there's kind of some of that。
    maybe there's being built up to。 But yes， there's always this idea。 We've got
    a fixed arc。 a lot of intricate kind of heart wrenching moments， but it's going
    to wrap up with something happy。
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 结尾有一个非常开心的元素。例如，世界上最大的婚礼。如果你观看《Casa de las velores》，那里面有一些这样的情节，可能会逐渐展开。但没错，总是有这样一个想法。我们有一个固定的情节弧，许多复杂的、揪心的时刻，但最后会以快乐的结局收尾。
- en: that we can all feel good about and move on， with our lives and not wonder what
    happens to these characters。
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们都能感觉良好并继续生活，而不必担心这些角色会发生什么。
- en: '![](img/86caa82786cf9dc042e1ddb9efafe87d_19.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86caa82786cf9dc042e1ddb9efafe87d_19.png)'
- en: So for thinking about examples of telenovelas， let's take a look at some of
    the ones。 that are available to us in Spanish。 So this is Kate Del Castillo， who
    I swear。 I think she's like my role model in life。 She's fantastic。 If any of
    you like watch that documentary， about talking with Chapo， who's an article， Troficante。
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 所以在考虑肥皂剧的例子时，让我们看看一些可以用西班牙语观看的剧集。这是凯特·德尔·卡斯蒂略，我发誓，我认为她是我生活中的榜样。她非常出色。如果你们中的任何人看过关于与查波对话的纪录片，那是关于一个毒贩的。
- en: she was doing that with Sean Penn， because that's what she does in her free
    time。 But she's very famous。 She's been in many telenovelas。 And one of the ones
    that is most known is La Reyna del Sor。 So that's an example of one that is really，
    really popular， in Mexico， and then many of us may know。
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 她和肖恩·潘一起做那个，因为这就是她在空闲时间做的事情。但她非常有名。她出演过许多肥皂剧。其中最著名的之一是《La Reyna del Sor》。所以这是一个在墨西哥非常、非常受欢迎的例子，很多人可能都知道。
- en: Jose Betelafeja。 Because actually， some of these telenovelas， have crossovers
    in English。 So the ones that I looked at， because again， we're predominantly，
    an English speaking population。 I looked at popular telenovelas in Spanish， looked
    for their English crossovers。 and then worked with that text。 So Queen of the
    South， which is actually on USA。
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 何塞·贝特拉菲哈。因为实际上，有些肥皂剧在英语中也有跨界。因此我关注的那些，因为我们主要是一个讲英语的人口。我查看了西班牙语的热门肥皂剧，寻找它们的英语跨界，然后使用那些文本。所以《南方女王》，实际上是在美国播出的。
- en: talks to us a little bit about that narrative， with Kate Del Castillo。 In this
    one。 in the Queen of the South， essentially what we have is the character who。
    we have a character who becomes the most prominent drug， lord trafficker in South
    Spain after she。 has to flee Mexico because her beloved is actually captured。
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 与凯特·德尔·卡斯蒂略讲述这个叙事。在这一部《南方女王》中，本质上我们有一个角色，她成为南西班牙最著名的毒贩，之后她不得不逃离墨西哥，因为她心爱的人被捕了。
- en: by the cartel that's poaching drugs in that area。 So of course， as one does。
    you relocate to another country， and then you start your vast drug empire。 So
    that one's going to have love lost。 It's going to be a thriller。 It's going to
    be a drama。 And it's going to be high emotional action impact。 And then with ugly
    Betty， what we have。
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 由在该地区贩毒的卡特尔引发的。所以当然，像人们所做的那样，你会迁移到另一个国家，然后开始你的庞大毒品帝国。所以这一部将会有失去的爱情。它将会是一部惊悚片。它将会是一部戏剧，情感冲击感非常强烈。然后在《丑女贝蒂》中，我们看到的。
- en: is more or less a rom-com。 This actually started in Colombia， and it's had，
    I think。 three or four crossovers， to different Spanish speaking markets starting
    in 1999， in Colombia。 But essentially， what we have is， what we have is a character，
    a Beatriz， who works in the fashion。 industry and is considered ugly because stereotypical
    things。
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 更或多或少是一部浪漫喜剧。这实际上起源于哥伦比亚，自1999年以来，它在不同的西班牙语市场已经有三或四个跨界。但本质上，我们有一个角色，比阿特丽斯，她在时尚行业工作，因为一些刻板印象被认为是丑的。
- en: like she has braces and she has glasses and things， that I have all worn in
    my life。 But anyways。 it's all about the story， about her struggles working in
    the fashion industry。 and trying to find love。 And then Jane the Virgin actually--。
    now I said I looked for popular crossovers， from Spanish to English， but for this
    one。
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 比如说她戴着牙套和眼镜，这些我在生活中都穿过。不过无论如何，这一切都是关于她在时尚行业工作时的故事，和她寻找爱的挣扎。而《贞女杰恩》实际上——现在我说我在寻找西班牙语到英语的流行跨界，但对于这一部。
- en: because I think there's enough comfort and knowledge， for people of what this
    one is--。 Jane the Virgin is actually a satirical melodrama， but it's fantastic
    because essentially， we。 follow Jane， who is a virgin， who， after visiting a doctor's，
    office， somehow becomes artificially。 and seminated and becomes pregnant。 So she's
    a pregnant virgin， and then various things。
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我认为人们对这一部有足够的熟悉感和了解——《贞女杰恩》实际上是一部讽刺的情感剧，但它很精彩，因为本质上我们跟随贞女杰恩，她是一名处女，在去看医生的办公室后，不知怎么的被人工授精，结果怀孕了。所以她是一名怀孕的处女，然后发生了各种事情。
- en: that unfold afterwards， and how she finds love， or how she doesn't find love
    leads us。 into this complex story that is Jane the Virgin。 So in thinking about
    telenovelas。 we understand a little bit of the arc。 We understand that it's a
    melodrama。 We understand that it has finite end。 We understand it's going to wrap
    up with something happy。
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 随后展开的故事，以及她如何找到爱情，或如何没有找到爱情，带我们进入复杂的故事，这就是《贞女无敌》。所以在考虑肥皂剧时，我们对情节有一点了解。我们知道这是一部情感剧。我们知道它有一个有限的结局。我们知道最后会以某种幸福的方式收尾。
- en: and we understand some of the trips that may be employed。 So now。 how can we
    start applying this to deep learning？ When we think about machine learning。
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也了解可能会采用的一些方法。那么，现在我们如何开始将这些应用于深度学习呢？当我们考虑机器学习时。
- en: '![](img/86caa82786cf9dc042e1ddb9efafe87d_21.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86caa82786cf9dc042e1ddb9efafe87d_21.png)'
- en: and I'm not sure if many of us in here do machine learning。 some of the things
    that you may think of are some of the language。 that's provided to us by the founders
    of this discipline。 So Arthur Samuel。 who actually coined the term machine learning，
    actually provides us with this language。
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我不确定在座的各位中有多少人从事机器学习。你可能想到的一些事情是该学科创始人提供给我们的某些术语。因此，阿瑟·塞缪尔，他实际上创造了“机器学习”这个术语，给我们提供了这些语言。
- en: saying that machine learning is a field of study， where computers have the ability
    to learn without being。 explicitly programmed。 So there's this idea that there's
    something that's not requiring。 us to actually have some intent or some purpose。
    So that is where we can start with machine learning。
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习是一个研究领域，计算机能够在没有明确编程的情况下学习。因此，这里有一个想法，就是不需要我们有某种意图或目的。因此，这就是我们可以从机器学习开始的地方。
- en: '![](img/86caa82786cf9dc042e1ddb9efafe87d_23.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86caa82786cf9dc042e1ddb9efafe87d_23.png)'
- en: Now， as we see here at PyCon， there's， many various talks that may talk about
    data science。 or machine learning， and this is a very， very， very， very vast space。
    So if we were looking at。 for example， technology and tools， we may see things
    like Python。 We may be talking about things like Spark or Julia， or other scientific
    programming languages。
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，正如我们在PyCon上所见，有许多各种各样的讲座可能会讨论数据科学或机器学习，这是一个非常、非常、非常、非常广泛的领域。因此，如果我们查看，例如技术和工具，我们可能会看到Python这样的东西。我们可能会讨论Spark或Julia等其他科学编程语言。
- en: If we're talking about the kind of problems we can solve， with machine learning。
    we'll see the problem categories， on the bottom left-hand side。 We might be doing
    classification。 clustering， optimization， kind of problems。 However。 what we want
    to focus on is this idea of a subfield。 So when we talk about deep learning。
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们谈论可以用机器学习解决的各种问题，我们会在左下角看到问题类别。我们可能在进行分类、聚类、优化等问题。然而，我们想要关注的是这个子领域的概念。因此，当我们谈论深度学习时。
- en: deep learning is a subfield。 It's a type of machine learning and is alongside
    other categories。 such as maybe ones you may have heard of， supervised。
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习是一个子领域。它是一种机器学习，与其他类别并列，比如你可能听说过的监督学习。
- en: '![](img/86caa82786cf9dc042e1ddb9efafe87d_25.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86caa82786cf9dc042e1ddb9efafe87d_25.png)'
- en: unsupervised， semi-supervised。 So to broaden on that definition of machine learning。
    and challenging us to think about how we can actually。 have a deep learning model
    that can actually generate an ML， script。 we probably need to reconceptualize，
    what machine learning means。
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督学习、半监督学习。因此，要扩展机器学习的定义，并挑战我们思考如何真正拥有一个可以生成机器学习脚本的深度学习模型，我们可能需要重新构思机器学习的含义。
- en: So this is a great definition given to us by Tom Mitchell。 And Tom Mitchell。
    the former chair of the computer science， department at Carnegie Mellon， who。
    wrote the quintessential book on machine learning published， in 1997。 It stands
    up really well。 I highly recommend reading it。 Basically says， when we think about
    machine learning。
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 这是汤姆·米切尔给我们的一个很好的定义。汤姆·米切尔是卡内基梅隆大学计算机科学系的前主任，他撰写了1997年出版的经典机器学习书籍。这本书至今仍然很好。我强烈推荐阅读。基本上说，当我们考虑机器学习时。
- en: it's not so much that it's a program that， isn't explicitly programmed。 But
    instead。 there's something that we want to accomplish。 So if we're thinking about
    a computer program that's。 said to learn， let's think about it in this framework。
    We have some objectives。 So we have a task that we want to accomplish。 And in
    order to accomplish that task。
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 这并不是一个明确编程的程序，而是我们想要完成的事情。因此，如果我们考虑一个被称为学习的计算机程序，让我们在这个框架内思考。我们有一些目标。我们有一个想要完成的任务。为了完成这个任务。
- en: we need to be able to provide it with some ability， to understand what that
    task is。 So we have a task， and then we provided some understanding。 to understand
    what some ability to understand what， that task is， which is the experience。 And
    then as that program continues， to attempt to do that task， we can develop a performance。
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要能够提供一些能力，让它理解这个任务是什么。因此，我们有一个任务，然后提供一些理解，来理解这个任务是什么，即经验。然后，随着该程序不断尝试完成这个任务，我们可以开发出性能。
- en: measurement to understand how it's improving over time。 So this language gets
    us a little bit closer， to thinking about what machine learning is when。 it applied
    to deep learning。
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 测量来理解它是如何随着时间改善的。因此，这种语言让我们更接近于理解机器学习是什么，特别是当它应用于深度学习时。
- en: '![](img/86caa82786cf9dc042e1ddb9efafe87d_27.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86caa82786cf9dc042e1ddb9efafe87d_27.png)'
- en: But it doesn't get us out the way。 So the trend with machine learning over time--。
    and it's kind of wild to think that in the 1950s， and some of these algorithms
    that we're talking about today--。 naive base， for example-- this is not new math。
    This is not new algorithms。 These are things that have been out there for 20 plus
    years。
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 但这并没有让我们走出困境。因此，机器学习的趋势随着时间推移——想想在1950年代，一些我们今天讨论的算法——比如朴素贝叶斯，这并不是新的数学。这不是新的算法。这些东西已经存在了20多年。
- en: But what we're seeing in the trend of the world， of machine learning and what
    does it mean for a machine。 to learn， we start with this realm of artificial intelligence，
    to move into machine learning， which。 is， again， that idea of a program not having，
    to be explicitly programmed to do that task。 And then we move into this world
    of deep learning。 And we start hearing about these things。
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 但我们看到的趋势是，机器学习的世界是什么，机器学习意味着机器如何学习，我们从人工智能的领域开始，进入机器学习。这是再次强调一个程序不需要明确编程来完成任务的思想。然后我们进入深度学习的世界，开始听到这些事情。
- en: called neural networks， which sounds very sci-fi and very， unclear what that
    is。 Frame it a little bit more simply， if we， were to think about what it means。
    that this evolution from AI to deep learning， we can say that an example of an
    artificial intelligence。 program is a door sensor because it's， able to adapt
    to its environment around it。
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 被称为神经网络，这听起来很科幻，且很不清楚它是什么。更简单地框定一下，如果我们考虑从人工智能到深度学习的演变，我们可以说一个人工智能程序的例子是门传感器，因为它能够适应周围的环境。
- en: With a machine learning， we can say， an example of a machine learning program
    is our spam filters。 We all know it。 We see it in our various email inboxes。 The
    idea that we know spam when we see it。 can we teach an algorithm how to do that？
    Yes， we can。 And we've done that with machine learning。 Now， from moving into
    the realm of deep learning， what we start getting into is this idea of using。
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，我们可以说，一个机器学习程序的例子是我们的垃圾邮件过滤器。我们都知道它。我们在各种电子邮件收件箱中看到它。我们知道看到垃圾邮件时，我们能否教一个算法如何做到这一点？可以的，我们做到了。这是通过机器学习实现的。现在，进入深度学习领域时，我们开始接触到使用。
- en: these neural networks that drives its own learning。 That is the engine by which
    the learning is empowered。 And if you see on this right-hand side here。 this project
    is called the Deep Dream， which， came from Google Research， wherein they。 are
    providing an image of an object to an algorithm。 And with the desired intent and
    output to say。
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 这些神经网络推动着它自己的学习。这是赋予学习动力的引擎。如果你在右侧看到，这个项目叫做Deep Dream，来自谷歌研究，他们向算法提供一个物体的图像，并且有意图和输出来说明。
- en: tell us what you see in this image。 So what we start seeing on the other side，
    is if we provide it。 let's say， an image of an apple， it's like， OK， I understand
    that this is an apple。 But what other images do I see in it， and what can I infer
    from this image that I'm seeing？
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 告诉我们你在这张图片中看到什么。所以我们在另一边开始看到的是，如果我们提供它，比如说，一个苹果的图像，它会说，“好的，我明白这是一个苹果。”但我还可以从这张图片中看到什么其他的图像，以及我能从中推断出什么？
- en: So kind of flipping it a little bit on its head， and getting into the realm，
    maybe。 to Android's dream of electric sheep， that's where we start saying， OK，
    we're。 going to give you some ideas， some definition。 But using this neural network，
    we。 want you to drive your own learning。 So if we were thinking about an example
    of applying deep。
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 所以稍微反转一下思路，进入安卓的电动羊的领域，也许在这里我们开始说，“好吧，我们要给你一些想法，一些定义。”但使用这个神经网络，我们希望你能驱动自己的学习。所以如果我们考虑一个应用深度学习的例子。
- en: learning to， let's say， a classification problem， if I had something like a
    flashlight。 and I wanted it to respond and learn from audio cues， to turn on，
    we could say， let's say。 I want to have a flashlight that if I say the word dark，
    it turns on。 Well。 with a deep learning implementation， it wouldn't just have
    that fixed understanding of saying， OK。
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 学习，比如说一个分类问题，如果我有一个手电筒，我想让它响应并学习音频提示来打开，我们可以说，“假设我想要一个手电筒，如果我说‘黑暗’，它就会打开。”那么，通过深度学习的实现，它不仅仅有那种固定的理解。
- en: dark maps to turn on because this， is the task of bringing light to the world。
    But instead。 it might understand other cues， other verbal cues that I might be
    able to supply to this model。 So I might say something like， I can't see， or the
    light switch won't work。 With a deep learning kind of implementation， the neural
    network would be able to learn and infer。
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 黑暗的地图被打开，因为这是将光带给世界的任务。但相反，它可能会理解其他提示，其他我可以提供给这个模型的语言提示。所以我可能会说一些类似于“我看不见”或者“开关不起作用”的话。通过深度学习的实现，神经网络将能够学习和推断。
- en: these other kinds of words that may fill in， to suggest that there's an absence
    of light。 to suggest that the area around you is dark。 So this learning， again，
    that happens， the brain。 which is that neural network， happens with an activation
    method。
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 这些其他类型的单词可能填补，表明存在光的缺失，表明你周围的区域是黑暗的。因此，这种学习再次发生，大脑，也就是那个神经网络，发生在一种激活方法下。
- en: '![](img/86caa82786cf9dc042e1ddb9efafe87d_29.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86caa82786cf9dc042e1ddb9efafe87d_29.png)'
- en: So moving now back to telenovelas， there's a type of problem called text generation。
    that we're going to be looking at。 So text generation is sequence processing。
    and sequence processing has a lot of applications， in the world of deep learning。
    We can use it for video processing， price modeling， and yes， text generation。
    Essentially。
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 所以现在回到肥皂剧，有一个叫做文本生成的问题，我们将要研究。文本生成是序列处理，序列处理在深度学习的世界中有很多应用。我们可以用它进行视频处理、价格建模，当然还有文本生成。本质上。
- en: what we want to do is there's， two approaches that we can have。 Using language。
    so actually using a body of text， we can say we want you to look at the individual
    words。 that are in this text to actually generate， some text on the other side，
    or we can actually develop。 the deep learning model to actually look at the individual。
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想要做的是有两种方法。使用语言。因此，实际上使用一段文本，我们可以说我们希望你查看文本中单个单词，以生成另一边的一些文本，或者我们可以实际开发深度学习模型，以查看单个单词。
- en: characters that are provided in the corpus， that we're working with。 Either
    way。 given the text that we， provided to the sequence processing model that we
    use。 what it's going to start trying to do， is approximate the relationship of
    what words mean。 what the relationship between one word is to another。
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 提供的字符在我们处理的语料库中。无论如何，给定我们提供给序列处理模型的文本，它将开始尝试近似单词的意义之间的关系，即一个单词与另一个单词之间的关系。
- en: and then as it tries to understand the relationship， between words。 try to think
    about how it is that it can build， its own patterns and generate its own vocabulary
    that。 kind of models the relationships it sees in this corpus of words， that we
    provide it。
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，当它试图理解单词之间的关系时，试着思考它如何建立自己的模式并生成自己的词汇，以模型化它在我们提供的单词语料库中看到的关系。
- en: '![](img/86caa82786cf9dc042e1ddb9efafe87d_31.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86caa82786cf9dc042e1ddb9efafe87d_31.png)'
- en: So I've been using the words neural networks， and deep learning a little bit
    interchangeably。 And while I want you to understand that there， are these artificial
    neural networks。 what essentially deep learning is it actually。
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我一直在将“神经网络”和“深度学习”这两个词稍微互换使用。我希望你明白，确实存在这些人工神经网络。深度学习本质上就是这样。
- en: '![](img/86caa82786cf9dc042e1ddb9efafe87d_33.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86caa82786cf9dc042e1ddb9efafe87d_33.png)'
- en: makes use of several neural networks。 And when we talk about neural networks。
    the power that drives a neural network is the neuron。 So the neuron here is it's
    got three parts。 We have the inputs， we have the weights， and then we have the
    activation function。
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 使用了几种神经网络。当我们谈论神经网络时，驱动神经网络的力量是神经元。所以这里的神经元有三个部分。我们有输入，有权重，还有激活函数。
- en: '![](img/86caa82786cf9dc042e1ddb9efafe87d_35.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86caa82786cf9dc042e1ddb9efafe87d_35.png)'
- en: So essentially what's happening is our words， are going to be the inputs， and
    we're。 going to represent those as either words or as either characters。 We then
    apply an arbitrary weight to it， and then as it runs through each layer in our
    neural network。 it's going to start trying to approximate that relationship。
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 本质上，我们的单词将作为输入，我们将其表示为单词或字符。然后我们施加一个任意的权重，接着在神经网络的每一层运行时，它会开始尝试近似这种关系。
- en: to understand relationships either from word to word， or character to character。
    And then it's going to go through this activation function。 whereas it's learning
    these relationships， it starts adjusting those weights。 to try to find the optimal
    weight to use， to actually start generating a thing， or in our case。
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 以理解单词与单词之间或字符与字符之间的关系。然后它将通过这个激活函数。在学习这些关系时，它开始调整这些权重，尝试找到最佳权重来生成内容，或在我们的案例中。
- en: generating text。 So the activation function， you can think， of something like
    a sigmoid function。 So essentially what we have is some kind of function。 that
    is a nonlinear function that is going to allow us to actually， start doing some
    math。 So if we have a weight applied to a word， let's say， it starts at 0。5。
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 生成文本。因此，你可以将激活函数想象为某种sigmoid函数。本质上，我们有某种函数，它是一个非线性函数，允许我们开始进行一些数学运算。如果我们对一个单词施加权重，假设它从0.5开始。
- en: but we need to start shifting it either higher， or lower based on the relationship
    between these words。 that the network is learning。 We've got this activation function
    to help us do that。 So when we're training a neuron， essentially what we need
    to do， is these three steps。 The neuron takes an initial guess at classifying
    the sample。
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 但我们需要根据网络正在学习的这些单词之间的关系开始将其调整得更高或更低。我们有这个激活函数来帮助我们做到这一点。因此，当我们训练一个神经元时，本质上我们需要做这三个步骤。神经元对样本进行初步分类。
- en: and then updates the guess by adjusting the weights。 and then it goes ahead
    and repeats and tries again。
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 然后通过调整权重来更新猜测。接着继续重复尝试。
- en: '![](img/86caa82786cf9dc042e1ddb9efafe87d_37.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86caa82786cf9dc042e1ddb9efafe87d_37.png)'
- en: So this process is very iterative。 And with neural networks， there's。 many kinds
    of neural networks that we can use--， feed forward， radial basis function， or
    an RNN。 And RNNs are good for sequential processing problems， like text generation。
    And what's really cool about this， is that it actually allows us to propagate
    data back and forth。
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程是非常迭代的。在神经网络中，有许多种神经网络可以使用——前馈神经网络、径向基函数或RNN。RNN适合于序列处理问题，例如文本生成。而这一点非常酷，因为它实际上允许我们在数据之间来回传播。
- en: '![](img/86caa82786cf9dc042e1ddb9efafe87d_39.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86caa82786cf9dc042e1ddb9efafe87d_39.png)'
- en: So as our neurons are all connected， so a deep neural network would be an example
    of what。 we have on the right-hand side。 Basically， it's a directed graph。 We
    have input going from one node。 but also we can move backwards。 So using an RNN，
    insights learned upstream or downstream。
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们的神经元都是连接的，所以深度神经网络是右侧所示的一个例子。基本上，这是一个有向图。我们有输入从一个节点传入，但也可以向后移动。因此，使用RNN，从上游或下游学习到的见解。
- en: '![](img/86caa82786cf9dc042e1ddb9efafe87d_41.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86caa82786cf9dc042e1ddb9efafe87d_41.png)'
- en: can move back and forth across these neurons。 So if we wanted to make an RNN
    in Python。 how do we do it？
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 可以在这些神经元之间来回移动。所以如果我们想在Python中创建一个RNN，该怎么做呢？
- en: '![](img/86caa82786cf9dc042e1ddb9efafe87d_43.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86caa82786cf9dc042e1ddb9efafe87d_43.png)'
- en: Well， there's more than one way， as we like to do。 So there's three really popular
    frameworks。 Keras， TensorFlow， and PyTorch。 And yes， I'm going to borrow the language
    from the 2017 keynote。 but yes， Python is surprisingly super awesome， or maybe
    unsurprisingly super awesome at scientific programming。
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，确实有不止一种方法，就像我们喜欢做的那样。所以有三个非常流行的框架：Keras、TensorFlow和PyTorch。没错，我会借用2017年主题演讲中的语言，但确实，Python在科学编程方面出乎意料地超级棒，或者说并不令人意外地超级棒。
- en: '![](img/86caa82786cf9dc042e1ddb9efafe87d_45.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86caa82786cf9dc042e1ddb9efafe87d_45.png)'
- en: So these three frameworks， when you're， thinking about which ones to use。 I
    like to think about questions to help me think about what， tools best for the
    task at hand。 How much technical expertise is needed to start using it？ What are
    your requirements？ For example。 do you have a certain SLO？ Do you have to meet
    what's the size of data you're working on？
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，这三个框架，当你考虑使用哪个时。我喜欢通过问题来帮助我思考哪些工具最适合当前的任务。开始使用它需要多少技术专长？你的要求是什么？例如，你有特定的
    SLO 吗？你需要满足什么？你正在处理的数据量有多大？
- en: Working with。 And also， how easy is it to start using the framework？
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 以及开始使用这个框架有多简单？
- en: So if we're looking at making an RNN in Python using Keras。
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 所以如果我们在 Python 中使用 Keras 创建 RNN。
- en: '![](img/86caa82786cf9dc042e1ddb9efafe87d_47.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86caa82786cf9dc042e1ddb9efafe87d_47.png)'
- en: '![](img/86caa82786cf9dc042e1ddb9efafe87d_48.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86caa82786cf9dc042e1ddb9efafe87d_48.png)'
- en: we've got these three steps that we're always， going to be emulating。 Basically。
    we have to take that text that text， and we have to transform it either into characters
    or words。 So we tokenize it。 Then once we've tokenized it， we generate these hot
    encodings。
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有这三个步骤始终需要模拟。基本上，我们必须将文本转换为字符或单词。因此我们对其进行分词处理。然后，一旦我们完成分词处理，就生成这些热编码。
- en: '![](img/86caa82786cf9dc042e1ddb9efafe87d_50.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86caa82786cf9dc042e1ddb9efafe87d_50.png)'
- en: which basically gives us-- let's say on the next slide， this actually gives
    us a nice example。 Let's say our corpus is the word bad。 And because we've got
    the letter B。 what we want to do is we've got nine characters in our corpus。 So
    we have other words that introduce these other characters。
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 这基本上给了我们一个示例。假设我们的语料库是单词 "bad"。因为我们有字母 B，我们想要做的是在我们的语料库中有九个字符。所以我们还有其他单词引入这些字符。
- en: What we want to do is basically be able to understand， and predict the likelihood
    for each corpus。 for each character in our corpus， to actually say what's the
    likelihood。 that this character will appear。
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想要做的基本上是理解并预测每个语料库中每个字符的出现可能性，以实际说出这个字符出现的可能性。
- en: '![](img/86caa82786cf9dc042e1ddb9efafe87d_52.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86caa82786cf9dc042e1ddb9efafe87d_52.png)'
- en: So as we generate this hot character encoding， we're going to use that as input
    to our neural network。 and we're then going to pass it through our RNN， and basically
    say， hey， start learning the weights。 and start coming up with the optimal weight，
    to represent the likelihood of what。 character you should be generating based
    on these--， again， trying to approximate that relationship。
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们生成这个热字符编码时，我们将其用作神经网络的输入。然后我们将其通过我们的 RNN，并基本上说，嘿，开始学习权重。并开始得出最佳权重，以表示根据这些字符生成的可能性，试图近似该关系。
- en: in the underlying text， and let's see what we get on the other side。
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在底层文本中，让我们看看在另一端得到的结果。
- en: '![](img/86caa82786cf9dc042e1ddb9efafe87d_54.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86caa82786cf9dc042e1ddb9efafe87d_54.png)'
- en: And to train this， we're going to run it through some number， of epochs。 So
    with Keras。 what we can notice here， for those of you， who may have used Scikit-Learn，
    we've got a really。 really nice--， we've got a really nice high-level API we can
    use。 Notice how we said sequential processing is a text--， generation is a sequential
    processing problem？
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练这个模型，我们将对其进行若干轮的训练。所以在 Keras 中，对于那些可能使用过 Scikit-Learn 的朋友们，我们注意到有一个非常好用的高级
    API。请注意我们所说的顺序处理是一个文本生成的顺序处理问题。
- en: Surprise， we're going to use this in Quenchle Model。 So LSTM， long short-term
    memory。 is a type of RNN， again， propagating that data back and forth。 This is
    pretty straightforward。 because it allows us， to pull in this in Quenchle Model。
    We specifically say we want to use an RNN of this type。 We then basically add
    in our data。
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 惊喜的是，我们将在 Quenchle 模型中使用这个。所以 LSTM，即长短期记忆，是一种 RNN，能够将数据前后传播。这是相当简单的，因为它允许我们在
    Quenchle 模型中引入这个。我们特别说我们想使用这种类型的 RNN。然后我们基本上添加我们的数据。
- en: and then we say， OK， let's go ahead and compile that model。 And notice this
    fit。 We're going to have 20 epochs that we're going to train over。 And what we
    want to do is we want to find that optimal weight， that we can generate。 So Keras
    has a very nice and user-friendly interface， that we can start using。
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们说，好的，接下来让我们编译这个模型。请注意这个 fit。我们将训练 20 轮。我们想要找到可以生成的最佳权重。因此，Keras 具有非常友好且用户友好的接口，我们可以开始使用。
- en: And if you were looking at what it may look like for that process， of running
    through epochs， well。 you can just basically think of a for loop。 It's very iterative。
    If we have 20 loops or we have 1。000， we can start doing things like that。 And
    then when we start wanting to actually use it。 we've got that model。predict functionality。
    So Keras is pretty cool。 Now let's look at TensorFlow。
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在考虑运行轮次时的过程，它可能看起来如何，你可以基本上把它想成一个for循环。这是非常迭代的。如果我们有20个循环或1,000个循环，我们可以开始进行这样的操作。当我们开始想要真正使用它时，我们有那个model.predict功能。因此Keras非常酷。现在让我们看一下TensorFlow。
- en: Mind you， I did some lifting of this data， so there's some logic that's kind
    of not captured。 but it is actually in my GitHub repository。 But again， when we're
    using TensorFlow， again。 we import TensorFlow， and we've actually， got an RNN
    that we can pull in。 Unlike in the last case where we've， got this sequential
    model and we've。
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我对这些数据进行了某种处理，所以有一些逻辑没有被捕获。但它实际上在我的GitHub仓库中。但再一次，当我们使用TensorFlow时，我们导入TensorFlow，实际上我们有一个可以拉入的RNN。与上一个案例不同，我们有这个顺序模型。
- en: got the LSTM that's kind of baked in， we're going to actually have to start
    making and creating。 our own object to represent the RNN。 We're going to be working
    with Tensors， which， is， again。 a representation of the vector data。 And what
    we need to do then is start。 writing a definition of the RNN that we want。 So
    notice that we've got the basic LSTM here that we can pull in。
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有一个内置的LSTM，我们实际上必须开始制作和创建我们自己的对象来表示RNN。我们将使用张量，张量再次是向量数据的表示。因此我们需要做的是开始编写我们想要的RNN的定义。所以请注意，我们有一个可以拉入的基本LSTM。
- en: But essentially， when we're using TensorFlow， we're going to have to have a
    little bit more knowledge。 about the underlying--， we're not given that high-level
    API interface where it just， says， hey。 here's the thing out of the box。 You actually
    have to go and take the components。
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 但本质上，当我们使用TensorFlow时，我们需要对底层有更多的了解——我们没有那个高级API接口，它直接说，嘿，这就是现成的东西。你实际上必须去获取组件。
- en: '![](img/86caa82786cf9dc042e1ddb9efafe87d_56.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86caa82786cf9dc042e1ddb9efafe87d_56.png)'
- en: and actually start building it yourself。 And likewise。 then when you're actually
    optimizing the weights， what you have to do is you actually。 have to run a session，
    a TensorFlow extension。 And notice， again， we have this epoch loop。 We're going
    to go ahead and do n iterations。 And then when we find the optimal weight。
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上开始自己构建它。同样，当你在优化权重时，你必须真正地运行一个会话，一个TensorFlow扩展。再次注意，我们有这个轮次循环。我们将继续进行n次迭代。当我们找到最佳权重时。
- en: what happens in both cases is you， can export those optimal weights once you've。
    found them after running so many epoch sessions。 Those weights， then。 let's say
    they're in a flat file， you can then import those for later use。 So TensorFlow。
    we don't have that high-level interface。 We've actually got to kind of meld some
    of those components。
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在这两种情况下发生的事情是，一旦找到最佳权重，你可以在运行了多次轮次会话后导出这些权重。这些权重，然后，假设它们在一个平面文件中，你可以导入这些以便日后使用。因此TensorFlow，我们没有那个高级接口。我们实际上需要将一些组件融合在一起。
- en: together ourselves。 But we understand what we're doing。 We've got to build the
    model。
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 我们要一起努力。但我们明白我们在做什么。我们必须构建模型。
- en: '![](img/86caa82786cf9dc042e1ddb9efafe87d_58.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86caa82786cf9dc042e1ddb9efafe87d_58.png)'
- en: We've got to fit the model。 We've got to train the model。 And with PyTorch，
    PyTorch actually。
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我们必须拟合模型。我们必须训练模型。而使用PyTorch时，PyTorch实际上。
- en: '![](img/86caa82786cf9dc042e1ddb9efafe87d_60.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86caa82786cf9dc042e1ddb9efafe87d_60.png)'
- en: gets a lot more interesting。 And you have a lot more flexibility。 to go in there
    and build things as you want。 So notice here， as we're building our RNN with PyTorch。
    notice how we're having to actually build the interface， for actually how forward
    propagation works。 So remember how you said data can move forward and backward？
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 变得更加有趣。你有更多的灵活性，可以进入并按自己的想法构建东西。因此，请注意，当我们用PyTorch构建RNN时，注意我们实际上在构建前向传播的接口。记住你说过数据可以向前和向后移动吗？
- en: Now you are able to actually go in and write that logic how。 that happens and
    how you're interacting。
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你可以实际进入并编写该逻辑，了解那是如何发生的，以及你是如何互动的。
- en: '![](img/86caa82786cf9dc042e1ddb9efafe87d_62.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86caa82786cf9dc042e1ddb9efafe87d_62.png)'
- en: with these other neurons。 So the interface actually is much more low level。
    And again。 when we're training， we have that great epoch loop， that we've come
    to know and love。 And also。 when we get our output， we can then export that output。
    We can then export that output to then import to our model， for later use as well。
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 以及这些其他神经元。所以接口实际上是更低级的。而且，当我们训练时，我们有那个伟大的周期循环，我们已经熟知并喜爱。而且，当我们得到输出时，我们可以导出该输出。然后我们可以将该输出导出以便稍后导入我们的模型。
- en: '![](img/86caa82786cf9dc042e1ddb9efafe87d_64.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86caa82786cf9dc042e1ddb9efafe87d_64.png)'
- en: So with those three neural networks examples， what actually can we start generating。
    with an RNN using Python？
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 那么有了这三个人工神经网络的例子，我们实际上可以开始用Python生成什么呢？
- en: '![](img/86caa82786cf9dc042e1ddb9efafe87d_66.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86caa82786cf9dc042e1ddb9efafe87d_66.png)'
- en: This is some of the most coherent texts， I actually started developing， which。
    is to say that it is a work in progress。 So one of the things I noticed is， as
    I。 started asking to make longer pieces of text， it just started getting really，
    really wild。 Remember the whale did that。 So some of the more coherent units of
    text。
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我实际开始开发的一些最连贯的文本，这意味着它仍在进行中。所以我注意到的一件事是，当我开始请求制作更长的文本时，它开始变得非常、非常混乱。记住鲸鱼的事情。所以一些更连贯的文本单元。
- en: I found was actually getting character strings of about 200。 So why is that？
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 我发现字符字符串大约为200。那么这是为什么呢？
- en: This kind of feels like it's in line， that it could be in a telenovela。 Don't
    threaten me。 Don't do you not understand me？ Whoa， whoa， whoa， whoa。 Right。 Maybe
    we're making that famous fight scene all over。
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 这感觉像是可以在肥皂剧中出现的情节。不要威胁我。你不明白我吗？哇，哇，哇，哇。对。也许我们正在重演那个著名的打斗场景。
- en: '![](img/86caa82786cf9dc042e1ddb9efafe87d_68.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86caa82786cf9dc042e1ddb9efafe87d_68.png)'
- en: But in thinking about the tooling that we need to build， think about those three
    questions。 How much technical expertise is needed to start with a framework？
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 但是在考虑我们需要构建的工具时，请考虑这三个问题。开始使用一个框架需要多少技术专长？
- en: I've told you that I'm new to deep learning or relatively new。 So I probably
    want something that's a little bit more， beginner-friendly， something that allows。
    me to build a small toy project quickly。 And something that has， for me， this，
    is super important。 a very， very good， robust user group that， gives lots of documentation
    and lots of examples。
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 我告诉过你，我对深度学习是新手或者相对新手。所以我可能希望有一些更适合初学者的东西，让我能够快速构建一个小玩具项目。而且对我来说，这一点是非常重要的，一个非常、非常好的、强大的用户群体，提供大量的文档和示例。
- en: of how to use it。 For my second question， what are my requirements？ Well， actually。
    because I focused on just looking， at those three English telenovelas。 I actually
    did not have that much data。 I had less than 100 megabytes。 And it was actually
    just looking at the raw text spoken word。
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 关于如何使用它。对于我的第二个问题，我的要求是什么？其实，因为我专注于观看那三部英语肥皂剧。我实际上没有那么多数据。我只有不到100兆字节。而且我只是查看了所说的原始文本。
- en: data itself that wasn't including things like sometimes。 there's cues and scripts
    that say something like， oh。 15-second pause with the dramatic stare to the side。
    So those weren't actually those kind of nitty-gritty pieces。 of information I
    actually didn't have in my text， that I was working with。 So thinking about that。
    I had a small data set， and something that I wanted to have some ability to debug。
    a little bit more friendly， let's continue， onto my third requirement， which is
    basically how easy。 is it to start using a framework。 And essentially， what I
    wanted was。
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 数据本身不包括一些东西，比如有时有提示和剧本，比如，哦，15秒的停顿，配合戏剧性的侧目凝视。所以这些其实并不是我在处理的文本中所拥有的那些细致的信息。因此考虑到这一点，我有一个小的数据集，并且我想要一些更友好的调试能力，接下来我们谈谈我的第三个要求，基本上就是使用框架的难易程度。实际上，我想要的是。
- en: to get something that allowed me just to be able to bootstrap， and get going，
    because I just wanted。 to know what it means to actually start working， with text，
    to start processing text。 and actually start doing text generation。 And all that
    to say that the one that won for me--。 notice my priorities？ Caris， Caris， Caris？
    Yes， I opted to work with Caris。
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 我想要一个可以让我启动并运行的东西，因为我只想知道实际上开始处理文本，开始生成文本意味着什么。所有这些都说明，我选择的那个——注意我的优先级？Caris，Caris，Caris？是的，我选择了与Caris合作。
- en: So what's really cool about TensorFlow， is you can actually--， Caris， you can
    plug in。 and you can use with TensorFlow。 And you can use TensorFlow and Caris
    quite nicely。
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 所以关于TensorFlow，真的很酷的是你可以实际地——Caris，你可以连接。你可以和TensorFlow一起使用。你可以非常顺利地使用TensorFlow和Caris。
- en: '![](img/86caa82786cf9dc042e1ddb9efafe87d_70.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86caa82786cf9dc042e1ddb9efafe87d_70.png)'
- en: So you can see it's really， really cool。 And you can see it's really cool。 And
    you can see it's really cool。 And you can see it's really cool。 And you can see
    it's really cool。 And you can see it's really cool。 And you can see it's really
    cool。 And you can see it's really cool。
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 所以你可以看到这真的非常，非常酷。你可以看到这真的很酷。你可以看到这真的很酷。你可以看到这真的很酷。你可以看到这真的很酷。你可以看到这真的很酷。你可以看到这真的很酷。你可以看到这真的很酷。
- en: And you can see it's really cool。 And you can see it's really cool。 And you
    can see it's really cool。 And you can see it's really cool。 And you can see it's
    really cool。 And you can see it's really cool。 And you can see it's really cool。
    And you can see it's really cool。
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到这真的很酷。你可以看到这真的很酷。你可以看到这真的很酷。你可以看到这真的很酷。你可以看到这真的很酷。你可以看到这真的很酷。你可以看到这真的很酷。你可以看到这真的很酷。
- en: And you can see it's really cool。 And you can see it's really cool。 And you
    can see it's really cool。 And you can see it's really cool。 And you can see it's
    really cool。 And you can see it's really cool。 And you can see it's really cool。
    And you can see it's really cool。
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到这真的很酷。你可以看到这真的很酷。你可以看到这真的很酷。你可以看到这真的很酷。你可以看到这真的很酷。你可以看到这真的很酷。你可以看到这真的很酷。你可以看到这真的很酷。
- en: '![](img/86caa82786cf9dc042e1ddb9efafe87d_72.png)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86caa82786cf9dc042e1ddb9efafe87d_72.png)'
- en: And you can see it's really cool。 And you can see it's really cool。 And you
    can see it's really cool。 And you can see it's really cool。 And you can see it's
    really cool。
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到这真的很酷。你可以看到这真的很酷。你可以看到这真的很酷。你可以看到这真的很酷。你可以看到这真的很酷。
- en: '![](img/86caa82786cf9dc042e1ddb9efafe87d_74.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86caa82786cf9dc042e1ddb9efafe87d_74.png)'
- en: And you can see it's really cool。 And you can see it's really cool。 And you
    can see it's really cool。 And you can see it's really cool。 And you can see it's
    really cool。 And you can see it's really cool。 And you can see it's really cool。
    And you can see it's really cool。
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到这真的很酷。你可以看到这真的很酷。你可以看到这真的很酷。你可以看到这真的很酷。你可以看到这真的很酷。你可以看到这真的很酷。你可以看到这真的很酷。你可以看到这真的很酷。
- en: And you can see it's really cool。 And you can see it's really cool。 And you
    can see it's really cool。 And you can see it's really cool。 And you can see it's
    really cool。 And you can see it's really cool。 And you can see it's really cool。
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到这真的很酷。你可以看到这真的很酷。你可以看到这真的很酷。你可以看到这真的很酷。你可以看到这真的很酷。你可以看到这真的很酷。你可以看到这真的很酷。
- en: '![](img/86caa82786cf9dc042e1ddb9efafe87d_76.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86caa82786cf9dc042e1ddb9efafe87d_76.png)'
- en: And you can see it's really cool。 And you can see it's really cool。 And you
    can see it's really cool。 And you can see it's really cool。 And you can see it's
    really cool。
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到这真的很酷。你可以看到这真的很酷。你可以看到这真的很酷。你可以看到这真的很酷。你可以看到这真的很酷。
- en: '![](img/86caa82786cf9dc042e1ddb9efafe87d_78.png)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86caa82786cf9dc042e1ddb9efafe87d_78.png)'
- en: And you can see it's really cool。 And you can see it's really cool。 And you
    can see it's really cool。 And you can see it's really cool。 And you can see it's
    really cool。 And you can see it's really cool。 And you can see it's really cool。
    And you can see it's really cool。
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到这真的很酷。你可以看到这真的很酷。你可以看到这真的很酷。你可以看到这真的很酷。你可以看到这真的很酷。你可以看到这真的很酷。你可以看到这真的很酷。你可以看到这真的很酷。
- en: And you can see it's really cool。 And you can see it's really cool。 And you
    can see it's really cool。 And you can see it's really cool。 And you can see it's
    really cool。 And you can see it's really cool。 And you can see it's really cool。
    And you can see it's really cool。
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到这真的很酷。你可以看到这真的很酷。你可以看到这真的很酷。你可以看到这真的很酷。你可以看到这真的很酷。你可以看到这真的很酷。你可以看到这真的很酷。你可以看到这真的很酷。
- en: And you can see it's really cool。 And you can see it's really cool。 And you
    can see it's really cool。 And you can see it's really cool。 And you can see it's
    really cool。 And you can see it's really cool。 And you can see it's really cool。
    And you can see it's really cool。
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到这真的很酷。你可以看到这真的很酷。你可以看到这真的很酷。你可以看到这真的很酷。你可以看到这真的很酷。你可以看到这真的很酷。你可以看到这真的很酷。你可以看到这真的很酷。
- en: And you can see it's really cool。 And you can see it's really cool。 And you
    can see it's really cool。 And you can see it's really cool。 And you can see it's
    really cool。 And you can see it's really cool。 And you can see it's really cool。
    And you can see it's really cool。
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到这真的很酷。你可以看到这真的很酷。你可以看到这真的很酷。你可以看到这真的很酷。你可以看到这真的很酷。你可以看到这真的很酷。你可以看到这真的很酷。你可以看到这真的很酷。
- en: And you can see it's really cool。 And you can see it's really cool。 And you
    can see it's really cool。 And you can see it's really cool。 And you can see it's
    really cool。 And you can see it's really cool。 And you can see it's really cool。
    And you can see it's really cool。
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到这真的很酷。你可以看到这真的很酷。你可以看到这真的很酷。你可以看到这真的很酷。你可以看到这真的很酷。你可以看到这真的很酷。你可以看到这真的很酷。你可以看到这真的很酷。
- en: '![](img/86caa82786cf9dc042e1ddb9efafe87d_80.png)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86caa82786cf9dc042e1ddb9efafe87d_80.png)'
- en: And you can see it's really cool。 And you can see it's really cool。 And you
    can see it's really cool。 And you can see it's really cool。 And you can see it's
    really cool。 And you can see it's really cool。 And you can see it's really cool。
    And you can see it's really cool。
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到这真的很酷。你可以看到这真的很酷。你可以看到这真的很酷。你可以看到这真的很酷。你可以看到这真的很酷。你可以看到这真的很酷。你可以看到这真的很酷。你可以看到这真的很酷。
- en: And you can see it's really cool。 And you can see it's really cool。 And you
    can see it's really cool。 And you can see it's really cool。 And you can see it's
    really cool。 And you can see it's really cool。 And you can see it's really cool。
    And you can see it's really cool。
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到这真的很酷。你可以看到这真的很酷。你可以看到这真的很酷。你可以看到这真的很酷。你可以看到这真的很酷。你可以看到这真的很酷。你可以看到这真的很酷。你可以看到这真的很酷。
- en: And you can see it's really cool。 And you can see it's really cool。 And you
    can see it's really cool。 And you can see it's really cool。 And you can see it's
    really cool。 And you can see it's really cool。 And you can see it's really cool。
    And you can see it's really cool。
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到这真的很酷。你可以看到这真的很酷。你可以看到这真的很酷。你可以看到这真的很酷。你可以看到这真的很酷。你可以看到这真的很酷。你可以看到这真的很酷。你可以看到这真的很酷。
- en: And you can see it's really cool。 And you can see it's really cool。 And you
    can see it's really cool。 And you can see it's really cool。 And you can see it's
    really cool。 And you can see it's really cool。
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到这真的很酷。你可以看到这真的很酷。你可以看到这真的很酷。你可以看到这真的很酷。你可以看到这真的很酷。你可以看到这真的很酷。
- en: '![](img/86caa82786cf9dc042e1ddb9efafe87d_82.png)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86caa82786cf9dc042e1ddb9efafe87d_82.png)'
- en: And you can see it's really cool。 And you can see it's really cool。 And you
    can see it's really cool。 And you can see it's really cool。 And you can see it's
    really cool。 And you can see it's really cool。 And you can see it's really cool。
    And you can see it's really cool。
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到这真的很酷。你可以看到这真的很酷。你可以看到这真的很酷。你可以看到这真的很酷。你可以看到这真的很酷。你可以看到这真的很酷。你可以看到这真的很酷。你可以看到这真的很酷。
- en: And you can see it's really cool。 And you can see it's really cool。
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到这真的很酷。你可以看到这真的很酷。
- en: '![](img/86caa82786cf9dc042e1ddb9efafe87d_84.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86caa82786cf9dc042e1ddb9efafe87d_84.png)'
- en: And you can see it's really cool。 And you can see it's really cool。 And you
    can see it's really cool。 And you can see it's really cool。 And you can see it's
    really cool。 And you can see it's really cool。 And you can see it's really cool。
    And you can see it's really cool。
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到这真的很酷。你可以看到这真的很酷。你可以看到这真的很酷。你可以看到这真的很酷。你可以看到这真的很酷。你可以看到这真的很酷。你可以看到这真的很酷。你可以看到这真的很酷。
- en: And you can see it's really cool。 And you can see it's really cool。 And you
    can see it's really cool。 And you can see it's really cool。 And you can see it's
    really cool。 And you can see it's really cool。 And you can see it's really cool。
    And you can see it's really cool。
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到这真的很酷。你可以看到这真的很酷。你可以看到这真的很酷。你可以看到这真的很酷。你可以看到这真的很酷。你可以看到这真的很酷。你可以看到这真的很酷。你可以看到这真的很酷。
- en: And you can see it's really cool。 And you can see it's really cool。 And you
    can see it's really cool。 And you can see it's really cool。 And you can see it's
    really cool。 And you can see it's really cool。 And you can see it's really cool。
    And you can see it's really cool。
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到这真的很酷。你可以看到这真的很酷。你可以看到这真的很酷。你可以看到这真的很酷。你可以看到这真的很酷。你可以看到这真的很酷。你可以看到这真的很酷。你可以看到这真的很酷。
- en: And you can see it's really cool。 And you can see it's really cool。 And you
    can see it's really cool。 And you can see it's really cool。 And you can see it's
    really cool。 And you can see it's really cool。 And you can see it's really cool。
    And you can see it's really cool。
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到这真的很酷。你可以看到这真的很酷。你可以看到这真的很酷。你可以看到这真的很酷。你可以看到这真的很酷。你可以看到这真的很酷。你可以看到这真的很酷。你可以看到这真的很酷。
- en: And you can see it's really cool。 And you can see it's really cool。 And you
    can see it's really cool。 And you can see it's really cool。 And you can see it's
    really cool。 And you can see it's really cool。 And you can see it's really cool。
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到这真的很酷。你可以看到这真的很酷。你可以看到这真的很酷。你可以看到这真的很酷。你可以看到这真的很酷。你可以看到这真的很酷。你可以看到这真的很酷。
- en: '![](img/86caa82786cf9dc042e1ddb9efafe87d_86.png)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86caa82786cf9dc042e1ddb9efafe87d_86.png)'
- en: And you can see it's really cool。 And you can see it's really cool。 And you
    can see it's really cool。
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到这真的很酷。你可以看到这真的很酷。你可以看到这真的很酷。
