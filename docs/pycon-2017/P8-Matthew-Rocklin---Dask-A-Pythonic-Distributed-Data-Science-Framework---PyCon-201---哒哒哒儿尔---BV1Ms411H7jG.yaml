- en: P8：Matthew Rocklin   Dask A Pythonic Distributed Data Science Framework   PyCon
    201 - 哒哒哒儿尔 - BV1Ms411H7jG
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: P8：马修·洛克林 Dask：一个Python分布式数据科学框架 PyCon 201 - 哒哒哒儿尔 - BV1Ms411H7jG
- en: There will be about five minutes for questions at the end， so hold off。 Great。
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 演讲结束后将有大约五分钟的提问时间，所以请稍等。太好了。
- en: '![](img/c756025e33a1d78115431bb7e1f1596e_1.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c756025e33a1d78115431bb7e1f1596e_1.png)'
- en: '[applause]， Oops， let''s close that down。'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[掌声]，哎呀，让我们关闭这个。'
- en: '![](img/c756025e33a1d78115431bb7e1f1596e_3.png)'
  id: totrans-4
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c756025e33a1d78115431bb7e1f1596e_3.png)'
- en: So， hi everyone。 My name is Matthew Rockman。 I work for Continuum Analytics。
    Continuum is a company that's a for-profit company。 and it's at the open source
    data science side of the Python ecosystem。 So。 they do consulting and training
    and such。 We also apply for grants。
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 大家好。我叫马修·洛克曼。我在Continuum Analytics工作。Continuum是一家营利性公司，专注于Python生态系统中的开源数据科学。因此，他们提供咨询和培训等服务。我们也会申请补助金。
- en: and those grants support open source software。 DASK is one such example of that。
    So。 I work in a project called DASK， which is library for parallel and distributed
    computing in Python。 Talk a little bit about that， and also generally about parallel
    computing in Python。 So。 this is going to be a combination of slides and of live
    demonstrations。
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 这些补助金支持开源软件。DASK就是这样一个例子。所以，我参与了一个叫DASK的项目，这是一个用于Python中并行和分布式计算的库。稍后会谈一谈这个项目，以及Python中的并行计算。此次演讲将结合幻灯片和现场演示。
- en: if the wireless plays nicely with us。 The slides are available at my webpage
    at MatthewRockman。com/slides/pycon-2017。 If I'm sort of boring， I want to sort
    of jump ahead。 So。 I come from like the NumPy side of the PyCon stack。 So， if
    you're sort of raising your hand。 if you're more familiar with NumPy than Django，
    and if you're more familiar with Django than NumPy。
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 如果无线网络能顺利运行的话。幻灯片可以在我的网页上找到，网址是MatthewRockman.com/slides/pycon-2017。如果我有点无聊，我想跳到下一部分。所以，我来自PyCon生态系统的NumPy部分。如果你比较熟悉NumPy而不是Django，或者比较熟悉Django而不是NumPy，请举手。
- en: yeah， that's great。 It's like 50/50。 This is a crazy conference。 So。 Jake mentioned
    this in the beginning of the keynote today， which I loved， that we are a mixed
    group。 This thing would be a talk about mixing a couple of those slides together。
    We're going to take the networking side of Python and use it alongside the data
    side of Python。
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，这很棒。差不多是50/50。这是个疯狂的会议。所以，Jake在今天开场的主题演讲中提到这一点，我很喜欢，我们是一个混合的小组。这次演讲将讨论将一些幻灯片混合在一起。我们将结合Python的网络侧和数据侧。
- en: to make a distributed computing system。 So， we have this really strong analytic
    system in Python。 This is libraries like NumPy and Pandas and Scikit Learn。 They're
    very fast， they're very intuitive。 scientists like them， analysts like them。 They
    have a flaw in which they were designed to run on a single core。 mostly， and on
    data that fits in RAM。 And that is becoming less than。
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个分布式计算系统。所以，我们在Python中有一个非常强大的分析系统。这些库包括NumPy、Pandas和Scikit Learn。它们速度很快，直观易用，科学家和分析师都喜欢它们。但它们的缺陷在于设计时主要是运行在单核上，并且数据需要适合RAM。这种情况正在变得不再适用。
- en: as we're sitting at larger and larger data sets and larger and larger clusters。
    we now want to expand those libraries， the whole ecosystem， beyond just single
    core processing。 That's hard to do。 So， we just want to parallelize a single library。
    we want to parallelize an ecosystem of thousands and thousands of packages。
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 随着我们处理的数据集越来越大，以及集群规模的不断扩大，我们现在希望将这些库和整个生态系统扩展到超越单核处理。这很难做到。因此，我们希望对单个库进行并行化。我们想要对成千上万的包进行并行化。
- en: '![](img/c756025e33a1d78115431bb7e1f1596e_5.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c756025e33a1d78115431bb7e1f1596e_5.png)'
- en: So， I'm going to steal some slides from Jake。 So， these are the slides Jake
    brought up in the keynote this morning of the Python scientific stack。 And there's
    Python at the core， there's some core libraries on top of that like NumPy。 On
    top of that we think like Pandas and SciPy and all these other slightly more peripheral
    packages。 As you go out， there's actually thousands of little small packages。
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，我要借用一些Jake的幻灯片。这些是Jake今天早上在主题演讲中提到的Python科学栈幻灯片。在核心是Python，上面有一些核心库，比如NumPy。在此之上，我们认为像Pandas和SciPy以及其他一些外围包。当你向外延伸时，实际上有成千上万的小包。
- en: packages for genomics and for people doing sequencing and people doing studying
    the sun。 And there's thousands of little small research groups that are all producing
    software。 using their expertise that they have alone to support lots of scientists。
    These are people doing real work helping the world。
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 这些软件包用于基因组学，供进行测序和研究太阳的人使用。还有成千上万的小型研究小组都在利用他们的专业知识来支持众多科学家。这些人正在做真实的工作，帮助这个世界。
- en: And we want to sort of parallelize not just NumPy or Pandas but everything。
    That's a big challenge。 That's sort of the challenge we're sort of faced with
    and are foolishly trying to solve。
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望不仅对 NumPy 或 Pandas 进行并行化，而是对所有内容进行并行化。这是一个巨大的挑战。这就是我们面临的挑战，我们傻乎乎地想要解决它。
- en: '![](img/c756025e33a1d78115431bb7e1f1596e_7.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c756025e33a1d78115431bb7e1f1596e_7.png)'
- en: These packages are also， they have custom algorithms， it's not just one kind
    of thing。 they'll have their own secret sauce。 They're made by smart people。 So。
    we need a parallel computing library that can， that can be flexible enough to
    handle both the。 you know， solar astronomers work and the genomicists work。
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 这些包也有自定义算法，不仅仅是一种东西。它们有自己的秘密配方。它们是由聪明的人制作的。所以，我们需要一个并行计算库，它能够灵活处理太阳天文学家和基因组学家的工作。
- en: It is familiar enough that it can be adopted by a large number of people。 This
    is again a challenge。 So， in order to try to solve this problem， I'm talking about
    DASK， something that I and others。 have worked on for the last few years and I
    hope you enjoy。 So， this talk will be sort of。 we're going to start from sort
    of the more NumPy Pandas-y side。 So。
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这足够熟悉，以至于可以被大量人群采用。这再次是一种挑战。因此，为了解决这个问题，我要谈谈 DASK，这是我和其他人过去几年里一直在研究的内容，希望你会喜欢。所以，这次演讲将从更偏向
    NumPy 和 Pandas 的方面开始。
- en: people who are more NumPy Pandas will be sort of happier， more comfortable in
    the beginning。 And we're going to end up more towards the sort of tornado networking
    concurrency side towards the end。 So， hopefully there's something for everyone。
    We're talking about first parallelizing NumPy and Pandas。 which is sort of the
    first intent of DASK。 Then we're going to talk about parallelizing more general
    code。
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 更熟悉 NumPy 和 Pandas 的人一开始会比较开心，更容易适应。我们将在最后更多地转向类似于 Tornado 网络并发的方向。因此，希望能满足每个人的需求。我们首先讨论如何对
    NumPy 和 Pandas 进行并行化，这也是 DASK 的首要目标。然后我们将讨论如何并行化更通用的代码。
- en: which ends up being necessary for one of， parallelized lots of packages。 We'll
    talk about task scheduling and task graphs and sort of compare with other computing
    systems that can do the sort of things。 things like Spark or Airflow。 We'll talk
    more about how DASK solves this problem。 We'll finish up with some sort of general
    talking about Python APIs and protocols that it's useful to adhere to in order
    to gain better adoption。
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这对于并行化许多软件包是必要的。我们将讨论任务调度和任务图，并与其他能够执行类似任务的计算系统进行比较，比如 Spark 或 Airflow。我们将更深入地讨论
    DASK 如何解决这个问题。最后，我们会稍微谈谈 Python API 和协议，这些是有助于获得更好采用的。
- en: And then we'll sort of finish with final thoughts。 So first。 we're going to
    parallelize NumPy and Pandas。 Who has used NumPy or Pandas？
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们将以最后的思考结束。因此，首先，我们将对 NumPy 和 Pandas 进行并行化。有谁使用过 NumPy 或 Pandas？
- en: Who used NumPy or Pandas in the last three days？ Okay， so there's some familiarity。
    I'm glad to see that。 So， talk about two sub-modules of DASK。 So， this is a DASK
    array。 which is a multi-dimensional array composed of many small NumPy arrays。
    I'm looking at climate data。 I have the temperature of the earth every square
    mile for the entire earth。
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 最近三天里有谁使用过 NumPy 或 Pandas？好吧，有些人是熟悉的。很高兴看到这一点。因此，谈谈 DASK 的两个子模块。这是一个 DASK 数组，它是由许多小的
    NumPy 数组组成的多维数组。我在查看气候数据。我有地球每平方英里的温度数据。
- en: That might be a very large array。 I might not be able to put that array inside
    of one computer。 It's not going to block that array up into many different pieces
    and put them on different computers or on my hard drive。 And DASK array is going
    to logically coordinate all of those NumPy arrays。 So。 DASK array is a collection
    of any NumPy arrays。 And DASK array is going to， when you type in。
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能是一个非常大的数组。我可能无法将这个数组放入一台计算机中。它不会把这个数组分成许多不同的部分，放在不同的计算机或我的硬盘上。DASK 数组将逻辑上协调所有这些
    NumPy 数组。因此，DASK 数组是任何 NumPy 数组的集合。当你输入时，DASK 数组会处理这些。
- en: you know， DASK array。sum， it's going to figure out how to compute that sum by
    doing all this coordination with a NumPy arrays。 Similarly， other products like
    DASK DataFrame， which is a logical connection of many Pandas DataFrames。 So， for
    example， we might have a time series and the data for every month is quite large。
    That can fit maybe on one computer。 We need to use many computers to store our
    entire dataset。
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 你知道，DASK数组.sum，它会通过与NumPy数组进行所有这些协调来计算和。同样，其他产品如DASK DataFrame，它是许多Pandas DataFrame的逻辑连接。因此，例如，我们可能有一个时间序列，而每个月的数据相当大。那可能适合在一台计算机上。我们需要使用多台计算机来存储我们的整个数据集。
- en: We need to use our hard drive， but it's for many， many， many months。 So。 what
    DASK DataFrame DASK array provides is they provide an interface that is very。
    very similar to NumPy and Pandas， but it operates in parallel。 either on your
    laptop with many cores， scaling out on your disk。
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要使用我们的硬盘，但这需要很多、很多、很多个月。因此，DASK DataFrame和DASK数组提供的是一个接口，这个接口与NumPy和Pandas非常相似，但它并行操作。要么在你拥有多个核心的笔记本电脑上，要么在你的磁盘上扩展。
- en: or across a cluster using many computers。 Additionally。 because they're using
    NumPy and Pandas under the hood， a lot of things work very easily with them。 So，
    it's a very sort of lightweight change to your code base。 So。 I'm going to switch
    to a couple of examples。 Hopefully。
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 或跨多个计算机集群使用。此外，因为它们在底层使用NumPy和Pandas，所以很多事情与它们一起工作非常简单。因此，这对你的代码库是一个非常轻量的改变。所以，我将切换到几个示例。希望。
- en: the wireless is working a little bit better today than yesterday。
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 无线网络今天比昨天稍微好一点。
- en: '![](img/c756025e33a1d78115431bb7e1f1596e_9.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c756025e33a1d78115431bb7e1f1596e_9.png)'
- en: So， I have here a cluster running on Google， Google Computer Engine。 And this
    cluster has 32 machines。 Each machine has two cores。 And I'm going to create an
    array。 Let's make that a little bit larger。 I'm going to create array， which is
    1，000 by 1，000， or 10。000 by 1，000， but composed of NumPy arrays or 1，000 by 1，000。
    So。
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，我这里有一个在Google计算引擎上运行的集群。这个集群有32台机器。每台机器有两个核心。我将创建一个数组。让我们把它做得大一点。我将创建一个1,000乘1,000的数组，或者10,000乘1,000，但由NumPy数组或1,000乘1,000组成。因此。
- en: sort of a 10 by 10 grid of NumPy arrays。 Each element of that grid is a NumPy
    array， 1，000 by 1，000。 And now， when I compute that on my cluster， DAS has gone
    ahead and has computed all those NumPy arrays for me。 We've seen that here on
    the left。 So， we're going to see these plots a fair amount throughout the talk。
    So， I have 64 cores， and they're on this axis here。 So。
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 一种10乘10的NumPy数组网格。该网格的每个元素是一个1,000乘1,000的NumPy数组。现在，当我在我的集群上计算时，DASK已经为我计算了所有这些NumPy数组。我们在左侧看到过这一点。因此，在整个演讲中，我们将会看到这些图表。所以，我有64个核心，它们在这个轴上。
- en: every line here is what a core has been doing over time。 And we see that this
    core called the NumPy random function a couple of times。 and created a couple
    NumPy arrays。 So， on my 32 machines， I have 100 NumPy arrays in memory on them。
    spread around them。 If I do something， let's say I'll compute the sum of this
    array。
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的每一行都是一个核心随时间所做的工作。我们看到这个核心调用了NumPy随机函数几次，并创建了几个NumPy数组。因此，在我的32台机器上，我有100个NumPy数组分散在它们上。如果我做某事，比如说我将计算这个数组的和。
- en: It's a really simple computation。 DAS is going to have to compute the sum of
    all the intermediate arrays。 It's going to transfer some of those intermediate
    values to other machines。 That's the red。 Red is data transfer。 And it's going
    to compute some final result。 And it gives us back a nice answer in a sort of
    fast time。 It's more complex。
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个非常简单的计算。DASK将计算所有中间数组的和。它会将一些中间值转移到其他机器。这是红色。红色是数据传输。它将计算一些最终结果，并以相对快速的时间给我们一个不错的答案。这更复杂。
- en: I'll try to scroll this down to the back。 Do something more complex。 If you
    are familiar with NumPy。 this syntax should look familiar to you。 And so， we can
    do a lot of these NumPy computations。 what look like NumPy computations， but are
    actually spread across a cluster。 So。 this is sort of the highest level use of
    DASK。 It looks like NumPy。
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我会尽量将这个向下滚动。做一些更复杂的事情。如果你熟悉NumPy，这个语法应该对你来说很熟悉。因此，我们可以做很多看似NumPy计算的事情，但实际上是分布在集群上的。所以，这大概是DASK的最高级别的使用。它看起来像NumPy。
- en: but it actually runs on terabytes of data， if you wish。 Certainly。 we'll consider
    a Pandas data frame example。 So， here I have a bunch of CSV files on Google Storage。
    It's going to go on S3 or on Amazon or as you like。 And this is the New York City
    Taxicab data set。 It's around 20 gigs on disk， around 60 gigs in RAM。 So， it's
    too big for my laptop。
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 但如果你愿意，它实际上可以在数TB的数据上运行。当然。我们将考虑一个 Pandas 数据框的例子。在这里，我有一堆位于 Google 存储上的 CSV
    文件。它将放在 S3 上或在亚马逊上，随你喜欢。这是纽约市出租车数据集。它在磁盘上大约 20GB，在 RAM 中大约 60GB。因此，这对于我的笔记本电脑来说太大了。
- en: But I can read a little bit of it with Pandas。 And this shows all of the cab
    rides in the City of New York through your 2015。 So， how many press-inders were
    in the cab？ When did they take off？ When did they stop？
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 但我可以用 Pandas 读取它的一小部分。这显示了2015年纽约市的所有出租车行程。那么，出租车里有多少乘客？他们什么时候出发？他们什么时候停下？
- en: As well as a breakdown of the fare。 So， it's a large time series data set。 maybe
    like you've sort of seen before。 It's sort of convenient because it's easy to
    understand and it's inconveniently large。 So， I can't read it all on one machine，
    but I can read it on many machines。 So， here。 rather than use the Pandas read
    CSV function， I've used the DASK data frame read CSV function。
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 以及费用的细分。因此，这是一个大型时间序列数据集。也许你以前见过。它有点方便，因为易于理解，但又不方便地大。我不能在一台机器上全部读取，但我可以在多台机器上读取。因此，在这里。与其使用
    Pandas 读取 CSV 函数，我使用了 DASK 数据框读取 CSV 函数。
- en: It has the same API， but breaks down my computation。 It takes those 12 CSV files
    and breaks it into hundreds of blocks of bytes。 And then calls the Pandas read
    CSV function and all those blocks of bytes。 And so。 what you're seeing here on
    the left is all the machines in my cluster are busy creating。
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 它具有相同的 API，但分解了我的计算。它将那 12 个 CSV 文件分解成数百个字节块。然后调用 Pandas 读取 CSV 函数和所有这些字节块。因此，您在左侧看到的都是我的集群中的所有机器都在忙于创建。
- en: Pandas data frames in their local memory。 The object we get back isn't a Pandas
    data frame。 it's a DASK data frame。 Which looks and feels the same。 And as we
    operate on that DASK data frame。 DASK will do the coordination to make sure all
    the Pandas， data frames are affected accordingly。 So。 again， DASK is sort of like
    a very good secretary。 So， not actually doing any computation。
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在它们的本地内存中生成 Pandas 数据框。我们得到的对象不是 Pandas 数据框，而是 DASK 数据框。它看起来和感觉都一样。当我们对这个 DASK
    数据框进行操作时，DASK 会协调以确保所有 Pandas 数据框都相应地受到影响。因此，再次强调，DASK 就像一个非常好的秘书。因此，实际上并不进行任何计算。
- en: it's coordinating many Pandas data frames to do， the right computations。 So，
    you know， this。 because it's all built on Pandas， it looks familiar to all the
    D-type， sniffing。 we like from Pandas read CSV。 And， you know， it runs the same
    way， it sort of renders the same way。 It should feel relatively familiar。 And
    this is really important if you want to get all of the people in the Python ecosystem
    to use it。
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这是协调多个 Pandas 数据框以进行正确计算。因此，你知道，这一切都是基于 Pandas 构建的，对所有 D 型的嗅探者来说看起来都很熟悉。我们喜欢从
    Pandas 读取 CSV。而且，你知道，它的运行方式相同，渲染方式也相似。它应该让人感觉相对熟悉。如果你想让整个 Python 生态系统的人都使用它，这一点非常重要。
- en: They tend to like Pandas， it turns out。 So， you know， simple computation。 we
    might compute the length of that data frame。 And that's， you know， that's done
    relatively simply。 we compute the lengths， all the intermediate values。 So， if
    you sort of zoom in over here。 We compute the length of this particular Pandas
    data frame， and it took， you know。
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 他们似乎喜欢 Pandas，事实证明。因此，你知道，简单的计算。我们可能会计算那个数据框的长度。那是相对简单的。我们计算所有中间值的长度。因此，如果你稍微放大一下。我们计算这个特定
    Pandas 数据框的长度，它花费了，你知道的。
- en: a few microseconds。 And we did that a few hundred times。 And we communicated
    some data over one machine， and we called it， we did this one。 It was a very simple
    computation， but we could do something more complex。 Here we'll see how well New
    Yorkers tip。 So， we're going to remove some bad rows。
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 几微秒。我们这样做了几百次。我们在一台机器上传输了一些数据，并且我们称之为，我们做了这个。这是一个非常简单的计算，但我们可以做更复杂的事情。在这里，我们将看到纽约人给小费的情况。因此，我们将去掉一些不良行。
- en: There were like some free rides in New York City， it turns out。 We're going
    to create a new column。 which is the tip fraction。 So， you know， the division
    of the tip versus the fair。 was it a 10% tip or 20% tip？ We're going to group
    by the hour of the day and by the day of the week。 and we're going to see the
    average of the tip fraction。
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 原来在纽约市有一些免费乘车的情况。我们将创建一个新列，即小费比例。所以，你知道，小费与车费的比率，是否是10%的小费或20%的小费？我们将按照一天中的小时和一周中的天进行分组，看看小费比例的平均值。
- en: '![](img/c756025e33a1d78115431bb7e1f1596e_11.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c756025e33a1d78115431bb7e1f1596e_11.png)'
- en: So， how well did New Yorkers tip？ Grouped by hour of day。 And what this does
    is produces， you know。 thousands of Python functions that then ran on our cluster。
    And it gives us a nice result。 So。 you know， data frame turned this Pandas-like
    computation into thousands of Python functions that had to run。 All of our Pandas
    data frames。 And then the Dask task scheduler ran all those functions for us in
    about like three or four seconds。
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，纽约人的小费水平如何呢？按照一天中的小时进行分组。这产生了成千上万的Python函数，这些函数在我们的集群上运行，并给我们带来了很好的结果。所以，你知道，数据框将这种类似Pandas的计算转变为成千上万的Python函数，需要运行所有的Pandas数据框。然后，Dask任务调度器在大约三到四秒内为我们运行了所有这些函数。
- en: We'll get back to the nice results that New Yorkers tip relatively generously
    as a recent migrant in New York。 I feel now proud of this。 It's about 20% on average
    with a startling spike at 4am。 It's 38% on average tips at 4am。 I understand this
    is last call at the bars。 But so， you know。 we did some data science on a large
    in-unit data set with APIs that we already knew on a cluster that was relatively
    easy to set up。
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将回到纽约人相对慷慨的小费这一美好结果，作为近期在纽约的移民，我对此感到自豪。平均而言，小费大约为20%，在凌晨4点时有一个惊人的峰值，达到38%。我知道这时是酒吧的最后叫酒时间。不过，正如你所知，我们对一个相对容易设置的集群上的大规模内部数据集进行了数据科学分析。
- en: So this is sort of like the first big hurrah of Dask。 Dask gives you a paralyzed
    or distributed NumPy or a Pandas data frame。 And for some people that is useful。
    However， it turns out， so we thought this was useful too。 And then we took it
    to academic groups and to companies and we said， "Hey， isn't this useful？"。
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这就像是Dask的第一次重大成功。Dask为你提供了一个并行或分布式的NumPy或Pandas数据框。对某些人来说，这是有用的。然而，事实证明，我们也认为这是有用的。然后我们把它带给学术团体和公司，问他们：“嘿，这不是很有用吗？”
- en: And I said， "Yeah， but our problems are actually a little more complex。"。 It
    turns out that not all problems are a big data frame or a big array or a big list。
    Instead。 people often had code look like this。 They sort of just normal Python
    code with four loops。 This is very clearly parallelizable。 But it's not clearly
    a data frame and array computation。
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我说：“是的，但我们的问题实际上要复杂一些。”事实证明，并不是所有问题都是一个大数据框或一个大数组或一个大列表。相反，人们的代码通常看起来像这样。它们就是普通的Python代码，有四个循环。这显然是可以并行化的。但它并不明显是数据框和数组的计算。
- en: And so then our first approach was to sort of try to force this into being a
    data frame computation。 Maybe sort of two data frames and we're joining them and
    doing sort of a filter。 That was an awkward process。 So instead， we developed
    other libraries or other modules of Dask that can work on more generic code。 So
    the system underneath Dask array or Dask data frame， this is doing all the coordination。
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们的第一种方法是尝试将其强制转换为数据框计算。可能是两个数据框，我们在进行连接和过滤。这是一个尴尬的过程。因此，我们开发了Dask的其他库或模块，可以处理更通用的代码。Dask数组或Dask数据框之下的系统正在进行所有协调。
- en: the secretary part。 That is actually fairly flexible。 And as long as we can
    expose that with more flexible， more fine-grained APIs。 we can parallelize far
    more clever and far more custom code。 So this actually ends up being far more
    used in practice than the big data frames or bigger arrays。
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 秘书部分。实际上，这是相当灵活的。只要我们能够用更灵活、更细化的API来暴露这一点，我们就能并行化更多聪明且定制的代码。因此，这在实践中使用得比大数据框或更大的数组要多得多。
- en: At least by sort of larger institutions。 So let's see an example。
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 至少是由更大的机构进行的。让我们看看一个例子。
- en: '![](img/c756025e33a1d78115431bb7e1f1596e_13.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c756025e33a1d78115431bb7e1f1596e_13.png)'
- en: I'm actually going to switch not to the cluster but just to my local machine。
    This runs on a cluster just fine but I want to show you that it is easy to use
    Dask on your laptop。 My goal at the end of this talk is to be able to use Dask
    on their laptop and sort of start playing with it。 So instead of connecting out
    to a particular cluster， normally when we create a client。
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我实际上将切换，不是到集群，而是直接到我的本地机器。这在集群上运行得很好，但我想向你展示在你的笔记本上使用Dask是很简单的。我的目标是让大家能够在他们的笔记本上使用Dask，并开始尝试。所以通常，当我们创建一个客户端时，并不会连接到特定的集群。
- en: we'll talk about this in a little bit。 We put in some address of sort of the
    head node of our scheduler。 In this case， we're not going to add anything。 That's
    a signal to Dask that it should create something locally。 So Dask is going to
    create sort of little Dask cluster on our laptop。 Actually， I'm sorry。 I don't
    want this notebook at all。 I want the other one。 So， forget what I was just saying。
    So。
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们稍后会谈论这个。我们输入一些调度器头节点的地址。在这种情况下，我们不打算添加任何内容。这是一个信号，告诉Dask它应该在本地创建一些东西。因此，Dask将在我们的笔记本上创建一个小型Dask集群。实际上，我很抱歉。我根本不想要这个笔记本。我想要另一个。所以，忘记我刚刚说的话。
- en: I have some functions here。 That simulate work。 So they're going to do a small
    bit of work but they're going to sleep for a while。 This is maybe some code you're
    writing on your own。 Inc adds a number adds one to a number。 decrements， removes
    one from a number and adds two numbers together。 I can call these locally。 Let's
    clear this out。 I can call these locally and they take a couple of seconds probably。
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我这里有一些函数，模拟工作。因此，它们将进行少量工作，但会休眠一段时间。这可能是你自己编写的某些代码。inc增加一个数字，decrements从一个数字中减去一，并将两个数字相加。我可以在本地调用这些。让我们清除一下。我可以在本地调用这些，可能需要几秒钟。
- en: sort of randomly。 Or I can import Dask and I can annotate those functions to
    be delayed。 That's what I mean here is that when we call that function now， we're
    not going to actually。 execute the code。 So we're going to put that function and
    its arguments inside of a task graph that we can。 execute later。 So now when I
    run that code， same code from before。
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 有点随机。或者我可以导入Dask，并可以对这些函数进行延迟标注。这就是我这里的意思，当我们现在调用那个函数时，我们并不会实际执行代码。因此，我们将那个函数及其参数放入一个任务图中，以便我们可以稍后执行。所以现在当我运行那段代码时，之前的同样代码。
- en: it computes immediately but hasn't actually done any work yet。 So instead of
    produced this object Z which holds onto a recipe of how it should be computed。
    whenever we want it to be done。 And so when I call compute on this object Z。 it's
    now going to run in parallel just on my， laptop。 Not on the cluster， not on any
    setup。
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 它立即计算，但实际上还没有做任何工作。因此它产生了这个对象Z，它保留了如何计算的配方。无论何时我们想完成它。因此，当我在这个对象Z上调用计算时，它现在将在我的笔记本电脑上并行运行。不是在集群上，也不是在任何设置上。
- en: I haven't set anything up here。 I imported Dask。 I called compute。 By default
    this ran in a thread pool。 By running a process pool or other things as well。
    But this is an interface that we can use to build up parallel computations in
    a sort of more python-like way。 Now I'm going to set up a cluster。 Again， just
    on my local laptop。
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我在这里没有设置任何内容。我导入了Dask。我调用了计算。默认情况下，这在一个线程池中运行。也可以运行一个进程池或其他东西。但这是我们可以用来以更像Python的方式构建并行计算的接口。现在我将设置一个集群。同样，只是在我的本地笔记本上。
- en: And one of the nice things is it gives me this nice dashboard page。 This is
    a bokeh dashboard。 a bokeh web application。 People are interested。 That must have
    been Sarah Bird。 There you are。 Thank you， Sarah。 And then we can run this on
    this dashboard。 We can see it run。 So we call it decrement on one of our threads。
    We call it increment on another thread。
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 其中一个好处是它给我提供了一个不错的仪表板页面。这是一个bokeh仪表板，一个bokeh网络应用程序。人们对此很感兴趣。那一定是**莎拉·伯德**。你来了，谢谢你，莎拉。然后我们可以在这个仪表板上运行它。我们可以看到它的运行。因此，我们在其中一个线程上调用它的递减。我们在另一个线程上调用它的递增。
- en: We then this little red bit is communication between two different threads。
    And we call it add on the results。 So Dask again is figuring out when to call
    things moving data around。 Doing other parts of parallel computing you don't want
    to think about。 While still letting you give you the freedom to do the things
    that you do。 So this is sort of neat。
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 然后这个小红点是两个不同线程之间的通信。我们在结果上调用它的加法。因此，Dask再次在确定何时调用这些内容、移动数据等方面做出决定。处理其他并行计算的部分，而不必思考。同时仍然让你有自由去做你想做的事情。所以这很不错。
- en: It becomes a lot better when you have four loops and you can make sort of arbitrary
    complex things。 So here I have got lots more computations and we are filling up
    these processes and doing。 lots of work。
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 当你有四个循环并且可以做一些任意复杂的事情时，这变得更好。所以我有更多的计算，我们正在填满这些进程并进行大量工作。
- en: '![](img/c756025e33a1d78115431bb7e1f1596e_15.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c756025e33a1d78115431bb7e1f1596e_15.png)'
- en: I am noticing from these progress parts it is going to take a while。 So I am
    going to start up some more workers pointing it to this scheduler。 So this is
    making 10 processes which we will have for threads running。
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我从这些进度部分注意到，这将需要一些时间。所以我要启动更多的工作线程，并将其指向这个调度器。这将生成10个进程，我们将用于运行线程。
- en: '![](img/c756025e33a1d78115431bb7e1f1596e_17.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c756025e33a1d78115431bb7e1f1596e_17.png)'
- en: So we are connecting it to the scheduler and now we are seeing the tasks that
    are responding。 to that is parallelizing nicely。 I could remove them。 Dask is
    resilient。 It can scale。 It can do all the nice things you want to cluster。 So
    now that we have this ability let's do something a little more complex。
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们正在将其连接到调度器，现在我们看到正在响应的任务。并行化得很好。我可以将它们移除。Dask 是弹性的。它可以扩展。它可以做你想要集群的所有美好事情。既然我们有了这个能力，让我们做一些稍微复杂的事情。
- en: We have all these numbers across all these processes on my laptop。 I am going
    to add them together。 One way to do this is just to call some and all of them。
    I have to migrate to one machine and add them together in one machine。 But there
    is something more fancy。 Let's add them up pair by pair。 The first two add those
    together。
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在我的笔记本电脑上有这些数字。我要把它们加在一起。一种方法就是调用一些并将它们全部相加。我必须迁移到一台机器上，将它们在一台机器上相加。但还有更复杂的方法。让我们逐对相加。前两个加在一起。
- en: The second two add those together。 So here is some Python code that does that。
    And this Python code is not a Dask code。 It is just Python code。 If you look at
    this for about a minute I think you will probably understand what it is doing。
    It has a list of values and it goes through that list and adds neighboring pairs
    in that list together。
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 第二组将那些加在一起。所以这里有一些 Python 代码可以做到这一点。这段 Python 代码不是 Dask 代码，而只是普通的 Python 代码。如果你看这一段大约一分钟，我想你会理解它在做什么。它有一个值的列表，并逐对相加。
- en: It keeps doing that while the list is longer than one。 We can visualize that
    result。 Maybe you can see that。 The graph we get is showing us the computation
    we want to compute。 You can see that it is indeed producing the kind of computation
    we wanted to do。 It looks like this image up here。 Nothing has run yet。
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 当列表的长度大于一时，它会继续这样做。我们可以可视化那个结果。也许你能看到这一点。我们得到的图表显示了我们想要计算的内容。你可以看到它确实产生了我们想要的计算。它看起来像上面的这张图。还没有任何东西运行。
- en: We have created a recipe for how to do these things。 If we ask Dask to compute
    that result for us it is going to use our cluster and use all。 of those processes
    to do that。 It is communicating between processes when necessary。 In the beginning
    there is lots of work to do。 Towards the end we have fewer and fewer ads to do。
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经创建了一个如何做这些事情的配方。如果我们让 Dask 为我们计算那个结果，它将使用我们的集群并利用所有这些进程。它在必要时在进程之间进行通信。一开始有很多工作要做。到最后，我们需要完成的加法会越来越少。
- en: If you like to come to the continuum booth I will give you this demo。 This demo
    is demonstrating two things。 One big data frames。 Two。 the ability to write your
    own Python code and paralyze it。 It is being a lot more attractive。 A lot more
    pragmatic。
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想来 Continuum 的展位，我会给你这个演示。这个演示展示了两件事。第一，大数据框。第二，能够编写你自己的 Python 代码并将其并行化。这变得更具吸引力，也更实用。
- en: '![](img/c756025e33a1d78115431bb7e1f1596e_19.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c756025e33a1d78115431bb7e1f1596e_19.png)'
- en: Let's go back to slides。 Let's dive deeper into what Dask is doing。 Let's explain
    tasks。 Dask does two things for you。 One， it produces tasks。 You write data frame
    code and it figures out a recipe for how to execute that code。 Two。 given such
    a graph and a cluster or laptop it figures out how to execute that graph in， parallel。
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回到幻灯片。让我们更深入地探讨 Dask 在做什么。让我们来解释任务。Dask 为你做两件事。第一，它生成任务。你编写数据框代码，它会找出执行该代码的配方。第二，给定这样的图和一个集群或笔记本电脑，它会找出如何并行执行该图。
- en: Those are two very different problems。 Really numpy pandas like the first kind
    of problem and tornado。 acing concurrency people like the， second kind of problem。
    Two things happening once。 We talk about the first part and second part。 Task
    graphs。 Is this font visible in the back？ No。 Does this mean it is visible or
    make it bigger？ It is good。 I am sitting some bads。
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种问题截然不同。实际上，numpy pandas 喜欢第一类问题，而 tornado。并发处理者则喜欢第二类问题。两件事情同时发生。我们谈论第一部分和第二部分。任务图。这个字体在后面可见吗？不。这是否意味着它不可见或者需要放大？可以的。我有些不好的感觉。
- en: Let's see what we can do。 Reveal is not -- there we go。 A little bit better
    hopefully。 I think it is about as good as I can go。 We are going to do a simple
    array computation and see how Dask is producing into a task graph。 I am creating
    with numpy I might create an array of 15 ones with a numpy one's function。 It
    produces a result like that。 Let's say I only have 20 bytes on my computer and
    split this up between many computers。
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看我们能做些什么。揭示并不 -- 好吧。希望能稍微好一点。我认为这已经是我能做到的极限了。我们将进行一个简单的数组计算，看看 Dask 如何生成任务图。我正在使用
    numpy 创建一个包含 15 个 1 的数组，使用 numpy 的 ones 函数。它会产生这样的结果。假设我在电脑上只有 20 字节，并将其分配到多台计算机之间。
- en: Then I might use the Dask array once function。 We are still creating array 15
    ones but we are chopping up into three arrays。 Each arrays of chunk size 5。 We
    are calling the numpy one's function three times and producing three numpy arrays。
    If I were to call something like sum on that array， Dask will know what I have
    to call sum。 and all the numpy one's function arrays。 We are going to compute
    the sum on all of them。
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我可能会使用 Dask array 的 ones 函数。我们仍在创建 15 个 1 的数组，但我们将其切割成三个数组。每个数组的块大小为 5。我们调用
    numpy 的 ones 函数三次，生成三个 numpy 数组。如果我对这个数组调用求和，Dask 将知道我需要调用求和。以及所有的 numpy ones
    函数数组。我们将对它们全部计算总和。
- en: We are seeing numpy code produce task graphs。 Now this is more interesting if
    we go to two dimensions。
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到 numpy 代码生成任务图。如果我们转到二维，那就更有趣了。
- en: '![](img/c756025e33a1d78115431bb7e1f1596e_21.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c756025e33a1d78115431bb7e1f1596e_21.png)'
- en: I am creating a 15 by 15 array that is composed of numpy arrays of size 5 by
    5。 I have nine blocks。 If I sum along one axis， I get the same summing behavior。
    This is something you can do with NAPRODUCE with Spark。 Numpy array computation
    becomes more complex。 Taking that array and adding it to its transpose。
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我正在创建一个 15 x 15 的数组，由大小为 5 x 5 的 numpy 数组组成。我有九个块。如果我在一个轴上求和，我会得到相同的求和行为。这是你可以用
    Spark 的 NAPRODUCE 做的事情。Numpy 数组计算变得更复杂。将这个数组与其转置相加。
- en: Numpy people will be interested to see that there is on the odd diagonal nodes
    only talk。 to themselves and off diagonal talk to their transpose neighbor。 This
    difference is actually where I will get to this in a bit。 If you look at the data
    base systems like Spark or Storm， when they are losing these symmetries。
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: Numpy 用户会对奇数对角节点之间只与自己交流而非对角线节点与其转置邻居交流感兴趣。这种差异实际上在后面会提到。如果你看一下像 Spark 或 Storm
    这样的数据库系统，当它们失去这些对称性时。
- en: that is being very good when you break down symmetries。 It handles generic cases。
    We can work on plex。 We can multiply。 We can keep going。 This is a simple thing
    to do。 If you look at the climate scientists， they do things that are ten hundred
    times more complex。 than they produce graphs with millions of nodes。 We have made
    this graph。
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 当你打破对称性时，这是非常好的。它处理通用情况。我们可以在复杂上工作。我们可以乘法运算。我们可以继续进行。这是一个简单的事情。如果你看看气候科学家，他们做的事情要复杂十倍百倍。他们生成带有数百万节点的图。我们已经制作了这个图。
- en: Now let's go ahead and compute it。 Again， whoops。 That's not topping at all。
    This object， why。 is still lazy。 We haven't done any work yet。 When I call white。compute，
    the answer is zero。 It is not surprising that the answer is zero。 What is interesting
    is that it returned the answer in about 47 milliseconds。 In that time， four threads，
    went through every circle in this task graph， ran that function。
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们继续计算。又来了，哎呀。这根本没有顶起来。这个对象，为什么。仍然是懒惰的。我们还没有做任何工作。当我调用 white.compute 时，答案是零。答案为零并不令人惊讶。有趣的是，它在大约
    47 毫秒内返回了答案。在这段时间里，四个线程遍历了这个任务图中的每个圆圈，执行了该函数。
- en: passed it to other functions， ran those functions， deleted the rules they didn't
    have to do。 and proceeded through that graph。 That's roughly what the task scheduler
    does。 That's the execute graph and give us a number back。 Ideally， in a short
    amount of time。 we're supporting interactive people， who are sitting at the computer
    typing at it。
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 将其传递给其他函数，运行这些函数，删除那些不需要做的规则，然后继续遍历该图。这大致就是任务调度程序所做的。执行图并给我们一个数字返回。理想情况下，在短时间内。我们支持那些坐在电脑前打字的互动用户。
- en: waiting for result back。 We want to be very fast。 Let's go here。 Great。 Systems
    like to ask a writer to ask that a frame produce task graphs like this one。 This
    is actually from a site-learn computation。 And a scheduler executes that graph
    in parallel。 Here's a trace of a single machine scheduler walking through that
    graph with four threads。
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 等待结果返回。我们希望速度非常快。让我们继续。很好。系统喜欢请求一个写入者，让一个框架生成像这样的任务图。这实际上来自一个网站学习计算。调度程序并行执行该图。这是一个单台机器调度程序在四个线程中遍历该图的跟踪。
- en: This is where we switch from the NumPy people to the more tornado concurrency
    networking people。 How do we run these graphs efficiently？ Before we talk about
    how to ask that。 I want to give a brief summary of other options， inside the Python
    space to motivate why we had to build our own thing。 We're talking about a few
    systems， simple systems like multi-processing or concurrent futures。
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们从 NumPy 的人转向更关注 Tornado 并发网络的人们的地方。我们如何高效地运行这些图？在讨论如何提出这个问题之前，我想简要总结一下 Python
    领域内的其他选项，以激励我们为什么必须构建自己的东西。我们正在谈论几个简单的系统，比如多处理和并发未来。
- en: Big data collections like Spark or Storm or Flink or Databases or TensorFlow
    and then task。 schedulers like Luigi or Airflow。 Let's consider a map。 Who has
    used multi-processing before？
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 大数据集合，如 Spark、Storm、Flink、数据库或 TensorFlow，然后是像 Luigi 或 Airflow 这样的任务调度程序。让我们考虑一下
    map。谁以前使用过多处理？
- en: Lots of people。 Almost everybody。 Who has not used multi-processing before？
    Sorry to pull you up。 Five brave people。 And maybe some less brave people。 So
    I take a function。 I have a bunch of data。 I want to apply that function and all
    that data。 Multi-processing is a very easy way to do this in parallel。 The pros
    and cons of multi-processing。 It's very easy to install。
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 很多人，几乎所有人，谁没有使用过多处理？抱歉让你举手。五个勇敢的人。也许还有一些不那么勇敢的人。所以我拿一个函数。我有一堆数据。我想把这个函数应用到所有数据上。多处理是一种非常简单的并行方式。多处理的优缺点。它非常容易安装。
- en: It's in the standard library and use。 The API is very simple。 It's also a very
    lightweight dependency。 Because in the standard library libraries don't mind depending
    on it。 Remember that's one of our goals。 We want to build a library that many
    libraries in the ecosystem。 all of these and thousands， more， are actually willing
    to depend upon。
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 它在标准库中使用。API 非常简单。它也是一个非常轻量级的依赖项。因为在标准库中，库并不介意依赖它。记住，这就是我们的目标之一。我们想构建一个许多生态系统中的库都愿意依赖的库。所有这些库以及更多的数千个，实际上都是愿意依赖它的。
- en: They're willing to put it in the requirements。txt file。
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 他们愿意把它放在 requirements.txt 文件中。
- en: '![](img/c756025e33a1d78115431bb7e1f1596e_23.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c756025e33a1d78115431bb7e1f1596e_23.png)'
- en: Some cons。 There's some costs moving data across a process。 That is unfortunate。
    And it's also not able to handle more complex computations。 So all of those graphs
    that we had map can't easily do those。 So let's go to something more complex。
    Let's look at big data collections。 Things like Spark or Database。
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 一些缺点。跨进程移动数据有一定成本。这很不幸。而且它也无法处理更复杂的计算。因此，我们所拥有的图中的 map 无法轻松处理这些。让我们看一些更复杂的东西。让我们看看大数据集合。像
    Spark 或数据库这样的东西。
- en: So these systems are a big step up。 They give you a fixed API。 Things like map
    and filter and group by and join。 And you use all of those operations， not just
    map。 And they will handle the parallelism for you。 This allows you to parallelize
    a much broader set of applications。 In particular many of these were implemented
    for ETL or data extraction and cleaning or。
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这些系统是一个巨大的进步。它们为你提供了一个固定的 API。像 map、filter、group by 和 join 这样的操作。你使用所有这些操作，而不仅仅是
    map。它们会为你处理并行性。这使你能够并行化更广泛的一组应用，特别是许多这些都是为 ETL 或数据提取和清洗等实现的。
- en: database comp computations or some lightweight machine learning。 So using the
    systems you can do all of those things。 So you're given a broader set of API to
    work with， like the SQL language。 It scales nicely。 And it's widely trusted by
    enterprise。 So I'm going to sort of take off my open source community hat and
    put on my like for profit evil hat。
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 数据库计算或一些轻量级机器学习。使用这些系统你可以做到所有这些。因此，你可以使用更广泛的API，比如SQL语言。它的扩展性很好，并且在企业中广泛可信。因此，我要摘下我的开源社区帽子，戴上盈利的邪恶帽子。
- en: I also like， you know， desk is open source and free and everything。 But it's
    nice to pay developers to do this。 And the more people we get using desk and we
    can like convince people to help us pay developers to work on it to make it better。
    So I actually care about this a little bit。 I'm a little bit evil and I apologize
    for that。 Okay。 [LAUGH]， So some cons about these systems。 They're somewhat heavyweight。
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我也喜欢，你知道，desk是开源和免费的。但付费给开发者来做这件事还是很不错的。我们能让更多人使用desk，并且说服他们帮助我们付费开发者改进它，这样我其实对此有一点关心。我有点邪恶，为此我道歉。好吧。[笑]，那么这些系统的一些缺点是，它们有点笨重。
- en: So it'll be sort of hard to convince libraries to depend on Spark。 There's sort
    of this big sort of transfer of， you sort of have to step into the Spark world
    for a bit using。 computations and the step out。 It is unpleasant for a few reasons。
    It's focused on the JVM。 Python is sort of always a second class citizen。 This
    is true with actually most of the parallel computing libraries out there。
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 所以说服库依赖于Spark会有点困难。存在这种大的转变，你必须在Spark世界中停留一段时间，进行计算后再走出来。这因几个原因而不愉快。它专注于JVM。Python总是处于第二公民的地位。这对于大多数并行计算库而言都是事实。
- en: You know， Storm， Flink。 There were often they were built out of the JVM stack
    and were sort of piggybacking on their success。 Also， these systems are not able
    to handle very complex computations。 Now， what do I mean by that？
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 你知道，Storm，Flink。它们通常是建立在JVM栈之上，借助其成功。此外，这些系统无法处理非常复杂的计算。现在，我所说的是什么？
- en: I mean the graphs that we saw before。 These sorts of very， these graphs without
    much symmetry。 Sort of arbitrary dynamic task graphs。 Where every。 every circle
    in here is one Python function to run on one piece of data。 So。 they're not able
    to handle these。 Systems like Spark databases tend to be good at mapping a function
    across many things。
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我是说我们之前看到的图。这些图没有太多对称性，有些是任意的动态任务图。这里的每个圆圈都是一个需要在一块数据上运行的Python函数。所以，它们无法处理这些情况。像Spark这样的数据库系统通常擅长在许多事物上映射一个函数。
- en: Sort of all， all， shuffle communications， reducing things， but always sort of
    in lock stuff。 It's not a sort of fine grander granular。 We need to do some of
    these more complex computations。 They're going to be need to handle in order to
    support all of these libraries。 However。 there are some libraries that do handle
    more messy computations。
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 所有的打乱通信、减少东西，但总是有点锁住。它不是一种细粒度的处理。我们需要进行一些更复杂的计算，它们需要处理，以支持所有这些库。然而，也有一些库可以处理更复杂的计算。
- en: These systems are actually commonly used in sort of the data engineering space。
    Libraries like Airflow or Luigi or Celery。 Who in the room has used one of those
    libraries？ Okay。 that's actually awesome。 That is not the case in like the SciPy
    or PyDotox or conferences。 So。 these libraries are able to handle much more complex，
    much more arbitrary task graphs。
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 这些系统实际上在数据工程领域中常用。像Airflow、Luigi或Celery这样的库。房间里有谁使用过这些库？好吧。那实际上很棒。在SciPy或PyDotox等会议上并不是这样。因此，这些库能够处理更复杂、更加任意的任务图。
- en: They sort of fit the model that we need。 They're also Python native often。 So，
    you know。 these are libraries that we can hack on， that our communities are willing
    to depend upon。 They're nicer in that sense。 However， there are some cons。 So。
    there's no inner worker storage or communication。 Latency is relatively high。
    So。
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 它们适合我们需要的模型。它们通常也是Python原生的。所以，你知道。这些是我们可以进行修改的库，我们的社区愿意依赖它们。从这个意义上来说，它们更好。然而，也有一些缺点。没有内部工作存储或通信。延迟相对较高。因此。
- en: like 100 milliseconds between tasks is an okay thing for these libraries。 It's
    not okay for us。 We work on sort of the millisecond to 100 microsecond level。
    We're not optimized for computation。 That being said， they're optimized for lots
    of things that Dask doesn't do。 I don't mean to sort of besmirch either of these
    libraries。
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这些库来说，任务之间的100毫秒是可以接受的，但对我们来说则不行。我们工作的时间尺度大约在毫秒到100微秒之间。我们并未针对计算进行优化。不过，它们在许多Dask不做的事情上进行了优化。我并不想贬低这两个库。
- en: Both Spark and Luigi and Airflow do lots of things that Dask won't do。 but Dask
    is sort of good at the sort of mixing between them。 So， we want a task scheduler。
    like Airflow and Luigi， but it's more computationally focused。 like something
    like Spark or Flinker TensorFlow。 And that ends up -- so。
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: Spark、Luigi和Airflow执行许多Dask无法做到的事情，但Dask在它们之间的混合方面表现良好。所以，我们想要一个像Airflow和Luigi那样的任务调度器，但它更专注于计算，像Spark或Flink
    TensorFlow那样。结果是这样的——
- en: our sort of attempt to build that system is called Dask， which -- so。 we've
    seen Dask data frame and Dask array， and those are things built with Dask。 That
    is not Dask。 Dask is a dynamic task scheduler at its core。 So， it gets a graph
    of tasks such as you would give to Luigi in a complex case。 and it executes them
    on parallel hardware， or might be your laptop， or might be a cluster。
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们试图构建的系统称为Dask，因此，我们已经看到了Dask数据框和Dask数组，这些都是基于Dask构建的东西。这并不是Dask。Dask本质上是一个动态任务调度器。因此，它获取一个任务图，就像你在复杂情况下给Luigi的那样，并在并行硬件上执行这些任务，可能是你的笔记本电脑，也可能是一个集群。
- en: We're relatively fast in various ways。 We're not as fast as MPI。 but we're sort
    of faster than most other things。 Also， lightweight， we'll see in a bit。 and it's
    well supported。 So， I'm talking a little bit about task schedulers。 There are
    two main task gutters within the Dask sort of world。
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在多种方面相对较快。我们没有MPI那么快，但比大多数其他东西快。而且，它轻量，稍后我们会看到，而且支持良好。所以，我将稍微谈谈任务调度器。Dask世界中有两个主要的任务调度器。
- en: There was one that was built many years ago for single machines。 It typically
    runs on top of a thread pool or a multi-processing pool。 And this runs with very
    low overhead。 It actually only depends， so it's about， you know。 50 microseconds
    per task overhead。 It's fast。 It can handle arbitrary graphs。
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 有一个多年前为单机构建的调度器。它通常在线程池或多处理池之上运行，且运行时开销非常低。它实际上只依赖于约50微秒的任务开销。它很快，能够处理任意图。
- en: It's relatively concise。 There's about a third of 1，000 lines of code。 It actually
    depends on nothing except for the standard library。 So。 even very conservative
    libraries are willing to depend on this code。 It has been stable。 it has been
    changed much， and people have not wanted to change this code for a long time。
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 它相对简洁。代码大约有1,000行的三分之一。它实际上只依赖于标准库。所以，即使是非常保守的库也愿意依赖这段代码。它一直很稳定，变化不大，长时间以来人们都不想更改这段代码。
- en: This is a stable code， it is lightweight， it's easy to use。 And it's used by
    many groups on many different applications。 The diversity of applications on these
    schedulers is very broad。 So。 it's likely that if you have a different application，
    it will also likely work。
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个稳定的代码，它轻量且易于使用。许多团队在不同的应用程序中使用它。这些调度器的应用多样性非常广泛。所以，如果你有一个不同的应用程序，它也很可能有效。
- en: They're not optimized for like array computing or for data frame computations。
    They're optimized for general computing。 Maybe like a year and a half ago。 we
    started building a distributed cluster scheduler， which is more sophisticated。
    but also a bit more heavyweight。 This is a tornado TCP application。 It's a little
    less concise。
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 它们并不是针对数组计算或数据框计算进行优化的，而是针对一般计算进行优化。大约一年半前，我们开始构建一个更复杂的分布式集群调度器，但也稍微重一些。这是一个tornado
    TCP应用，简洁性稍低。
- en: It's fully asynchronous。 So， you can submit graphs to the thing， even as it's
    running。 You can get sort of a constant conversation back and forth。 It supports
    sort of the HDFS space。 the Hadoop space， and it also supports the more sort of
    traditional cluster HBC space。 So。 when you're seeing things running on a cluster，
    we're running this distributed scheduler。
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 它是完全异步的。所以，你可以在它运行的同时提交图形。你可以得到一种持续的双向对话。它支持HDFS空间、Hadoop空间，同时也支持更传统的集群HBC空间。所以，当你看到集群上运行的东西时，我们正在运行这个分布式调度器。
- en: How it is organized， there is a single process running somewhere on your cluster。
    which is the scheduler。 So， it might call us like the master or the head node。
    This process is going to coordinate all of the other processes in your cluster。
    There are then many workers that will take instructions from that scheduler。 So。
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 它的组织方式是，在你的集群的某个地方有一个单独的进程在运行，那就是调度程序。所以，它可能被称为主节点或头节点。这个进程将协调你集群中的所有其他进程。然后会有很多工作节点从那个调度程序获取指令。所以。
- en: the scheduler might say， "Hey， worker one， I want you to compute this task。"。
    The worker will compute that task and hold on to the result and inform the scheduler
    that it's finished。 The workers will communicate peer-to-peer to share data around。
    so they're not communicating through one central bottleneck。 And then we， as a
    client。
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 调度程序可能会说：“嘿，工作节点一，我想让你计算这个任务。”工作节点将计算这个任务并保留结果，并通知调度程序它已经完成。工作节点将点对点通信以共享数据，因此它们不是通过一个中心瓶颈进行通信。然后我们作为客户端。
- en: down here from our， you know， maybe from our Jupyter Notebooks or from some
    script we're running。 you know， nightly or whatever， we're then going to submit
    graphs up to that scheduler。 So。 the workers and the schedulers are all TCP servers。
    They're communicating over various interesting protocols。
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 从我们的，你知道，可能是从我们的 Jupyter Notebooks 或我们正在运行的一些脚本中下来。你知道，夜间或其他什么，我们接下来会将图形提交给那个调度程序。所以，工作节点和调度程序都是
    TCP 服务器。它们通过各种有趣的协议进行通信。
- en: And the client is just a client up to the scheduler。 So， you can set this up
    relatively easily。 You can install on your laptop like I did before。 You can run
    it on a cluster。 It's relatively lightweight also。 So， at the bottom。 I'm actually
    just running it inside of one process。 And I can start this up。
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 客户端只是一个到调度程序的客户端。所以，你可以相对容易地设置这个。你可以像我之前那样在你的笔记本电脑上安装它。你可以在集群上运行它。它也相对轻量。所以，在底部。我实际上只是在一个进程内部运行它。我可以启动这个。
- en: including the fancy dashboard and everything。 There are 43 milliseconds。 So，
    you can import this。 You can run it。 You can tear it down。 And it is not a big
    thing。 It's cheap。 You should not think of distributed computing as being like
    a far away hard thing to do。 It is like。 this is faster than importing pandas。
    Importing pandas takes like 200 milliseconds。 So。
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 包括精美的仪表板和所有内容。总共 43 毫秒。所以，你可以导入这个。你可以运行它。你可以拆除它。而且这不是一件大事。它很便宜。你不应该把分布式计算视为一个遥不可及的难题。这就像，导入
    pandas 比这慢。这花费大约 200 毫秒。所以。
- en: you should think of this as being a cheap thing to do。 You can also， if you
    don't like Kanda。 you can pin this to all the ask。 Everything is pure Python。
    So。 I'm going to go through the time I'm not going to go through this。 But this
    shows an example of how the scheduler might work in a simple example。 So。
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该把这看作是一件便宜的事情。如果你不喜欢 Kanda，你也可以将这个固定到所有请求。所有东西都是纯 Python。所以。我将要快速浏览这段时间，我不会详细讲解。但这展示了调度程序在一个简单示例中可能如何工作。所以。
- en: DAS is easy to use and adopt。 You already know a lot of the APIs that DAS presents。
    There's no single DASK API。 We largely steal the APIs of other languages or other
    projects。 And you probably already have dependencies installed。 You probably have
    tornado installed。 You probably have message pack installed。 If you want， you
    know， DAS data frames， you need pandas。
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: DAS 易于使用和采用。你已经知道 DAS 提供的许多 API。没有单一的 DASK API。我们大多数情况下借用了其他语言或其他项目的 API。你可能已经安装了依赖项。你可能安装了
    tornado。你可能安装了 message pack。如果你想要，知道，DAS 数据框，你需要 pandas。
- en: but you probably have that installed。
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 但你可能已经安装了它。
- en: '![](img/c756025e33a1d78115431bb7e1f1596e_25.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c756025e33a1d78115431bb7e1f1596e_25.png)'
- en: if you want that anyway。 So， in order to improve adoption。 in order to sort
    of reduce the amount of like creativity， we had to have。 DASK largely uses existing
    Python APIs。 So， we support a lot of the NumPy and Pandas APIs and protocols where
    they exist。 We support PEP 31-48， which is concurrent futures。 If you're familiar
    with that library。
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你确实想要。所以，为了提高采用率。为了减少创造性的工作量，我们必须做到。DASK 在很大程度上使用现有的 Python API。因此，我们支持许多
    NumPy 和 Pandas API 及其存在的协议。我们支持 PEP 31-48，也就是并发未来。如果你熟悉那个库。
- en: DASK supports that perfectly。 We also do async await。 If you like concurrent
    async work。 it'll also use job。lib。 So， you actually take existing scikit-learn
    code and there's a way in joblib。 which is， scikit-learns parallelism library，
    to hijack how it does parallelism。 As you can run your scikit-learn code on a
    cluster。 It may not work very well in all cases。 But we。
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: DASK完美支持这一点。我们也做async await。如果你喜欢并发异步工作。它也会使用job.lib。因此，你实际上可以使用现有的scikit-learn代码，并且在joblib中有一种方法，joblib是scikit-learn的并行库，可以劫持它的并行方式。你可以在集群上运行你的scikit-learn代码。它可能在所有情况下都不是很好用。但我们。
- en: you know， where there is an existing protocol for parallel computing and Python。
    we have usually implemented that。 So， you probably already know how to use DASK。
    You probably also。 it's also very lightweight。 You guys sort of just， you know，
    using existing libraries。 Let's look at， yeah， a little bit of time。 So， let's
    look at some of these APIs。 So， again。
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 你知道，哪里有一个现有的并行计算和Python的协议。我们通常会实现它。因此，你可能已经知道如何使用DASK。你可能也知道，它非常轻量。你们可以说，只是使用现有的库。让我们看看，嗯，花一点时间。让我们看看这些API。再一次。
- en: we've seen， you know， if you know NumPy， you probably know how to use DASK array。
    Here's an example of scikit-learn。 This came from the， this is an example of the
    scikit-learn docs。 And， you know， someone， so we're making a pipeline。 We're going
    to do a cross-valided grid search across that pipeline。 Just normal scikit-learn
    code。
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到，你知道，如果你知道NumPy，你可能知道如何使用DASK数组。这里是scikit-learn的一个例子。这来自，这个是scikit-learn文档的一个例子。你知道，有人，所以我们正在创建一个管道。我们将在那个管道上进行交叉验证网格搜索。只是普通的scikit-learn代码。
- en: And it takes。 Or， eight seconds or nine seconds。 And now someone， actually his
    name is Jim Christ。 a word library， a DASK search CV， which is a drop-in replacement
    for grid search。 So。 now when you run that， it's going to run that on our own
    cluster。 And it looks just the same。 We're using all the scikit-learn APIs。 So，
    it looks， you know， it fits into existing work as well。
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 而且它花了。或者，八秒或九秒。现在有一个人，他的名字叫Jim Christ。一个词库，一个DASK搜索CV，它是网格搜索的直接替代品。因此。现在当你运行它时，它将在我们自己的集群上运行。看起来就是这样。我们使用所有的scikit-learn
    API。因此，它看起来，嗯，它也适合现有的工作。
- en: It ran faster。 We also noticed， like， there was a lot of white space here。 A
    lot of our workers weren't doing anything。 So， now that we have a bit more computing
    power。 let's go ahead and， you know， increase this a little bit。 We'll increase
    our parameter search space。 If you don't know scikit-learn， that's okay。 You should
    just understand that I'm increasing inputs and we'll get more work。
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 它运行得更快。我们也注意到，这里有很多空白。我们的许多工作者没有在做任何事情。因此，现在我们有了更多的计算能力。让我们继续，增加一下。我们将增加参数搜索空间。如果你不知道scikit-learn，没关系。你只需理解我在增加输入，我们会得到更多的工作。
- en: And it won't be slow。 So， now that we have more work， more computing power。
    we're going to sort of search this space a little bit better。 And again。 if you
    sort of know scikit-learn， this should have been pretty familiar to you。 So。 a
    lot of the DASK work is just helping other libraries parallelize themselves using
    those same interfaces。
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 而且它不会慢。因此，现在我们有了更多的工作，更多的计算能力。我们将更好地搜索这个空间。再一次。如果你知道scikit-learn，这对你应该相当熟悉。因此，DASK的许多工作只是帮助其他库使用那些相同的接口进行并行化。
- en: DAS supports concurrent futures interface。 So， here we can， you know， do the
    sort of executive。submit model。 It's actually fully asynchronous。 So， like， as
    work comes in。 you can submit more work in a fully real-time way， which is fun。
    We do things like async await。 So。 here's， you know， a fully asynchronous thing，
    you know。
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: DAS支持并发未来接口。因此，在这里我们可以，嗯，做执行的.submit模型。它实际上是完全异步的。因此，像工作进来时。你可以以完全实时的方式提交更多工作，这很有趣。我们做类似async
    await的事情。因此，嗯，这是，完全异步的事情，你知道。
- en: all those sort of Python code that you've seen。 None of， except for， like。 the
    import DASK statement， none of the code you've seen on this notebook， you would。
    you would qualify as DASK code。 It was all the kind of code you've seen before。
    But it was all parallelized in a nice way。
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 你看到的所有那种Python代码。除了像import DASK语句之外，你在这个笔记本上看到的代码，你都不会认为是DASK代码。它都是你之前见过的那种代码。但它都是以很好的方式并行化的。
- en: '![](img/c756025e33a1d78115431bb7e1f1596e_27.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c756025e33a1d78115431bb7e1f1596e_27.png)'
- en: And that's， again， sort of one of the objectives of DASK。 Okay， so again， at
    a high level。 DASK gives you parallel APIs。 Parallel pandas， parallel mumpy， parallel
    second-learn。 subset of all those， not complete。 It's also very good at parallelizing
    existing custom systems。 At a low level， DASK is a task scheduler， which means
    that it runs Python functions on Python OAuth。
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 这再次是DASK的目标之一。好吧，从高层次来看，DASK为你提供了并行API。并行pandas、并行mumpy、并行second-learn。所有这些只是一个子集，并不是完整的。它在并行化现有的自定义系统方面也非常出色。在低层次上，DASK是一个任务调度器，这意味着它在Python
    OAuth上运行Python函数。
- en: It functions on Python objects on parallel hardware。 Well。 those Python functions
    are what the Python objects are， it's up to you。 It can be your own special object，
    your own special functions。 DASK doesn't need to know what it is。 It can just
    run it， and it will figure out where and when to run those functions。
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 它在并行硬件上对Python对象进行操作。嗯，这些Python函数就是Python对象，它取决于你。它可以是你自己特殊的对象，或你自己特殊的函数。DASK并不需要知道这是什么。它可以直接运行，并且会弄清楚在哪里和何时运行这些函数。
- en: If the machine goes down， it will bring it back up。 All those sorts of nice
    parallelism things。 Okay， so I want to take a few minutes about sort of just general
    ecosystem comments。 So。 I'm normally a fairly critical or sort of like pessimistic
    person。 I apologize。 but I'm going to gush a little bit。 So， I think that the
    Python ecosystem is really the best place to build this kind of project。
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 如果机器崩溃，它会将其恢复。所有这些优雅的并行性特性。好吧，我想花几分钟时间谈谈一些一般的生态系统评论。因此，我通常是一个相当批评或悲观的人。我道歉，但我想稍微夸赞一下。所以，我认为Python生态系统真的是构建这类项目的最佳地方。
- en: We built DASK way more easily than you would expect us to be able to。 That's
    because of all the work that's already happened in the ecosystem。
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们构建DASK的过程远比你想象的要容易得多。这得益于生态系统中已经完成的所有工作。
- en: '![](img/c756025e33a1d78115431bb7e1f1596e_29.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c756025e33a1d78115431bb7e1f1596e_29.png)'
- en: So， let's look at sort of the Python strengths and weaknesses from sort of a
    parallel data analysis point of view。 So， Python is a very strong algorithmic
    tradition。 The community who was like writing code with punch cards or in Fortran
    has moved to Python。 And they know how to do those things very， very well。 They
    know algorithms very well。
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，让我们从并行数据分析的角度来看看Python的优缺点。Python有着非常强大的算法传统。那些用打孔卡或Fortran编写代码的社区已经转向Python。他们知道如何非常非常好地完成这些事情。他们非常了解算法。
- en: They know they have PhDs and math or something。 A lot of battle hard in Fortran
    code。 which one's very efficiently。 They use just the best SSE2 instruction on
    your CPU。 We also have alongside that a very strong networking currency stack。
    And it's very sort of very rare to have both of those in the same language。
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 他们知道自己有数学或其他领域的博士学位。很多人在Fortran代码上经过了激烈的磨练。这个非常高效。他们使用你CPU上最好的SSE2指令。我们还拥有一个非常强大的网络货币栈。而且在同一种语言中同时具备这两者是非常罕见的。
- en: We also have sort of very standard in teaching。 So， there's lots of people using
    Python。 Weaknesses。 I think， are not actually that true。 People think that Python
    is slow。 There's the Gil。 Those are questions about packaging。 So， a few comments
    about this。 So。 the NumPy and Py--the C sort of numeric stack on Python is very，
    very fast。
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在教学中也有非常标准的方式。所以，很多人在使用Python。缺点，我认为，并不是那么真实。人们认为Python很慢。还有Gil。这些都是关于打包的问题。因此，我对这件事做了一些评论。NumPy和Py—在Python上的C类数值栈是非常非常快的。
- en: It is running bare metal speeds。 So， here I have 1，000 by 1。000 array and I'm
    doing a matrix multiply。 There's about a billion multiplies and ads in that computation。
    And we ran that in around 70 milliseconds。 It's about 10 billion operations per
    second。 I think about my laptop。 My laptop only like has like a two or three gigahertz
    processor。 So。
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 它运行在裸金属速度上。所以，我这里有一个1,000乘1,000的数组，我正在进行矩阵乘法。这个计算中大约有十亿次乘法和加法。我们在大约70毫秒内完成了它。这相当于每秒约100亿次操作。我想到了我的笔记本电脑。我的笔记本电脑的处理器只有两到三吉赫兹。
- en: actually doing more ads than I have cycles in my system。 If you know about CPUs。
    that doesn't surprise you。 But we're running up bare metal speeds。 The Gil。 people
    often concerned about parallelism and the Gil。 Again， for the numeric stack。 this
    actually doesn't matter。 The Gil is not a problem if you're mostly running numeric
    code with libraries like NumPy。
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，我执行的加法操作比系统中的周期还要多。如果你了解CPU，这不会让你感到惊讶。但我们正在以裸金属速度运行。Gil。人们通常会担心并行性和Gil。再说一次，对于数值栈，这实际上并不重要。如果你主要运行与NumPy等库相关的数值代码，Gil就不是问题。
- en: and Pandas and scikit-learn。 So， the Gil stops two Python threads from operating
    at the same time。 But--or two Python threads are actually using Python code at
    the same time。 But our threads are just calling out some C function and then they
    sort of wait until a function returns。 So， we're not actually calling Python code。
    We're calling C or Fortran code。 So。
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 和 Pandas 以及 scikit-learn。所以，Gil 阻止两个 Python 线程同时操作。但是——或者说两个 Python 线程实际上是同时使用
    Python 代码。但我们的线程只是调用一些 C 函数，然后在函数返回之前等待。所以，我们实际上并没有调用 Python 代码。我们在调用 C 或 Fortran
    代码。
- en: you can run Python with many threads and saturate your hardware very easily
    if you're。 using NumPy or Pandas or that sort of stack。 You should use threads
    freely unless you're doing。 you know， Python extreme manipulation。 But use threads，
    you know， just general comment。 Alongside that， there's also this totally different
    community。
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用 NumPy 或 Pandas 等栈，你可以轻松地用多个线程运行 Python，并充分利用你的硬件。你应该自由地使用线程，除非你在做，知道的，Python
    的极端操作。但是使用线程，你知道的，只是一般评论。与此同时，还有一个完全不同的社区。
- en: It's at the same time simultaneously building out a very strong and very intuitive
    concurrency networking stack。 So， here's some code that， you know， such as you
    might see inside of a set of desk that。 does a lot of concurrent things。 You might
    now write that with a single wait。 I'm going to ask you。 use tornado。 And this
    code looks simple。 It looks easy to understand。
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 同时构建出一个非常强大且直观的并发网络栈。所以，这里有一些代码，你知道的，类似于你可能在一组桌面中看到的。做了很多并发的事情。你现在可能会用一个单一的等待来写这个。我会问你。使用
    tornado。这段代码看起来很简单。理解起来很容易。
- en: And that is actually remarkable。 This would not have been the case 10 years
    ago。 At the same time。 it's also quite fast。
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 这实际上是非常显著的。十年前情况可不是这样的。同时，它的速度也相当快。
- en: '![](img/c756025e33a1d78115431bb7e1f1596e_31.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c756025e33a1d78115431bb7e1f1596e_31.png)'
- en: So， this is from a blog post about UV loop。 Let's see if I can get that link
    in there。 Talking about， you know， various UV loops， various event loops。 And
    that's actually running on tornado， which is way down here。 But even that is running
    at 20。000 TCP requests per second， which is like totally fine for us。 So。
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 这是关于 UV loop 的一篇博客文章。让我看看我能否把那个链接放进去。谈论你知道的，各种 UV loops，各种事件循环。这实际上是在 tornado
    上运行的，这在这里很下方。但是即使如此，它每秒也处理 20,000 个 TCP 请求，这对我们来说完全可以。所以。
- en: this community is also very focused on performance。 Not in a numeric computing
    way。 but in a concurrency sort of way。
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 这个社区也非常专注于性能。不是以数字计算的方式，而是以并发的方式。
- en: '![](img/c756025e33a1d78115431bb7e1f1596e_33.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c756025e33a1d78115431bb7e1f1596e_33.png)'
- en: So， Python is both an excellent numeric computing stack and an excellent concurrency
    networking stack。 And that's actually quite rare。 It's really a blessing that
    we had sort of those two groups of people in the beginning。 And they're both in
    the same room together。 And it's because of that。 that desk was actually really
    easy to build。 So， most of the work of building desk was built。
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，Python 既是一个优秀的数字计算栈，也是一个优秀的并发网络栈。这实际上是相当罕见的。我们能有这两组人一起，真的是一种福气。正因为如此，desk
    的构建变得非常容易。因此，构建 desk 的大部分工作都是已经完成的。
- en: you know， five or ten years ago by all of you。 So， I appreciate your efforts。
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 你们在五或十年前所做的。所以，我非常感谢你们的努力。
- en: '![](img/c756025e33a1d78115431bb7e1f1596e_35.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c756025e33a1d78115431bb7e1f1596e_35.png)'
- en: In the interest of time， I'm going to stop。 But many people work on desk。 It's
    not just me。 There are various government organizations and nonprofits that fund
    a desk。 We like to thank them as well。 Yeah， just show engagement in the last
    48 hours。 These projects have mentioned or committed code using desk as well on
    GitHub in search for a desk。
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 为了节省时间，我要停止了。但是很多人都在为 desk 工作。不仅仅是我。还有各种政府组织和非营利机构资助 desk。我们也想感谢他们。是的，过去 48
    小时的参与情况。这些项目也在 GitHub 上提到或承诺使用 desk 的代码。
- en: So， it's sort of nice to see that。 Okay。 Thank you。 [applause]。 We've got about
    five minutes for questions。 So， if you could line up by the microphones。 Sure。
    Is there a C API？ There's not a C API。 Python， or a desk is a Python project。
    Okay。 So。 let's say I have some code that takes， it's a wrappedless swig and it
    takes in a NumPy。
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，看到这一点还是挺不错的。好的。谢谢。[鼓掌]。我们还有大约五分钟的时间来提问。所以，如果你可以排队到麦克风旁。好的。有一个 C API 吗？没有
    C API。Python，或者说 desk 是一个 Python 项目。好的。所以，假设我有一些代码，它是一个无包装的 swig，并且它接收一个 NumPy。
- en: or a NumPy array。 Could it also potentially operate on a desk array and still
    take advantage of the functionality。 or is it a complete no-go？ So， a desk array
    does not implement sort of the NumPy low-level ABI interface。 Okay。 However， if
    you have a function because a desk is composed of any NumPy arrays。 you might
    be able to just apply your function on all of them， depending on what your function
    does。
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 或者一个 NumPy 数组。它是否也可能在桌面数组上操作，并仍然利用该功能？还是完全不可行？所以，桌面数组并没有实现 NumPy 低级 ABI 接口。好的。但是，如果你有一个函数，因为一个桌面是由任何
    NumPy 数组组成的，你可能能够将你的函数应用于它们所有，具体取决于你的函数的作用。
- en: Okay。 Thank you。 Thank you。 In your demos， you were acting as a single client
    to your cluster。 How does it behave if you have， say， multiple clients competing
    for the cluster's resources。 at any given time？ Yeah。 So， one desk cluster can
    have multiple clients working on the same data。 desk graphs or a， Merkle tag。
    If you know what that means， you'll be great。 So， it can share nicely。
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 好的。谢谢你。在你的演示中，你作为单个客户端操作你的集群。如果有多个客户端竞争集群的资源，表现如何？是的。所以，一个桌面集群可以有多个客户端在同一数据上工作。桌面图或一个
    Merkle 标签。如果你知道这意味着什么，你会很棒。所以，它可以很好地共享。
- en: That being said， most people who do multi-client workloads end up allocating
    separate schedulers。 and workers。 Great。 Thank you。 Does desk do any optimization
    of the task graph？
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 话虽如此，大多数进行多客户端工作负载的人最终会分配单独的调度器和工作节点。很好。谢谢。桌面对任务图是否做了任何优化？
- en: Is that a thing that is a priority for the project right now？ Yeah。 So。 we do
    the kind of optimizations that are all linear in time。 So。 the kind of things
    you would see in a normal compiler， but none of the complex ones。 So。 in particular，
    we don't do any sort of like data frame optimizations， like ordering， reordering。
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 这现在是项目的优先事项吗？是的。所以，我们进行的优化都是线性的。因此，你会在普通编译器中看到的那种东西，但没有复杂的那种。因此，特别是，我们不做任何数据帧优化，比如排序或重排序。
- en: because we don't， at the task graph level， we sort of already lost that information。
    But loop fusion， you know， those sorts of operations we do nicely， calling the
    graph。 Yeah。 Have you looked into or do you do any sort of use of OpenCL or CUDA
    for even higher parallelization。 on the GPU？ Uh， no， but you can in your Python
    function。 And then。
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 因为在任务图层面，我们已经失去了那种信息。但循环融合，你知道，那些操作我们做得很好，调用图。是的。你有没有考虑过或使用 OpenCL 或 CUDA 来实现更高的并行化？在
    GPU 上？呃，没有，但你可以在你的 Python 函数中。然后。
- en: desk can run that Python function on various machines with GPUs。 So。 desk runs
    Python functions on Python objects。 What those Python functions do is entirely
    up to the user。 You can annotate that certain workers in your graph have GPUs
    and desk will respect that。 and allocate accordingly。 But we don't actually like
    use CUDA to execute our parallelism。 Okay。
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 桌面可以在配备 GPU 的各种机器上运行 Python 函数。所以，桌面在 Python 对象上运行 Python 函数。这些 Python 函数的作用完全取决于用户。你可以注释说你图中的某些工作节点有
    GPU，桌面会尊重这一点并相应分配。但我们实际上不使用 CUDA 来执行我们的并行性。好的。
- en: Thank you。 Can the schedule take advantage of any data locality？
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 谢谢。调度能利用任何数据局部性吗？
- en: Say this node has this set of files。 So。 Yes。 [laughter]， In many ways， yes。
    Is there any limitation to the work that you can do inside of the function that
    you're passing。 and if it's a custom function？ Sorry， second please。 Is there
    any limitations into what you can do inside of the function that gets passed into。
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 假设这个节点有这一组文件。所以，是的。[笑]在许多方面，是的。在你传递的函数内部，有什么限制吗？如果这是一个自定义函数？抱歉，第二次请。你在传递的函数内部能做的事情有什么限制吗？
- en: the distributed workload system？ Um， your function must be serializable with
    Cloud Pickle。 So you can have like locks or open files you're passing into it。
    Um。 your function should not mutate state that's given to it。 Uh， so you generally，
    to be resilient。 we have to sort of assume that。 Um， you shouldn't call us to
    segift， but other than that。
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 分布式工作负载系统？嗯，你的函数必须可以用 Cloud Pickle 序列化。因此，你不能传递锁或打开的文件。嗯。你的函数不应该改变给定的状态。呃，所以一般来说，为了有弹性，我们必须假设这一点。嗯，你不应该调用我们来
    segift，但除此之外。
- en: Even that would be fine。 Like that's what we're covering from that just fine。
    Okay。 Thanks。 Okay。 One more and then we've got to add。 How big can it scale up？
    Like。 what's the largest cluster you've run it on？ What are some super massive
    workloads you've tried it against？
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 即使那样也不错。就像我们从中覆盖的那样，完全可以。好吧。谢谢。好，再来一个，然后我们就得加了。它能扩展到多大？像。你运行过的最大集群是什么？你尝试过哪些超级庞大的工作负载？
- en: The largest cluster I've had my hands on is a thousand， oddly windows machines。
    Um， um， uh。 the number I have in your head is that every task is about a 200 microsecond。
    overhead inside the scheduler。 So if your tasks take about one second each。 you
    can saturate around 5，000 cores。 Sort of the back envelope， uh， number to have
    in your head。
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 我接触过的最大集群是千台奇怪的Windows机器。嗯，嗯，呃。你脑海中需要记住的数字是每个任务大约需要200微秒的开销在调度器内部。所以如果你的任务每个大约需要一秒，你可以饱和大约5,000个核心。这大致是你需要记住的数字。
- en: But a thousand is pragmatically what I've seen。 I've actually seen someone try
    bigger and fail。 but I assume if you had to 100，000， you'd run， into something。
    Thank you all for your time。 [applause]。
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 但千台机器实际上是我所见的。实际上我见过有人尝试更大的集群，但失败了。不过我假设如果你有100,000台，你会遇到一些问题。谢谢大家的时间。[掌声]。
- en: '![](img/c756025e33a1d78115431bb7e1f1596e_37.png)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c756025e33a1d78115431bb7e1f1596e_37.png)'
- en: '[silence]。'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '[沉默]。'
- en: '![](img/c756025e33a1d78115431bb7e1f1596e_39.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c756025e33a1d78115431bb7e1f1596e_39.png)'
