- en: P6：Larry Hastings   The Gilectomy Hows It Going   PyCon 2017 - 哒哒哒儿尔 - BV1Ms411H7jG
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: P6：Larry Hastings Gilectomy 进展如何 PyCon 2017 - 哒哒哒儿尔 - BV1Ms411H7jG
- en: past so。 It's time for our last talk before lunch everyone so we really should
    make a start now。
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 过去的事情。所以在午餐前是我们最后一次演讲，大家，我们真的应该现在开始了。
- en: '![](img/8819b309a1dc896d5dfc43ab416628b0_1.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8819b309a1dc896d5dfc43ab416628b0_1.png)'
- en: Our next speaker is here to talk about the Gilek to me and how it's going。 Please
    make welcome。
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的下一个演讲者在这里谈论 Gilek 的进展。请热烈欢迎。
- en: '![](img/8819b309a1dc896d5dfc43ab416628b0_3.png)'
  id: totrans-4
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8819b309a1dc896d5dfc43ab416628b0_3.png)'
- en: Larry Hastings。
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: Larry Hastings。
- en: '![](img/8819b309a1dc896d5dfc43ab416628b0_5.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8819b309a1dc896d5dfc43ab416628b0_5.png)'
- en: Thank you。 My name is Larry Hastings。 Ladies and gentlemen boys and girls children
    of all ages。 This is the Golek to me。 How's it going？ PyCon 2017 edition。 I call
    the talk that because when。 I go to conferences now people walk up to me and say，
    "Oh Larry， oh you're the guy working on the。 Golek to me。 How's it going？" And
    I'm like， "Ugh， I'm allowed to speak for 40 minutes about how the。
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 谢谢。我叫 Larry Hastings。女士们，先生们，男孩们，女孩们，各个年龄段的孩子们。这是 Golek 对我来说，进展如何？PyCon 2017
    版。我称这次演讲为 Golek，因为现在当我去会议时，人们走过来问我：“哦，Larry，你是那个在做 Golek 的人吗？进展如何？”我回答：“唉，我只能讲
    40 分钟关于这个的内容。”
- en: Golek to me is going and it's， I don't want to answer for 40 minutes。 Every
    person walks off to me。 at a conference。 So I'm going to do it for everybody at
    once and now you don't have to ask that question。
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 对我来说 Golek 是一个过程，我不想回答 40 分钟。每个人在会议上都会走过来问我。所以我打算一次性为大家解答，这样你就不必再问这个问题了。
- en: '![](img/8819b309a1dc896d5dfc43ab416628b0_7.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8819b309a1dc896d5dfc43ab416628b0_7.png)'
- en: You can ask a more specific question。 So about this talk I want to warn you
    just like last year。 I gave a talk about the Golek to me last year in introductory
    talk。 This is kind of the same。 preface slide。 This talk will be exceedingly technical
    and it's going to assume that everybody in here。 is comfortable about me talking
    about multi-threading issues and about CPython internals。
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以问一个更具体的问题。关于这次演讲，我想像去年一样提醒你。我去年做过关于 Golek 的介绍演讲。这次是同样的前言幻灯片。这次演讲将是极其技术性的，并且假设在座的每个人都对我谈论多线程问题和
    CPython 内部结构感到舒适。
- en: This year I'm， kind of assuming that you're mildly familiar with like maybe
    my talk from last year。 So there's a lot， of stuff that I'm not going to explain。
    I'm just going to dive right into some of these concepts。 If you want to know
    more。 you can watch my talk from two years ago， Python's infamous GIL。
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 今年，我假设你们对我去年的演讲有一定的了解。所以有很多内容我不会解释。我会直接进入一些概念。如果你想了解更多，可以观看我两年前的演讲，关于 Python
    致命的 GIL。
- en: where I explained the GIL itself and then last year's talk where I kind of got
    started。 the Golek to me removing， oops， typo， removing Python's GIL。 So the goal
    of the Golek to me。 the Golek to me， I want to run existing multi-threaded Python
    programs。 So a Python program you could， write today using the threading module。
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我解释了 GIL 本身，去年的演讲则是我开始讲 Golek 的过程，哎呀，拼写错误，去除 Python 的 GIL。所以 Golek 的目标是让现有的多线程
    Python 程序运行。因此，你今天可以使用 threading 模块编写的 Python 程序。
- en: I want to run it on multiple cores simultaneously。 I want to， run it with as
    little。 I want to implement this with as little C API breakage as possible。 It's。
    impossible to not break the C API。 There are some guarantees the GIL gives you
    that I cannot。 guarantee anymore。 But with as little as breakage as possible。
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我想在多个核心上同时运行它。我想尽量以最少的方式实现这一点。我想尽可能少地破坏 C API。破坏 C API 是不可能的。有些 GIL 给你的保证，我无法再保证。但是我要尽可能少地破坏它。
- en: And I want to run it faster than CPython， does with a GIL by wall time。 So this
    is if you had a stopwatch and you started your program running。 when it finishes
    and you hit it again， if the program ran faster then I will declare the Golek。
    to me a success， which it has never done。 The approach that I'm taking， and again
    this was most。
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我想让它比 CPython 更快，CPython 通过墙时实现了 GIL。所以，如果你有一个秒表，启动你的程序运行。当它完成后，你再按一次，如果程序运行得更快，那么我就会宣称
    Golek 对我来说是成功的，这从未发生过。我正在采取的方法，当然这大部分。
- en: of the talk last year， Python uses reference counting for tracking the lifetimes
    of objects。 And I switched that to atomic， that's something that your Intel CPU
    does for you， where it will。 increment or decrement a number stored in memory
    in an atomic way such that there are no race。 conditions。 You're guaranteed that
    after you're done the number has been incremented by one。 And。
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 去年谈到，Python使用引用计数来跟踪对象的生命周期。我把这个切换为原子操作，这是你的Intel CPU为你做的，它会以原子的方式增加或减少存储在内存中的数字，从而避免竞争条件。你可以保证在操作完成后，数字会增加一。
- en: it was done in a safe manner。 I added locks to all the internal data structures
    inside of Python。 objects that are mutable， like Dix and lists， so that those
    operations are safe。 If you append to。 a list or you add a set an item on a dict
    that has to be an atomic operation， it has to be safe。 And so those items， those
    objects lock themselves internally。 I also added a bunch of locks around。
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 这是以安全的方式完成的。我在Python内部所有可变对象（如字典和列表）的数据结构上添加了锁，以确保这些操作是安全的。如果你向列表追加元素，或者在字典中添加一个项，那必须是原子操作，必须安全。因此，这些对象内部会自我锁定。我还在C
    Python内部使用的许多数据结构周围添加了锁，例如自由状态。
- en: a bunch of internal data structures that are used inside of C Python， like free
    state。 But。 there's two in particular I want to call out。 Ob malloc， which is
    what we call the small block。 allocator。 This is used for memory allocation for
    small objects， which is like under 256 bytes or。 something。 I don't remember what
    the cutoff point is。 But this is used for most of the objects that。
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 有两项特别想提及。Ob malloc，我们称之为小块分配器。这用于小对象的内存分配，大约在256字节以内。我不记得截止点是什么。但这用于大多数对象。
- en: are allocated inside of C Python。 And this is very， very finely tuned and it's
    super fast。 And。 C Python relies on it being fast。 And adding， locking to that
    in order to make it safe has。 slowed it down a great deal。 Also， there are a bunch
    of free lists inside of C Python。 It's very。 handy if you're using a bunch of，
    say， integers or frame objects。 If you allocate a fresh one every。
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 垃圾收集是在C Python内部分配的。这非常、非常精细调优，速度超级快。C Python依赖于它的快速性。而为了安全性而添加锁会使其变得非常缓慢。此外，C
    Python内部还有许多免费列表。如果你每次都分配一个新的对象，比如整数或帧对象，这非常方便。
- en: time， that's a little slower than just say， oh， the last one that I use， I'm
    going to put it on the。 list and then reuse it next time I need to free a frame
    object or an integer or something。 So there。 are a bunch of free lists internally
    of Python objects that get reused constantly。 And I， of course。 had to put locks
    around those data structures or make them per thread。 I also disabled the garbage。
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这比仅仅说“我最后使用的那个，我将把它放在列表中，下次需要释放一个帧对象或整数时再重用它”稍慢。因此，Python对象内部有许多免费列表不断被重用。当然，我必须在这些数据结构周围添加锁，或者使它们按线程分配。我还禁用了垃圾收集器。
- en: collector entirely。 I'm just not ready to deal with the garbage collection under
    the， galectomy。 There's a lot of research。 There are absolutely thread safe， multi-threaded
    friendly。 garbage collecting algorithms。 That's going to be a lot of work。 And
    that's not the thing that。 interests me。 What interests me is getting Python to
    be fast enough that this is useful for somebody。
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 垃圾收集器完全不处理。我还没有准备好应对垃圾收集的问题。有很多研究。有绝对线程安全、多线程友好的垃圾收集算法。这将需要大量工作。这不是我感兴趣的事情。我感兴趣的是让Python快到足够有用的程度。
- en: ever。 So my general approach is I got it working and now I figure out what the
    slowest thing is。 And I run it on a profiler or I come up with pod experiments
    and I experiment and I try something。 and I see if I can make it go faster。 If
    I make it go faster， then I keep it。 If it doesn't go faster， then I don't throw
    it away。
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我的总体方法是先让它工作，然后找出最慢的地方。我会用分析器运行它，或者进行实验，尝试不同的东西，看看能否加快速度。如果我能加快速度，就保留它。如果不能，我就不保留。
- en: I may put it on the back burner and try it again later because sometimes。 you
    turn a corner and now this technology is going to work again。 Just to give you
    an idea of how。 crappy this project is， my official benchmark that this is literally
    all the code I ever run under。 the galectomy is a really bad Fibonacci number
    generator。 It's a recursive Fibonacci。 So this is。
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我可能会把它放在一边，稍后再试，因为有时你会转个弯，这项技术可能会再次工作。只是给你一个概念，说明这个项目有多糟糕，我的官方基准测试实际上是我在 galectomy
    下运行的所有代码，这真的是一个非常糟糕的斐波那契数生成器。它是一个递归斐波那契。因此，这就是。
- en: exercising an if statement。 It's exercising some math。 It's exercising recursive
    function calls and。 of course bytecode and a bunch of internal stuff inside of
    the Python engine。 But this is all I。 ever run。 So I'm running on this on multiple
    cores simultaneously inside of C Python。 So an overview of what's happened since
    last year when I gave the galectomy talk。
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 通过执行一个 if 语句。它正在执行一些数学运算。它正在执行递归函数调用，当然还有字节码和 Python 引擎内部的一些东西。但这就是我所运行的所有内容。所以我在
    C Python 中的多个核心上同时运行这个。自从去年我做了 galectomy 演讲以来，情况概述如下。
- en: we start with what， I'm going to call， I'm going to call last June's version
    of the galectomy。 the atomic version for， using atomic hearing redecker。 I switched
    to something called buffered reference counts， which was done by about October。
    And then I did a bunch of work on OpMALIC and that was done in， about April。
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从我称之为 galectomy 去年六月版本开始。我称之为原子版本，使用原子加锁的重构。我切换到了一种叫做缓冲引用计数的东西，这大约是在十月完成的。然后我在
    OpMALIC 上做了大量工作，这大约是在四月完成的。
- en: And then just this month I did some work that I'm calling no TLS。 TLS， when
    I say TLS。 what I mean is thread local storage。 This is data that you can store
    per thread so that each thread。 has its own version of something。 You need that
    in order to， you can store something there and no。 other thread is going to examine
    it and so you don't have to lock it in order to talk to it。
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 然后就在这个月，我做了一些我称之为无 TLS 的工作。TLS，当我说 TLS 时，我指的是线程局部存储。这是你可以按线程存储的数据，因此每个线程都有自己版本的数据。你需要这个来存储某些东西，没有其他线程会检查它，因此你不必锁定它以便与之交互。
- en: So I'm going to go through each of these items and basically what it is and
    I'll show you how。 much it got faster。 But I want to talk for just a moment about
    how benchmarking is impossible on。 modern computers。 As Victor talked about in
    his talk this morning， modern CPUs， like an Intel CPU。 like a Xeon， has a speed
    step technology where it's constantly adjusting the run， the speed of the。
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我将逐一介绍这些项目，基本上是什么，我会向你展示它变得多么快。但我想稍微谈一下为什么在现代计算机上进行基准测试几乎是不可能的。正如维克托今天早上在他的演讲中提到的，现代
    CPU，比如英特尔 CPU，比如 Xeon，具有动态调整速度的技术，它不断地调整运行速度。
- en: cores inside of your CPU。 And so this， I have a computer that literally has
    56 cores。 It's an。 enormous server。 And this is some of the cores at a particular
    moment in time how fast they're running。 So you can see I was running a benchmark
    at the time。 So the ones in the lower right of the fastest。 ones， they're running
    at 3，100 megahertz。 The slowest ones are running at 1，200 megahertz。
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 你的 CPU 内部有多个核心。因此，我有一台实际上有 56 个核心的计算机。这是一个巨大的服务器。这是某个特定时刻这些核心的运行速度。所以你可以看到我当时正在运行基准测试。因此，右下角的那些最快的核心，它们的运行速度是
    3100 兆赫。最慢的那些运行速度是 1200 兆赫。
- en: You really don't know from moment to moment how fast your CPU core is running。
    And so it makes。 it almost impossible to do meaningful benchmarks in any meaningful
    way。 So this is just to give you。 an idea。 Like already my benchmarking is crappy。
    They're even crappier because of this。 I haven't。 figured this one out yet。 Victor
    says， oh yeah， there's a way you can turn it on for Linux。 We'll。
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 你真的不知道你的 CPU 核心每时每刻运行得有多快。因此，这几乎使得在任何有意义的方式中进行有意义的基准测试几乎不可能。所以这只是给你一个概念。就像我目前的基准测试就很糟糕。由于这个原因，它们甚至更糟。我还没有弄清楚这个问题。维克托说，哦，对，确实有一种方法可以在
    Linux 上启用它。我们会。
- en: figure it out。 But that's just a proviso to give you the sense of let's look
    at the benchmarks from。 a sort of a 10，000 foot view。 Let's not get concerned
    about like 5% of this way or another。 So this is a graph of the speed of the galectomy
    versus stock C Python。 And again， this is。 I'm talking about different errors。
    So I'm going to have update the graph every time I do。
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 弄明白这一点。但这只是一个前提，让你感受到从 10,000 英尺的高度来看基准测试的样子。我们不必担心这个方法的 5% 误差。因此，这是 galectomy
    与标准 C Python 的速度图。再次强调，我在谈论不同的误差。所以我每次做完都会更新这个图。
- en: a new thing。 So the red line at the bottom is stock C Python， which was the
    base revision that。 I started the galectomy against， which was last February。
    It was trunk。 So it's what became 3。6。 But， there was a lot of work put in the
    3。6 after I branched。 And then the blue line is the galectomy。 The bottom。
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 一件新事物。所以底部的红线是股票C Python，这是我在galectomy上开始的基础修订，时间是去年二月。它是主干。所以它就是后来成为3.6的版本。但在我分支后，3.6进行了大量工作。然后蓝线是galectomy。底部。
- en: the x-axis is the number of cores I'm using because it's the number of threads
    that， I'm running。 So C Python， of course， is only， is running multiple threads，
    but it's only ever。 using one core at the time。 I really should level that threads，
    not cores。 And then the y-axis， of。 course， is the number of seconds that it took。
    So just to drive home， how bad this is。
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: x轴是我使用的核心数量，因为它是我运行的线程数量。所以C Python当然只运行多个线程，但它只使用一个核心。我真的应该标记线程，而不是核心。然后y轴当然是所花费的秒数。所以只是为了强调，这有多糟糕。
- en: on seven cores， C Python runs in 4。4 seconds。 And the galectomy is running in
    83。0 seconds。 It is 18。9 times slower。 As an aside， this is why I got to say I
    haven't been terribly gracious。 Like a lot of people like， to talk to me and they
    say， well。 have you heard about this super fast locking mechanism？ That's not。
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在七个核心上，C Python运行4.4秒，而galectomy运行83.0秒。它慢了18.9倍。顺便说一下，这就是我不得不说我没有表现得特别优雅的原因。很多人想跟我谈话，他们会说，嘿，你听说过这种超级快的锁机制吗？那可不是。
- en: where the war is going to be won。 The difference between 19 times slower is
    not， oh， you should use。 a slightly faster locking mechanism。 There are major
    battles to be won here。 Once I get it down。 to the point where it's kind of within
    shooting range of C Python， then I might be interested in。 more efficient locking
    mechanism。 Right now， there are more fundamental structural problems inside。
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 战争将在何处取得胜利。19倍慢的差别不是，你应该使用稍微更快的锁机制。这里有重大战役要打。一旦我把它降到接近C Python的水平，我可能会对更高效的锁机制感兴趣。现在，内部还有更基本的结构问题。
- en: the galectomy， enormous nails that need to be hammered down。 And that's where
    my head is at。 So。 unless you're spending time doing analysis on the galectomy
    and saying， I found a super slow thing。 did you know about this， Larry， here's
    a way that you can make it faster。 That would be super。 interesting。 Oh， this
    is the way that Grand Central does logging。 Not so interesting。
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: galectomy，巨大的钉子需要被钉下。这就是我所考虑的地方。所以，除非你花时间对galectomy进行分析，说我发现了一个超级慢的东西。你知道吗，Larry，这里有一种让它更快的方法。那会非常有趣。哦，这就是大中央如何记录日志的方式。不太有趣。
- en: So let's start with where we were in last June。 Like I mentioned， C Python uses。
    incorrect and decker for reference counting。 And I was using atomic anchor and
    decker just to get。 the galectomy working。 But this was 30% slower off the top，
    just using two threads。 And they get。 slower and slower。 Each thread that you
    add is more than is more overhead than the last。 I think。
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 所以让我们从去年的六月开始说起。正如我提到的，C Python使用不正确的decker进行引用计数。我使用原子锚和decker只是为了让galectomy工作。但这在开始时就慢了30%，仅使用两个线程。每增加一个线程，开销就比上一个更多，我认为。
- en: what's going on here， and I don't know for certain， but what I believe is going
    on here is that the。 cores need to talk to each other。 There's an internal bus
    that they use to tell each other， "Hey。 I'm doing an atomic increment on this
    thing。 You need to not look at it right now。"。 There's some internal bus that
    they're using to communicate with each other。 And the more。
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 这里发生了什么，我不能确定，但我相信的是，核心需要彼此通信。他们使用内部总线来告诉对方：“嘿，我正在对这个东西进行原子递增。你现在不需要看它。”他们之间有某种内部总线在通信。而且越多。
- en: auto -- excuse me， the more atomic anchor and decker that I'm doing， the more。
    traffic there is on that bus until we're starting to saturate the bus and people
    are having to wait。 So getting that work not to be atomic would be a major win。
    It turns out -- so first of all。
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 不好意思，我越是使用原子锚和decker，巴士上的流量就越多，直到我们开始饱和巴士，人们不得不等待。所以让那项工作不再是原子的将是一次重大胜利。事实证明——首先。
- en: '![](img/8819b309a1dc896d5dfc43ab416628b0_9.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8819b309a1dc896d5dfc43ab416628b0_9.png)'
- en: I spent a lot of time thinking about this problem because I knew this was my
    hardest problem to solve。 I actually came up with my own technique to solve it，
    and then I felt terrible because I was like。 "Nobody invents anything new in computer
    science。 Everything's been done。 You don't want to invent。 something new。 If you
    do that， you've probably done something wrong because some -- all the good。
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我花了很多时间思考这个问题，因为我知道这是我最难解决的问题。我实际上想出了自己的技术来解决它，然后我感到很糟糕，因为我想：“在计算机科学中，没有人能发明任何新东西。所有的事情都已经被做过了。你不想发明新的东西。如果你这样做，你可能做错了，因为所有好的东西——
- en: ideas have already been taken， particularly in multi-core， multi-threading。
    It's all the work。 was done only in years ago。 So it turns out there's this book
    called "The Garbage Collection Handbook。"， the second edition， just updated in
    the last couple of years。 And they have -- it's mostly about garbage， collection。
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 一些想法已经被采纳，尤其是在多核和多线程方面。所有的工作都是几年前完成的。所以事实证明，有一本书叫《垃圾收集手册》，第二版，最近更新了。它主要是关于垃圾收集的内容。
- en: What technically is called tracing garbage collection， but reference counting
    also。 counts as garbage collection。 So they have a chapter in the beginning about
    reference counting。 and then it turns out they have a chapter at the end about
    reference counting in multi-threaded。 programs。 And there are two things in this
    rather slim chapter。 One of them was exactly the thing。
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 从技术上讲，这被称为跟踪垃圾收集，但引用计数也算作垃圾收集。所以他们在开始时有一章关于引用计数的内容，然后最后发现他们有一章关于多线程程序中的引用计数。而这一章相当薄，其中有两件事。其中一件正是我所想到的。
- en: that I invented on my own。 The other one is something that's very complicated
    that I don't think。 we can use called the recycler。 To be honest， I don't understand
    how the recycler works internally。 I got to sit down and just read this book，
    but I don't think we can use it。 I think it requires。 like compile time support。
    But let's talk about this concept， what the technique is called。
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我自己发明了一个。另一个是非常复杂的东西，我认为我们不能使用，叫做回收器。老实说，我不理解回收器的内部工作原理。我得坐下来好好读这本书，但我认为我们不能使用它。我觉得这需要像编译时支持那样的东西。但我们先来谈谈这个概念，这个技术叫什么。
- en: buffered reference counting。 So let's examine our problem。 We have this object
    O at the top。 We have。
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 缓冲引用计数。所以让我们来检查一下我们的问题。我们有这个对象O在顶部。我们有。
- en: '![](img/8819b309a1dc896d5dfc43ab416628b0_11.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8819b309a1dc896d5dfc43ab416628b0_11.png)'
- en: three threads that want to talk to object O， and every time they want to talk
    to it， they want to。 increment the reference count。 And when they're done talking
    to it， they want to decrement the。 reference count。 So they're all trying to talk
    to the same area of memory， and that's why I have to。 use the atomic anchor and
    decker。 We'd like to make it cheaper。 The way you make it cheaper is。
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 三个线程想与对象O通信，每次它们想与之通信时，想要增加引用计数。当它们完成与之通信后，它们想要减少引用计数。因此，它们都在尝试与同一内存区域通信，这就是我必须使用原子锚和解锁器的原因。我们希望使其更便宜。使其更便宜的方法是。
- en: let's have only one person talking to the reference count at a time ever in
    a guaranteed way。 and then， he doesn't have to use the atomic anchor and decker。
    How do you do that？ Well。 let's add a separate， thread。 We're going to call the
    reference count committing thread。 We're going to make a big log。 We have our
    hands to say this is the memory。
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 让只有一个人以保证的方式与引用计数交谈，然后他就不必使用原子锚和解锁器。你怎么做到这一点？好吧。让我们添加一个单独的线程。我们称之为引用计数提交线程。我们将创建一个大的日志。我们有我们的手来说明这是内存。
- en: This is the biggest we'll ever need it to be。 It's a reference count log。 It's
    like a transaction log for reference count changes。 So now the three threads talk
    to that instead。 Every time they want to increment the reference。 count on O，
    they write down O at one with the reference count。 Every time they want to decrement。
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们需要的最大大小。这是一个引用计数日志。就像是引用计数变化的事务日志。所以现在这三个线程转而与此通信。每当它们想要增加O的引用计数时，它们就会在一个地方写下O和引用计数。每当它们想要减少时。
- en: they say， oh， minus one with the reference count。 And then the separate thread
    sits there spinning。 watching the log， and whenever there's work to do， it goes
    and doesn't。 Since the reference count。 committing thread is the only thread that
    ever touches reference counts， he doesn't have to use。 the atomic anchor and decker。
    He can just modify the memory directly。 As a matter of fact。
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 他们说，哦，引用计数减一。然后另一个线程在那里旋转，观察日志，每当有工作要做时，它就去处理，但不处理。由于引用计数的提交线程是唯一一个接触引用计数的线程，因此它不需要使用原子锚和解锁器。实际上，它可以直接修改内存。
- en: it's a good， idea if he's the only thread who ever looks at reference counts。
    Because one of the problems of， this technique is going to be now reference counts
    don't happen in real time。 Now reference counts， you don't know whether they're
    accurate or not。 There could be reference count changes waiting， in the log that
    haven't been committed yet。
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 如果他是唯一一个查看引用计数的线程，这个主意是不错的。因为这种技术的一个问题是，引用计数不会实时发生。现在你不知道引用计数是否准确。可能有等待中的引用计数更改在日志中尚未提交。
- en: So this is going to cause some problems。 I'll talk， about it in a minute。 But
    let's move on。 This solves the problem of atomic anchor and decker。 But all we've
    really done is change it so that now we have contention around this。 ref count
    log where that has to be locked and unlocked per thread。 But we can solve that。
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这将引发一些问题。我稍后会讨论它。但我们继续。这解决了原子锚和解锁器的问题。但我们实际上所做的只是改变它，使得现在我们在这个引用计数日志周围有争用，必须每个线程锁定和解锁。但我们可以解决这个问题。
- en: Let's have one ref count log per thread。 So now each thread writes this reference
    count。 log changes to its own local log。 Now there's no content in over the logs。
    They just need a lock a little bit between the commit log， the committing thread
    and the。 individual threads。 But you just take the buffer and pass it off。 That's
    no big deal。
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们为每个线程都有一个引用计数日志。因此现在每个线程将这个引用计数日志的更改写入自己的本地日志中。现在日志之间没有内容冲突。它们只需要在提交日志、提交线程和各个线程之间锁定一点。但你只需将缓冲区传递出去。这并不是什么大事。
- en: This solves all the contention problems。 But now we have an ordering problem。
    So let's walk through that。 Let's say that we have three threads。 We really only
    need two。 threads for this example。 And let's say for the sake of argument we
    have a list and that's capital。 L。 And it has an object inside of it。 I'm gonna
    call it O。 It's unnamed in this example。 And。
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这解决了所有的争用问题。但现在我们有一个排序问题。所以让我们来探讨一下。假设我们有三个线程。实际上我们只需要两个线程用于这个例子。假设为了论证我们有一个列表，叫做大写的L。它里面有一个对象。我称之为O。在这个例子中它是没有名字的。
- en: list L has the only reference to object O。 That's the only reference that exists。
    So O's reference。 count is one。 And all of those changes happened a million years
    ago。 It's all the dust is all。 settled。 So it's nice and stable right now。 So
    currently O has a reference count of one。 And that reference is being held by
    L。 Thread one comes along and says for X and L， print X。
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 列表L对对象O的唯一引用。这是唯一存在的引用。因此O的引用计数是1。所有这些更改发生在一百万年前。一切尘埃落定。所以现在非常稳定。因此目前O的引用计数为1，而这个引用由L持有。线程一过来并说，对于X和L，打印X。
- en: that does an increment and a decrement on object O。 And then later thread zero
    comes along and says。 L got clear。 Blows away everything inside of L。 That says
    O minus one。 That drops the reference to L， to object O。 So that's gonna kind
    of look like this。 In the reference count log for thread one， we're gonna have
    an O plus one， O minus one。
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 该线程对对象O进行了增量和减量操作。然后线程零过来并说，L已清空。清除了L里面的所有内容。这表明O减一。这会减少对L的引用，指向对象O。因此，这看起来像这样。在线程一的引用计数日志中，我们将有一个O加一，O减一。
- en: And then later in reference count log for thread zero， we're gonna have an O
    minus one。 The problem is what if we commit the reference count log for zero。
    before we commit the reference count log for one。 We're gonna drop the last reference
    to object O。 The object will be destroyed。 And then we're gonna later commit the
    reference count changes in thread。
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 然后在引用计数日志中，对于线程零，我们将有一个O减一。问题是如果我们在提交线程零的引用计数日志之前提交线程一的引用计数日志，会发生什么？我们将删除对象O的最后一个引用。对象将被销毁，然后我们将在稍后提交线程中的引用计数更改。
- en: one。 And we're gonna say O plus one。 And now your program is no longer correct
    because you're touching。 an object that has been destroyed。 Now you might say，
    well just do those in the opposite order。 The first thing I would say to that
    is how do you know you were supposed to do them in the。 opposite order？ And the
    second thing I would say is what if I do this to you？
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 一个。我们将说O加一。现在你的程序不再正确，因为你正在触碰一个已被销毁的对象。现在你可能会说，那就反过来做。对此我首先要问的是，你怎么知道应该反过来做？其次，我要问的是，如果我这样做呢？
- en: Now we have two lists L， L and L2。 We have two objects， O and O2。 We iterate
    over the the lists in one thread each， and then we clear the other list。 There
    is no order that you can do process these logs in， where your program will be
    correct。 Now your program is inevitably incorrect。 So how do we solve that problem？
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有两个列表L、L1和L2。我们有两个对象O和O2。我们在每个线程中迭代这些列表，然后清除另一个列表。没有一种顺序可以处理这些日志，而你的程序将是正确的。现在你的程序不可避免地是错误的。那么我们如何解决这个问题呢？
- en: One way might be to write down a timestamp every time we write down。 a reference
    count change in the log。 That's expensive。 And it may not even be correct。 There's
    been bugs in the past with what I would use the RDT instruction on Intel， x86。
    where that gives you a timestamp counter that's internal to the CPU。 That's very
    high precision。
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 一种方法是在每次记录日志中的引用计数变化时写下一个时间戳。这是昂贵的，可能甚至不正确。过去在我使用Intel的RDT指令时出现过错误，x86的RDT指令提供了一个CPU内部的时间戳计数器，精度非常高。
- en: which is what I would need for this。 But if you have actually two physical CPUs
    in your computer。 like I have， sometimes they can drift so that the timestamp
    counters will be different。 And that would be problematic。 That would mean we
    would commit these things out of order。 So it。 would also be expensive and unsafe。
    We can solve this problem， but it takes a stepping back and。
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我需要的。但是如果你的计算机中实际上有两个物理CPU，像我一样，有时它们可能会漂移，导致时间戳计数器不同。这将是一个问题。这意味着我们将以错误的顺序提交这些事情。因此，这也将是昂贵且不安全的。我们可以解决这个问题，但需要退一步思考。
- en: reexamining the problem itself。 Because it turns out we don't actually need
    super strong ordering。 of a reference count changes。 We can do something much
    weaker that is much cheaper and we can。 achieve that very easily。 So let's start
    with this observation。 Let's talk about two objects。 excuse me。 Let me get this
    right。 Let's talk about two reference count change events。 These are two。
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 重新审视问题本身。因为事实证明，我们实际上并不需要非常强的引用计数变化顺序。我们可以做一些更弱的、成本更低的事情，并且可以非常容易地实现这一点。那么让我们从这个观察开始。让我们谈谈两个对象。抱歉，让我纠正一下。让我们谈谈两个引用计数变化事件。这是两个。
- en: events that we're going to write to the log。 I'm going to talk about reference
    count change one and。 two。 And one might be an incurrer or a decker。 And two might
    be an incurrer or a decker。 And the。 question is， can I swap them in time？ Is
    that harmless？ And it turns out three times out of four。 the answer is yes。 If
    you have an anchor followed by an anchor， you can swap those that's harmless。
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们要写入日志的事件。我将讨论引用计数变化的一和二。一可能是一个incurrer或decker，而二可能是一个incurrer或decker。问题是，我能否在时间上交换它们？这样做是否没有害处？结果是四分之三的时间，答案是肯定的。如果你有一个anchor后面跟着一个anchor，你可以交换它们，这没有害处。
- en: If you have a decker followed by a decker， you can swap those that's harmless。
    If you have a decker。 followed by an anchor， you can swap that that's harmless。
    Because they can't be talking about the。 same object or if they are， the object
    is obviously still alive。 Because we did a decker。 we assumed， the program is
    safe。 If we did a decker and we did an anchor on the same object。
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有一个decker后面跟着一个decker，你可以交换它们，这没有害处。如果你有一个decker，后面跟着一个anchor，你也可以交换，这没有害处。因为它们不可能在谈论同一个对象，或者如果是的话，该对象显然仍然存在。因为我们进行了一个decker，我们假设程序是安全的。如果我们对同一个对象进行了一个decker和一个anchor。
- en: I assume that that， was safe because the object is still alive。 The only one
    you have to make sure that you keep in the。 same order is if you have an anchor
    followed by a decker。 If those are talking about the same。 object and there was
    only one reference to the object and you did that anchor， that keeps it alive。
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我假设这是安全的，因为对象仍然存在。你必须确保保持相同顺序的唯一情况是，如果你有一个anchor后面跟着一个decker。如果它们在谈论同一个对象，而该对象只有一个引用，并且你进行了anchor，那么这将使其保持存活。
- en: when you do a decker immediately afterwards。 You cannot swap those。 But all
    this is telling us is。 we need to make sure that any decker that happens after
    an anchor in time has to happen afterwards。 when we commit it。 And that's a lot
    cheaper to achieve。 So here's what we do。 We have two separate。 logs per thread。
    One for anchors。 One for deckers。
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 当你在之后立即进行堆栈操作时，你无法交换这些。但所有这一切告诉我们的是，我们需要确保在时间上锚点之后发生的任何堆栈操作必须在我们提交时发生。这要实现便便宜得多。所以我们要做的就是每个线程有两个独立的日志。一个用于锚点，一个用于堆栈操作。
- en: And all we need to do is queue them in such a way， that any time we process
    a decker log。 we ensure that all the anchors that could have possibly。 happened
    before at the time are committed first。 I got this working and then I iterated
    on the。 algorithm and now I have a very safe queuing algorithm that is in constant
    time for everything。
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要做的就是以一种方式排队，使得每当我们处理一个堆栈日志时，我们确保所有可能在那个时间之前发生的锚点首先被提交。我已经让这个工作起来，然后我对算法进行了迭代，现在我有一个在所有情况下都能保持常量时间的非常安全的排队算法。
- en: So this is working really well。 As an aside， right here， this was an indispensable
    tool in working。 on the Galactomy。 This is called UndoDB。 It is a reversible debugger
    where you can， it behaves。 exactly like GDB。 And what you do is you set a break
    point and you examine data and you're like。 how the hell did we get in this state？
    And so you set a memory watch point and then you run your。
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这个运行得很好。顺便说一下，这在处理Galactomy时是一个不可或缺的工具。这叫UndoDB。它是一个可逆调试器，它的行为与GDB完全相同。你所要做的是设置一个断点，检查数据，你会想，我们怎么会处于这个状态？然后你设置一个内存监视点，然后你运行你的。
- en: program backwards and you see when that triggers。 It's like， oh， that memory
    changed in this way。 Well， how did that get there？ And then you set another break
    point or a memory watch point or。 something and you run the program backwards
    some more。 Indispensable for solving these problems。 Because you can get in these
    situations where it's like， I have no idea how the program got in this。
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 向后运行程序，你会看到触发器的表现。就像，哦，这个内存以这种方式发生了变化。那么，它是怎么到那里的呢？然后你设置另一个断点或者内存监视点，继续向后运行程序。这对于解决这些问题是不可或缺的。因为你可能会遇到这种情况，仿佛，我完全不知道程序是怎么到这个状态的。
- en: state。 Now you can run it backwards and find out。 So I've been。 I used UndoDB
    a lot in the development， of the reference count manager。 But let's talk about
    how。 what has happened as a result。 So this， is the old macro in C for performing
    an ink graph。 This is what adds one to the reference count。 It's very simple and
    straightforward。
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你可以向后运行它，找出原因。所以我在开发引用计数管理器时大量使用了UndoDB。但是让我们谈谈这作为结果发生了什么。因此，这个是用于执行墨水图的旧宏。这是将引用计数加一的方式。非常简单直接。
- en: We take the object， we find the reference count inside and we add， one。 It's
    more complicated now。 So the first thing we do， look at the bottom， I say， PyInkRef
    is InkRef1。 InkRef1。 what it does is it goes to thread local storage and pulls
    out this ref log object。 which is where we're storing our reference counts per
    thread。
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们拿到对象，找到里面的引用计数，然后加一。现在更复杂了。因此，我们首先要做的是，看底部，我说，PyInkRef是InkRef1。InkRef1的作用是去线程本地存储中拉出这个引用日志对象，这是我们每个线程存储引用计数的地方。
- en: And it does an InkRef2 on the object now。 So InkRef2 is implemented like this，
    where we say。 is there space in the ref log right now for me， to write another
    pointer？ Oh， there is。 Then I'm going to go ahead and write one。 Oh， there isn't。
    Then I need to rotate the logs out。 get fresh， Inkre， and Decker buffers， and
    now write the log。 And then I have three， which is unsafe。
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 现在它对对象执行InkRef2。所以InkRef2的实现方式是，我们要问，当前的引用日志中是否有空间可以让我写入另一个指针？哦，有。那么我就去写一个。哦，没有。那么我需要旋转日志，获取新的Inkre和Decker缓冲区，然后写入日志。然后我有三个，这样是不安全的。
- en: which just writes the thing directly。 And the reason that I。 have three of these
    is because you can use， they get progressively faster。 This is the really slow。
    version。 But I knew that it was going to be slow。 And so I have this macro。 It
    says PyRef cache。 You put that at the top of a function， and now it's cached，
    I've cached the ref log that's for your。
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 直接写入的方式。而我有三个这样的原因是因为你可以使用它们，它们会逐渐变快。这是非常慢的版本。但是我知道它会很慢。所以我有这个宏。它说PyRef缓存。你把它放在一个函数的顶部，现在它被缓存了，我已经缓存了用于你的。
- en: thread as a stack variable。 And now you can just refer to that all the time。
    So now you can change。 all your PyInkRef and PyDeckRefs into PyInkRef2 and PyDeckRef2，
    and they're just faster for free。 If you want to work a little harder， then you
    can sort of pre-insure that there's room in the ref log。 for all of the thing
    you're about to do。 If you're about to do five decrefs in a row。
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 线程作为栈变量。现在你可以随时引用它。所以现在你可以把所有的PyInkRef和PyDeckRefs改成PyInkRef2和PyDeckRef2，它们只是更快。如果你想多花点力气，可以提前确保引用日志中有足够的空间，来处理你即将进行的所有操作。如果你准备连续进行五次减少引用。
- en: then you don't have to check each time as their space。 You can say， is there
    space for five。 to decrefs？ And if there is， then we just go。 And if there isn't，
    then we rotate right then。 That's a little troublesome just because you can't，
    you have to be right there。 It can't。 you can't recurse into the Python interpreter
    because that's going to do its own in-graphs and。
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 然后你不必每次检查是否有空间。你可以说，是否有五个减少引用的空间？如果有，那我们就直接进行。如果没有，那么我们就在那时旋转。这有点麻烦，因为你必须正好在那里。你不能递归进入Python解释器，因为那样会进行它自己的内图和。
- en: decrefs。 So you need to make sure that this is very tightly coupled， but you
    can occasionally。 use this PyInkRef and PyDeckRef3 by establishing that you're
    going to have room in the logs。 This isn't actually， it looks like a lot of code
    is actually pretty fast。 It's really not a big deal。 Like these temporary rows
    go away。
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 减少引用。因此，你需要确保这一点是非常紧密耦合的，但你可以偶尔通过确保日志中有空间来使用PyInkRef和PyDeckRef3。实际上这并没有那么多代码，实际上非常快。并不是大问题。这些临时行会消失。
- en: All the PyRef pad is really doing is doing a， do you referencing a pointer storing
    to it and incrementing？
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 所有的PyRef pad实际上只是进行引用，存储指针并增加计数。
- en: So the other thing that I， that this resulted in， the other problem that this
    caused。 apart from making in-graph and decref a lot more complicated， is that
    there are a couple of places。 There's one place I know of for certain where you
    really need real time reference counts on an object。 And that is weak graphs。
    The way that a weak ref is implemented， I'll remind you that you can。
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，我想说的另一个问题是，这导致了其他问题。除了让内图和减少引用变得更加复杂外，还有几个地方。我知道有一个地方，确实需要对象的实时引用计数，那就是弱引用图。弱引用的实现方式，我提醒你，可以。
- en: have an object， let's say object A， and then you have a weak ref object， there's
    a separate object。 called， we're going to call that B， and B is a weak reference
    to A。 You can say， hey， B， give me a。 strong reference to A， it's like get weak
    reference on there， get ref。 And it goes and gives you one。 And the way this implemented
    inside of。
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你有一个对象，称为对象A，然后你有一个弱引用对象，还有一个单独的对象，我们称之为B，B是对A的弱引用。你可以说，嘿，B，给我一个对A的强引用，就像在那上面获取弱引用，获取引用。然后它会给你一个。实现的方式在内部。
- en: see Python is A knows that there are weak graphs pointing to it。 And when A
    is destroyed。 it calls all of its weak graphs and tells it， hey， A is being destroyed，
    you're not legal anymore。 And they say， okay， sorry。 But from a technical point
    of view， B has a pointer， to A。 but it does not have a reference to A。 It does
    not actually know at any particular time whether。
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 看到Python中A知道有弱引用图指向它。当A被销毁时，它会调用所有的弱引用图，并告诉它，嘿，A正在被销毁，你不再合法。它们会说，好吧，对不起。但从技术角度来看，B有一个指向A的指针，但它没有对A的引用。它实际上并不知道在任何特定时间是否。
- en: or not A is alive。 It's relying on the fact that we're running in the gill and
    A is going to tell。 it， oh， I'm dead now， to be safe。 But under the collection，
    of course， we don't have the gill。 we don't have that safety。 And so I had no
    way， because of these buffer reference counts， I didn't。 know in real time whether
    or not an object was alive or not。 What I wound up solving this problem。
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 或者说A是否存活。它依赖于我们在全局解释器锁下运行的事实，而A会告诉它，哦，我现在死了，以确保安全。但是在垃圾收集时，我们当然没有全局解释器锁，也没有那种安全性。因此，由于这些缓冲引用计数，我无法实时知道一个对象是否存活。我最终解决了这个问题。
- en: with was a secondary reference count that is only used for weak graphs in a
    couple of these other。 special examples， which is atomically modified。 It's actually
    not anchored in decor the way that。 it works is you read the value。 If it's negative
    one， the object is being destroyed and you're done。 If it's not negative one，
    add one to the value you got and do an atomic test， compare and swap。
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个次要引用计数，仅在其他几个特殊示例中的弱图中使用，它是原子修改的。实际上，它并没有像装饰器那样被固定。它的工作原理是你读取值。如果值是负一，表示对象正在被销毁，你可以结束了。如果不是负一，增加你得到的值并进行原子测试，进行比较和交换。
- en: If that succeeds， then you've kept the object alive and you have a reference
    and you're good。 If you get back a negative one， then you know that the object
    is being destroyed and you can't。 modify it anymore。 So that solved the problem
    for weak graphs。 I'm actually also using it for。 something called interned moral
    strings。 People have said， Larry， you're using too big a hammer。
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 如果成功，那么你就保持了对象的活性，你有一个引用，情况就好了。如果你得到负一，那么你知道对象正在被销毁，你不能再修改它。因此，这解决了弱图的问题。我实际上也在使用它来处理一些叫做内部道德字符串的东西。人们说，拉里，你使用的工具太大了。
- en: here。 You can solve that in another way。 I'm using it right now。 I haven't worried
    about it。 because it's kind of a minor implementation detail。 It's not a performance
    hit thing。 But I got。 interned moral strings using the same technology。 I don't
    think I need to。 Finally， someone pointed。 out， again， I think it's the same guy
    actually， Mark Shannon pointed out， I have a problem with。
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里。你可以用另一种方式解决这个问题。我现在正在使用它。我对此并不担心，因为这只是一个次要的实现细节。这并不是一个性能瓶颈。但我得到了使用同样技术的内部道德字符串。我觉得我不需要。最后，有人指出，实际上，我认为还是同一个人，马克·香农指出，我有一个问题。
- en: resurrecting objects。 If you have a dunderdell method that takes self and writes
    it to an。 external variable that's doing an ink wrap on the object and that's
    going to keep the object alive。 Well， guess what？ I'm already -- I don't know
    that the object's being kept alive。 I don't have a real， time reference count。
    I go ahead and delete it and now there are references to delete an object。
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 复活对象。如果你有一个dunder方法，接受self并将其写入一个外部变量，这会对对象进行包装，并使对象保持活着。那么，猜猜怎么着？我已经--我不知道对象是否被保持活着。我没有一个真实的、实时的引用计数。我继续删除它，而现在却有引用要删除这个对象。
- en: I have no idea how I'm going to solve that。 He says he has an idea。 I don't
    believe him。 Anyway。 my advice is don't resurrect objects inside of dunderdell。
    That's never been a good。 idea in Python and now it's an even worser idea。 Actually，
    Jython and IronPython had to solve a。 lot of these problems themselves。 I can
    always talk to the IronPython and Jython guys and ask them。
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我不知道我将如何解决这个问题。他说他有一个想法。我不相信他。无论如何，我的建议是在dunder方法内部不要复活对象。这在Python中从来不是个好主意，现在更糟糕。实际上，Jython和IronPython必须自己解决许多这些问题。我总是可以与IronPython和Jython的开发者交流并询问他们。
- en: They said they have a solution that it's terrible， but it works。 So let's look
    at the graph。 This is what happened as a result of making the reference count
    manager。 Again。 we have the same two lines。 Red is always going to be Stocksy
    Python。 Blue is always going to。 be the atomic version as of June。 This new green
    line， that's what it was like as of October。
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 他们说他们有一个解决方案，虽然很糟糕，但确实有效。那么我们来看看图表。这是因为创建引用计数管理器而发生的结果。同样，我们有这两条线。红色总是表示标准的Python。蓝色总是表示截至六月的原子版本。这条新的绿色线是截至十月的情况。
- en: It's getting faster。 The next thing that seemed to be slow was the small block
    allocator。 what I'm going to call obmalic。 What I originally did was I had one
    big lock for just all of。 obmalic。 You grab that and allocate memory and drop
    it。 That was super hot right away。 Internally。 obmalic calls these memory classes。
    It's a range of bytes that you want to allocate。
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 它变得更快了。接下来，似乎慢的东西是小块分配器，我将称之为obmalic。我最初做的是为所有的obmalic设置一个大锁。你抓住它，分配内存然后释放。这一过程非常频繁。在内部，obmalic调用这些内存类。它是你想要分配的一段字节范围。
- en: If you want to allocate zero bytes， that's just illegal。 It always gives you
    a pointer back to this。 valid。 One to eight bytes is a class and then nine to
    16 bytes is a class and 17 to 24 bytes is a class。 So it thinks of those internally
    as being separate。 What I did is I added per class locking and that， made it faster，
    but it still wasn't fast enough。
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想分配零字节，那就是非法的。它总是返回一个指向这个的指针。有效的。1到8字节是一个类，9到16字节是一个类，17到24字节是一个类。所以它在内部将这些视为分开的。我所做的是添加了每个类的锁定，这使得它更快，但仍然不够快。
- en: So I added two stage per class locking。 There was， the fast lock。 which is for
    the super fast case of we have memory。 We just need to slice it off and， hand
    it back。 That's got its own super fast lock。 It's actually spin lock。 Then there's
    a slower。 heavier lock for I need to go and talk to an arena or I need to actually
    allocate memory or whatever。
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我添加了每个类的两级锁。这是快速锁。适用于我们有内存的超级快速情况。我们只需要切割它，然后归还。这有自己的超级快速锁。实际上是自旋锁。然后还有一个更慢、更重的锁，适用于我需要去和一个arena对话，或者我需要真正分配内存，或者其他任何事情。
- en: Anything that isn't the super fast thing allocates a second heavier lock。 which
    doesn't prevent other， threads from doing the faster thing if that actually happens
    to work for them at the time。 That sped things up。 I also added a per thread free
    list for generic allocated memory。 on this stored and thread local storage。 That
    sped things up again。 And finally， around this time。
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 任何不是超级快速的东西都会分配第二个更重的锁。这并不妨碍其他线程在那时执行更快的操作（如果它们恰好有效的话）。这加快了速度。我还为通用分配内存添加了每个线程的空闲列表。这存储在线程本地存储中。这再次加快了速度。最后，大约在这个时候。
- en: I also went through， I gathered a bunch of statistics when I'm doing。 debugging
    of the collect me and I turned that on and that slows things down immensely。 And
    I did a conscientious job of making sure that there was absolutely no overhead
    from。 statistics when statistics were turned off because I was pretty sloppy about
    it before。
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我还查看了一下，在进行collect me调试时收集了一些统计数据，我打开了它，这极大地减慢了速度。我尽心尽力确保在关闭统计时绝对没有任何开销，因为我之前对此相当马虎。
- en: So as a result， it definitely got faster。 The problem is it doesn't show up
    in the graph。 I don't know what happened here。 When I ran these benchmarks， you
    can tell this is janky。 you can also tell it's above the green line。 It really
    should be below。 I guarantee you。 I swear it was faster when I was done， but I
    don't have the data to show for it。 I'm sorry。
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，结果是它确实变得更快了。问题是它在图表中没有显示出来。我不知道这里发生了什么。当我运行这些基准测试时，你可以看出这是不稳定的。你也可以看出它在绿色线之上。它实际上应该在下面。我向你保证。我发誓完成时它更快，但我没有数据来证明这一点。对不起。
- en: I don't know what I did。 But you're going to get used to it。 You're going to
    be staring at your。 own line。 So that's as of February， I think。 Yeah。 No， that's
    as of April of this year。 That's。 as far as I took a break for a couple of months。
    I was just tired of working on it。 But I came back。 to it。 And the next thing
    that seemed to be slow was the physical act of pulling things out of。
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我不知道我做了什么。但是你会习惯的。你会盯着自己的那一行。所以就截至二月，我认为。对。不是的，那是截至今年四月。那是。至于我休息了几个月。我只是厌倦了这个工作。但我又回来了。接下来似乎慢的事情是从中提取东西的物理操作。
- en: thread local storage。 So I'm going to show you a little bit of see Python internals
    here。 The function that actually runs bytecode is this one gigantic function called
    pi eval_eval_frame。exe。 That's the guy who literally runs bytecode。 So I needed
    to pull out my thread local storage。 variable in order to look at stuff inside
    of that。 So at the top of the function， I say。
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 线程本地存储。所以我将给你展示一下Python内部的内容。实际运行字节码的函数是一个巨大的函数，叫做pi eval_eval_frame.exe。它是字面上运行字节码的那个。所以我需要提取我的线程本地存储变量，以便查看里面的内容。因此，在函数的顶部，我说。
- en: pi thread state equals t state equals pi thread state yet。 That goes to thread
    local storage。 pulls it out， sticks it in a stack variable。 Then whenever you
    make a recursive function call。 it calls a function called call function。 Call
    function needed that t state so it pulled， it out。 And then that recursively calls
    another function called fast function。 Fast function needed。
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: pi线程状态等于t状态等于pi线程状态等。这会存储到线程本地存储中。取出来，放进一个栈变量中。然后，每当你进行递归函数调用时。它调用一个叫做call function的函数。call
    function需要那个t状态，所以它把它取出来。然后它递归调用另一个叫做fast function的函数。fast function是需要的。
- en: that thread state thing。 And then that calls pi eval_frame。exe。 These three
    functions get called。 every time you make a function call in C Python。 And the
    Fibonacci benchmark is nothing but recursive。 So I'm making millions of recursive
    function call all over the place。 And every time I do it。 I'm looking in thread
    local storage three times。 So I was making 370 million calls to。
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 那个线程状态的东西。然后调用pi eval_frame.exe。这三个函数每次你在C Python中进行函数调用时都会被调用。而斐波那契基准测试就是递归的。所以我在各处进行了数百万次递归函数调用。每次这样做时，我要查看线程本地存储三次。因此，我进行了3.7亿次调用。
- en: pi thread get specific。 Which seemed to be at the top -- it wasn't dominating
    runtime， but it was。 an enormous piece of runtime。 And I wanted to get rid of
    that。 That's pretty straightforward。 actually。 All I did is I sort of added an
    external membrane。 I took all the existing functions and。 I added two to the end。
    Again， the galactomy， we're not going to merge this。 We're going to do a。
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: pi线程特定。它似乎在顶部——它并没有主导运行时间，但却占据了巨大的运行时间。我想去掉它。这其实相当简单。我所做的就是添加了一个外部膜。我把所有现有的函数都拿来，末尾加了两个。再说一次，galactomy，我们不打算合并这个。我们要做一个。
- en: proper job if it makes it into C Python。 But just to get it going， I added two
    to the end of all。 these internal functions， these three function calls。 And I
    made the externally visible one。 if I'll frame EX， I made that so that it looks
    up the thread local storage thing and passes it in。 as a parameter。 And now pi
    eval_frame EX2 takes as a parameter。
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 如果它能进入C Python，算得上是一个合适的工作。但为了让它运行，我在所有这些内部函数的末尾加了两个，这三次函数调用。我让外部可见的那个，如果我框架EX，我让它查找线程本地存储的东西并将其作为参数传递。现在pi
    eval_frame EX2作为参数。
- en: Call function 2 takes as a parameter。 Fast function 2 takes as a parameter。
    Now we only look at it once and we can do 8 million， recursive function calls。
    So we're fine。 That sped things up again。 So this is where we are today。 This
    black line。 that's what I'm calling the no_tls line。 You'll notice that it is
    faster still than。
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 调用函数2作为参数。快速函数2作为参数。现在我们只查看一次，就可以进行800万次递归函数调用。所以我们没问题。这又加速了。因此，这就是我们今天的状态。这条黑线，我称之为no_tls线。你会注意到它仍然比。
- en: any of the previous lines。 It is getting faster and faster。 Now I want to draw
    your attention to the， fact this is the CPU time graph。 So this is the collective
    amount of CPU time I've spent across all。 seven cores at the right side of the
    graph， as opposed to C Python， which is only using one core。
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 任何之前的行。速度越来越快。现在我想引起你注意的是，这个是CPU时间图。右侧是我在所有七个核心上花费的集体CPU时间，而C Python只使用一个核心。
- en: But what I said at the beginning was I'm actually interested in the wall time。
    I'm defining。 successor failure on this in terms of wall time。 So here's the wall
    time graph。 Guess what？
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 但我一开始说的是，我实际上对墙面时间感兴趣。我在此定义成功与失败，取决于墙面时间。所以这是墙面时间图。你猜怎么着？
- en: The black line looks a lot better now， doesn't it？ I'm pretty close。 Again，
    the benchmarking is so。 terrible。 And the CPU cores are changing frequency。 The
    ground is shifting up beneath my feet。 I don't， know。 It could be that I'm actually
    faster。 Probably not。 It could be that I'm slower than that。 That's more likely。
    But I'm within striking distance。
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 黑线现在看起来好多了，不是吗？我离目标很近。再次强调，基准测试是如此糟糕。CPU核心在频率变化。地面在我脚下移动。我不知道。这可能意味着我实际上变快了。可能没有。也可能我比之前更慢。这更有可能。但我在可达范围内。
- en: And I feel like if I find another big thing that's， a problem。 I may actually
    start to dip below it now。 And then at which point I will say the。 "Glad music
    success， everybody。" So the next thing that I'm working on， I'm working on。 Ovenalock
    again。 Because I think it's still dominating runtime。 At one point I switched。
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我感觉如果我再找到一个大的问题。我可能会实际开始降到它以下。到那时我会说，“大家，音乐成功了。”接下来我在做的事情，我正在再次研究Ovenalock。因为我认为它仍然主导着运行时间。在某个时刻我切换了。
- en: Ovenalock to using spin locks instead of my few text based locks。 And the few
    text locks I ran。 under the profiler。 And allocate memory was 20% of runtime。
    And free memory was 20% of runtime。 That doesn't show up when I use the few text
    based locks for some reason。 I don't know。 Maybe cache grind doesn't like me。
    But I'm like， if I change that lock out， the runtime doesn't。
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 使用自旋锁而不是我的几个基于文本的锁的Ovenalock。我在分析器下运行的几个文本锁。分配内存占据了20%的运行时间。而释放内存也占据了20%的运行时间。奇怪的是，当我使用几个基于文本的锁时，这些情况没有出现。我不知道。也许缓存磨损不喜欢我。但我觉得，如果我更换那个锁，运行时间不会改变。
- en: change significantly。 But the graph changes。 I suspect that maybe it's like，
    I think it's still。 using the time。 It's just not showing up in the profiler。
    So I want to rewrite Ovenalock so that。 there's a central first data structure
    that's called used pools。 And that's global for the。 entire process。 I want to
    make that per thread。 And actually， technically， I did make it per thread。
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 变化显著。但图表在变化。我怀疑可能是，嗯，我觉得它仍在使用时间。只是没有显示在分析器中。所以我想重写Ovenalock，使得有一个中央的第一数据结构，称为使用池。这对整个进程来说是全局的。我想把它做成每个线程的。实际上，从技术上讲，我确实做成了每个线程的。
- en: And now it's not working properly。 And so I run my benchmark。 And instead of
    using 200 megabytes。 it uses 10 gig before it's done。 And the physical act of
    allocating and freeing 10 gig worth of。 objects or whatever it is doing underneath
    itself is slow。 And so the whole program got a lot slower。 Once I figure out what
    that memory allocation problem is。
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 现在它的工作不正常。因此，我运行了基准测试。结果使用的不是200兆字节，而是10吉字节才完成。而实际分配和释放10吉字节对象的过程非常缓慢。因此，整个程序变得慢了很多。一旦我搞清楚了那个内存分配问题。
- en: I can make that go away。 But I still， have another experiment or two to try。
    I have an idea called private locking。 You start with the。 observation that most
    objects never leave the thread they were created in。 And so if we create。 a dict
    and only lives in a single thread and then it's destroyed and never escapes that
    thread。
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我可以让这消失。但我还有另一个或两个实验要尝试。我有一个叫做私有锁定的想法。你首先观察到大多数对象从不离开它们被创建的线程。所以，如果我们创建一个字典，并且它只在一个线程中存在，然后被销毁并永远不离开那个线程。
- en: then why do we have to lock and release every time we talk to that dict？ It
    would be cheaper。 to do something else where we knew that it was a per thread
    object and we didn't have to do all。 that locking。 So I have an idea that essentially，
    dicks and lists and other objects like that。 are created in this pre-locked model
    where if another thread wants to talk to it， it has to say。
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 那么为什么每次我们与那个字典交谈时都要锁定和释放呢？做点别的事情会更便宜，毕竟我们知道这是一个每个线程的对象，不需要做所有的锁定。因此，我有一个想法，基本上，字典和列表以及其他类似对象是在这个预锁定模型中创建的，如果另一个线程想与其交互，它必须说。
- en: hey， I actually want to talk to this。 You need to turn into a regular lockable
    object。 And before。 that， locking and unlocking would be a simple， non-atomic
    increment and decrement。 I tried it once。 it made it slower。 But like I said，
    I may turn the corner and I may figure something out。 Maybe。 I botched it the
    first time。 If I tried again， maybe I'll get it to be faster。
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 嘿，我其实想谈谈这个。你需要把它变成一个常规的可锁对象。在此之前，锁定和解锁只是简单的非原子增减。我尝试过一次，它使得速度更慢。但正如我所说，我可能会转变一下，或许会想出什么。也许。我第一次搞砸了。如果再试一次，或许能更快。
- en: The other idea I have， this was implemented for me actually by Thomas Wooders
    at the sprints last year。 He did something， where he was at work where they store
    the reference count outside of the object。 So right now， the reference count is
    stored at the top of the object。 And the reason that this might help the， gallectomy
    is because of cache lines。
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 我还有一个想法，这实际上是由托马斯·伍德斯在去年的冲刺中为我实现的。他做了一件事，就是在工作中将引用计数存储在对象之外。所以现在，引用计数存储在对象的顶部。这可能有助于*垃圾回收*，因为缓存行的原因。
- en: Every time that you change memory， that invalidates that cache， line for all
    the other cores。 So if you have eight cores in your CPU all looking at the same
    cache， line， you change that memory。 Suddenly it tells you you have to throw away
    your memory and get a， fresh copy because it's changed。 And since we're storing
    the reference count in the object。
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 每次你改变内存时，都会使所有其他核心的缓存行失效。如果你的CPU有八个核心都在查看同一个缓存行，你改变了那块内存。突然，它告诉你必须丢弃你的内存并获取一个新副本，因为它已经改变。而且由于我们将引用计数存储在对象中。
- en: that means we're changing the memory on every object and Python at any time
    we examine it。 including objects that are otherwise immutable， like say， an integer
    or a string。 So again。 the gallectomy is doing nothing but integers。 And every
    time it examines the number zero or one or。 two， it's invalidating the cache lines
    for all the other cores。 If we could get it so that these。
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着我们在任何时候检查每个对象时都在改变内存，包括那些其他不可变的对象，比如说，一个整数或字符串。所以，再一次，*垃圾回收*只是在处理整数。每当它检查数字零、一或二时，它都会使所有其他核心的缓存行失效。如果我们能让这些。
- en: objects were genuinely immutable， then they wouldn't blow away those cache lines。
    That would get rid， of contention on this internal bus where the cores are talking
    to each other。 It would， it actually would be better for copy on write semantics
    for when you were doing multi-process。 although again， the gallectomy is making
    so you don't have to go multi-process。 But storing the。
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 如果对象确实是不可变的，那么它们不会破坏那些缓存行。这将消除核心之间在这个内部总线上的竞争。这实际上会更好地支持多进程时的写时复制语义。尽管再次强调，去掉
    GIL 意味着你不需要使用多进程。但存储这个。
- en: reference count outside of the object itself is overhead and it made it slower。
    So， but maybe in。 the future， maybe we'll figure out a better way or maybe I just
    botched it and maybe it'll be faster。 in the future。 It certainly might help with
    copy on write semantics。 We'll see。 If all else fails。 I have one more huge thing
    that I can do。 This was suggested to me by multiple。 This wasn't my idea。
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 对象自身以外的引用计数会带来额外开销，使其变得更慢。但也许在未来，我们会找到更好的方法，或者我只是搞砸了，也许未来会更快。这肯定有助于写时复制语义。我们拭目以待。如果一切失败，我还有一个巨大的方案可以实施。这是多个来源向我建议的，这并不是我的想法。
- en: This was Mark Shannon and Wukash both suggested independently。 The idea is。
    everybody knows that tracing garbage collection is faster than reference counting
    if you wanted。 to multi-thread it。 And that's what everybody does these days。
    Like Java is a reference。 garbage is tracing， garbage collection， Rust， Go， everybody
    these days。 All the new languages。
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 这是马克·香农和沃卡什独立提出的建议。这个想法是，大家都知道，如果你想要实现多线程，追踪式垃圾回收比引用计数要快。而这就是如今大家所采用的方式。像 Java
    是引用的，垃圾是追踪的，垃圾回收，Rust，Go，如今所有的新语言都是如此。
- en: are all tracing garbage collection。 So， we switched CPython， internal to use
    tracing。 garbage collection。 This is a more difficult API to get right。 It's going
    to break the entire。 C API。 But there's a technology inside of PyPy called CPiX。
    PyPy of course， you know， is the。 Python that's implemented in Python and it uses
    a JIT and it uses real tracing garbage collection。
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 全部都是追踪式垃圾回收。因此，我们切换了 CPython 的内部使用追踪式垃圾回收。这是一个更难以正确实现的 API，它将破坏整个 C API。但 PyPy
    内部有一种技术叫做 CPiX。PyPy 当然是用 Python 实现的 Python，它使用 JIT 并且使用真正的追踪式垃圾回收。
- en: It doesn't use reference counts。 So， their object model is very different from
    CPython。 But a classic problem for PyPy is that it doesn't run the C extensions。
    They had an idea where they。 could run C extensions unmodified。 Like you literally
    could use a compiled shared library and just plug。 it into CPython to PyPy。 But
    what they would do is simulate the C API and that involves simulating。
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 它不使用引用计数。因此，他们的对象模型与 CPython 非常不同。但 PyPy 的一个经典问题是它无法运行 C 扩展。他们曾有一个想法，可以在不修改的情况下运行
    C 扩展。你可以实际使用一个已编译的共享库，将其直接插入到 CPython 和 PyPy 中。但他们要做的是模拟 C API，这涉及到模拟。
- en: CPython's object model which involves simulating reference counts in a tracing
    garbage collection。 environment。 We don't have their problem of their internal
    representation of an object is very。 different。 So， what they had to do eventually
    was they had two different objects。 There was the。 actual internal PyPy object
    of whatever it was。 And then there's the CPiX X representation。 And。
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: CPython 的对象模型涉及在追踪式垃圾回收环境中模拟引用计数。我们没有他们内部对象表示的问题，它们是非常不同的。因此，他们最终不得不创建两个不同的对象。有一个是实际的内部
    PyPy 对象，无论是什么类型。然后还有 CPiX 的表示。
- en: they had a problem keeping them and saying and stuff。 We don't have that problem。
    We can just use。 the internal CPython object without the gill。 And CPi extensions
    can talk to that。 And we would。 just simulate reference counts on top of it。 That
    would be pretty harmless。 And so， it would be a。 lot simpler for us。 And all we
    need to do is simulate reference counts。 And we could probably。
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 他们在保留对象和传递信息方面遇到了问题。但我们没有这个问题。我们可以直接使用内部的 CPython 对象，而不需要全局解释器锁（GIL）。而 CPi 扩展可以与此进行交互。我们只需在其上模拟引用计数。这将对我们来说简单得多。我们需要做的就是模拟引用计数，可能我们能做到。
- en: get that working。 That would be pretty close。 Again， we can't guarantee no breakages
    on the， C API。 But we can -- we could actually do some things to mitigate the
    multi-threadedness of the， galactomy。 And we simulate reference counts。 And we
    could get away perhaps with running。 CIPython extensions in this galactomy version
    with tracing garbage collection。
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 让这个工作起来。这将会非常接近。同样，我们无法保证C API没有中断。但是我们可以——我们实际上可以采取一些措施来缓解galactomy的多线程性。我们还可以模拟引用计数。或许我们可以在这个galactomy版本中运行CIPython扩展，并追踪垃圾回收。
- en: That would be a lot of， work。 I really don't want to do it。 So。 I'm going to
    try and push this existing approach as far as I， can。 But if I really have to
    give up。 maybe I can start over and do this。 So， the final thing I want。 to talk
    about is just this is what we want to see。 This is the graph we actually want
    to see， right？
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 那将会是很多工作。我真的不想做这个。所以，我打算尽可能推进这个现有的方法。但如果我真的必须放弃，或许我可以重新开始。最后我想谈的就是，我们希望看到的就是这个。这就是我们实际上想看到的图，对吧？
- en: I just drew this purple line here。 The idea is you add threads through your
    program and it's just。 use as multiple cores and doesn't care。 The program doesn't
    get any slower。 This is what we wish we had， right？ Actually， we have that because
    that's Jython。 And what I say is that Jython and Iron， Python are actually existence
    proofs that the galactomy will work because consider。
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 我在这里画了一条紫色的线。这个想法是通过你的程序添加线程，它只是使用多个核心，不会受到影响。程序不会变得更慢。这就是我们希望拥有的，对吧？实际上，我们已经拥有，因为那是Jython。我要说的是，Jython和Iron
    Python实际上证明了galactomy会工作，因为考虑到。
- en: Let's just say that， we have a whole pile of C code。 And it just happens that
    like one of the piles is a Java interpreter and。 the other one is a CLR runtime。
    But it's a big pile of C code essentially。 And at the end of the day， they have
    a Python interpreter that's running multi-threaded without a go。
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 就假设我们有一大堆C代码。刚好其中一堆是Java解释器，另一堆是CLR运行时。但基本上它是一大堆C代码。最终，他们有一个正在多线程运行的Python解释器，而不需要go。
- en: This proves that you can write a， multi-threaded Python interpreter in C without
    a go。 The question is not， can we write one？ Will it， work？
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 这证明了你可以在C中编写一个多线程的Python解释器而不需要go。问题不是我们能否写一个？它能否工作？
- en: The question is how much of the C API do I have to break before I can get the
    Python to work。
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 问题是，我必须破坏C API的多少才能使Python正常工作。
- en: '![](img/8819b309a1dc896d5dfc43ab416628b0_13.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8819b309a1dc896d5dfc43ab416628b0_13.png)'
- en: while I have a go？ Thank you。 [ Applause ]， Larry Hastings， ladies and gentlemen。
    By the way。 I have stickers。 Lunch is happening at the moment。 So if you are wanting
    to leave for that。 can you please do so， quietly？ If you have questions for Larry。
    there are two microphones here in the aisles。 Please line， up。
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 我可以尝试一下吗？谢谢。[掌声]，Larry Hastings，女士们，先生们。顺便说一下，我有贴纸。午餐正在进行中。如果你想离开去吃午餐，请安静地离开。如果你有问题要问Larry，这里有两个麦克风在过道里。请排队。
- en: And please be quiet if you're walking out。 What I say is you can have one of
    each design of。 the sticker。 We have a question here on my right。 Hi。 Yeah， Larry
    can hear you。 I can hear you。 Go。 ahead。 So some of this sounded very intel and
    architecture specific。 Is there a -- I think when。 you think about ARM CPUs or
    writing Python in other environments， do you expect this approach will。
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你要走，请保持安静。我所说的是你可以拥有每种设计的一个贴纸。我们在我的右边有一个问题。嗨。是的，Larry可以听到你。我可以听到你。继续吧。所以有些听起来非常特定于Intel和架构。是否有——我觉得当你考虑ARM
    CPU或在其他环境中编写Python时，你是否期望这个方法会。
- en: be feasible in those places as well？ People in the audience， can you please
    be quiet， there are。 questions and answers happening and people are still trying
    to listen。 Okay。 I heard everything。 you said。 Is that your whole question？ The
    question is， is your approach to this going to be usable。 in other architectures
    like ARM？ You talked about intel-specific CPU instructions。 Right。 So the。
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些地方也可行吗？在场的人，请保持安静，有问题和答案正在进行，人们仍然在试图听。好的，我听到了你说的所有内容。这是你的全部问题吗？问题是，你的方法在其他架构，比如ARM上是否可用？你提到了特定于Intel的CPU指令。对，所以。
- en: answer is it should be。 Right now it's not。 Literally， so like I said。 I discovered
    about -- a friend， of mine told me about a week ago。 did you realize that your
    cores are changing speed on you？ I said， oh my god no。 So I wanted to test on
    something that I had handy that I knew was multi-core and I。
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 答案是它应该是。现在不是。字面上来说，就像我说的。我发现大约一个星期前，一个朋友告诉我，你意识到你的核心正在改变速度吗？我说，哦，我的天哪，不。所以我想在我手边有的东西上测试一下，我知道这是多核的。
- en: guessed was not going to be sophisticated enough to change the speed on me like
    that。 So I tried。 compiling the Galactomy on a Raspberry Pi 3， which is 4-core
    and ARM。 And it didn't work because it's --， first of all， I had one problem and
    then I fixed it and I had another problem。 And then I said， oh yeah， I can't include
    this x86 internals header file。
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 我猜测不会复杂到像这样改变我的速度。所以我尝试在Raspberry Pi 3上编译Galactomy，它是4核ARM。但它没能工作，因为首先，我有一个问题，然后我修复了它又有了另一个问题。然后我说，哦对，我不能包含这个x86内部头文件。
- en: That was something that GCC had given me。 And so I like， okay， I can't solve
    that。 But I -- all of the atomic instructions that I'm using， I talk about intel-specific
    things。 RDTSA of course is intel-specific。 I'm really only using， that for statistics
    gathering。 not using that as an implementation base of the Galactomy。
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 这是GCC给我的东西。所以我说，好吧，我不能解决这个。但我使用的所有原子指令，我谈到英特尔特定的东西。RDTSC当然是英特尔特定的。我其实只是在进行统计收集时使用它，并不是把它作为Galactomy的实现基础。
- en: Every modern architecture does have atomic increment and decrement。 Every modern
    architecture。 has atomic tests and set or compare and swap instructions。 So all
    of those things should be。 portable。 There will be more platform reliance in the
    Galactomy because we have to use those。 things and that's not part of the standard
    C API。 But they're all available。 And so we -- all we。
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 每种现代架构都有原子增减。每种现代架构都有原子测试和设置或比较和交换指令。因此，所有这些东西都应该是可移植的。在Galactomy中会有更多的平台依赖，因为我们必须使用这些东西，而这不是标准C
    API的一部分。但它们都是可用的。因此，我们……
- en: need to do is a little bit more platform hacking in order to get those to work。
    Next question。 Okay。 so basically you said that benchmarking is really bad。 Everyone
    knows this。 So how worried are you that the fib function isn't good enough？ I
    mean， if you try any different。 function or anything else， that the results would
    be completely different。 Like how worried are you。
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 我需要做的是多做一些平台黑客，以使其正常工作。下一个问题。好吧，所以基本上你说基准测试真的很糟糕。大家都知道这一点。那么你有多担心fib函数不够好？我的意思是，如果你尝试任何不同的函数或其他东西，结果会完全不同。你有多担心？
- en: about that？ I don't think it'd be a world of difference。 Again， so this is specifically，
    exercising。 I had a list of all the things it's exercising。 So for example， someone
    suggested。 an optimization to me。 They said the Fibonacci function itself， looking
    up the Fibonacci function。 takes a lot of time because we have to do a dict lookup
    and actually has to look in the module。 So。
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 关于这个？我认为这不会有太大的区别。再说，这具体是关于**锻炼**的。我有一个锻炼的所有事项清单。例如，有人向我建议了一个优化。他们说，Fibonacci函数本身查找Fibonacci函数需要花费大量时间，因为我们必须进行字典查找，并且实际上需要在模块中查找。
- en: it's not cached locally。 So it needs to lock and unlock a dict。 And that module
    dict itself was hot。 And so if we did multiple reader single writer lock on the
    dict， that would speed it up。 And I。 said， well， on the one hand， I think that's
    optimizing for my particular benchmark。 Let's not do that。 But on the other hand，
    that's a generally purpose of that's a helpful optimization for a lot of。
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 它没有在本地缓存。因此，它需要锁定和解锁一个字典。而该模块字典本身是热的。所以如果我们在字典上使用多个读者单个写者锁，这将加快速度。我说，好吧，一方面，我认为这是为我的特定基准优化。我们就不这样做。但另一方面，这对许多人来说是一个有用的优化。
- en: people。 And it really isn't all that's tailored to my code and it would work
    on all decks。 And it。 would be fine。 So I think ultimately we're going to merge
    that and make that benchmark faster。 Of course， we wouldn't see that if we went
    to a different benchmark。 Like the one that David Beasley， uses when he's timing
    things and playing with the Gill。
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 人们。而且这并不是完全针对我的代码，它会在所有设备上工作。这样就没问题。所以我认为最终我们会合并这些，并加快这个基准。当然，如果我们转到一个不同的基准，我们是不会看到这些的。比如David
    Beasley在计时和玩Gill时使用的那个。
- en: he uses countdown， which is just a for loop， over 10 million times or something
    like that。 So at the end of the day， I'm not all that worried， because fundamentally
    I'm exercising byte code。 I'm exercising function calls。 I'm exercising， dict
    and list lookups internally。 I'm exercising a little bit of Boolean logic。 I'm
    exercising， integers， not strings。
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 他使用 countdown，这只是一个循环，运行超过 1000 万次之类的。因此，到头来，我并不太担心，因为从根本上说，我在执行字节码。我在执行函数调用。我在内部执行
    dict 和 list 查找。我在执行一点布尔逻辑。我在处理整数，而不是字符串。
- en: But all the things that are implemented and see I'm not that worried about。
    I'm worried about the internal core byte code engine really of CPython is what
    I'm worried about。 And one function is as good as another at that point。 So once
    I'm more confident about the。 galactomy in the approach， then I'd be more interested
    in running more code through it anyway。
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 但我并不担心所有已实现的内容。让我担心的是 CPython 的内部核心字节码引擎。此时，一个函数和另一个函数没有太大区别。因此，一旦我对这种方法的 galactomy
    更有信心，那么我会更感兴趣于运行更多代码。
- en: But right now I just have my head down。 It's like if I could make Fibonacci
    run faster than。 stock CPython， then it becomes much easier to make other things
    run fast and CPython。 So yes。 eventually I'll run other benchmarks， but not now。
    Another question？
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 但现在我只是埋头苦干。如果我能让 Fibonacci 的运行速度比标准 CPython 更快，那么让其他东西在 CPython 上运行得更快就会容易得多。所以是的，最终我会运行其他基准测试，但现在不行。还有其他问题吗？
- en: So if I understand your ref count log implementation correctly， it would actually
    truly make like。 Dell invocations like actually non-deterministic。 I've unfortunately
    seen a lot of Python code that。 kind of relies on reference counting even though
    that was never really guaranteed by the actual。 implementation per se。 But if
    you did go forward with this plan， I could see code breaking because。
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 所以如果我理解你的引用计数日志实现正确的话，它实际上会真正使得 Dell 调用变得非确定性。遗憾的是，我见过很多 Python 代码，某种程度上依赖于引用计数，尽管这从未真正由实际实现保证过。但如果你确实推进这个计划，我可以看到代码可能会出现问题。
- en: they relied on that reference counting implementation。 And I guess what is your
    like。 Okay。 so you're talking about how do you go forward with that？ You're talking
    about the fact that。 people rely on， you're not talking about reference counting
    per se， you're talking about the people。 that rely on the fact that once the last
    reference to an object is dropped， the object is freed。
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 他们依赖于那种引用计数实现。我想你的想法是什么？好吧，你在谈论如何继续前进？你在谈论人们依赖的事实，你并不是在谈论引用计数本身，而是那些依赖于一旦对象的最后一个引用被丢弃，该对象就会被释放的人。
- en: immediately。 Correct。 Right。 The Python language spec specifically says you're
    not allowed to rely。 on that。 And none of the other implementations make it happen。
    And that's just a fact of life。 That。 is a Python visible side effect of the implementation
    of the Gill。 And if we ever merged it and became， official Python， then yes， it
    would go away。 And I'm sorry。
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 立即。正确。对。Python 语言规范明确表示你不能依赖于此。其他实现也没有让它发生。这是生活的事实。这是 Gill 实现的一个 Python 可见副作用。如果我们合并并成为官方
    Python，那么是的，它将消失。抱歉。
- en: So that's why we have things like context， managers。 Context managers explicitly
    there for this sort of object lifetime management。 Yeah。 can't do anything about
    it。 Sorry， can't help you。 All right。 Another question。 I thought you were。 going
    to bring up another topic， which I didn't touch on really quickly。 One side effect，
    the。
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这就是为什么我们有像上下文管理器这样的东西。上下文管理器明确存在于这种对象生命周期管理中。是的，不能对此做任何事情。抱歉，帮不了你。好吧，另一个问题。我以为你会提到另一个话题，但我真的没有快速触及。一个副作用是，
- en: original implementation of the reference count manager， of course， it was doing
    the inkers and。 deckers on this other thread， right？ Which meant that the last
    decker happened on the other thread。 which meant that the Alex happened on that
    other thread。 And so now that's two effects。 One。 the Alex happened on a different
    thread from where the object was originally like where it would。
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 引用计数管理器的原始实现，当然，它是在另一个线程上执行 inkers 和 deckers，对吧？这意味着最后一个 decker 发生在另一个线程上。这意味着
    Alex 发生在那个其他线程上。因此，现在有两个影响。一个，Alex 在与对象最初存在的线程不同的线程上发生。
- en: have naturally happened。 And two， that meant that the， uh， the， uh， the。 the
    commit thread had to work， like a pit pony。 It turned out that that just swamped
    it。 It was spending all this time doing， the Alex and it never got。 it fell behind
    immediately and never caught up。 So what I wound up doing。
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 这自然发生了。第二，这意味着，呃，提交线程必须像小马一样工作。结果显示这完全淹没了它。它花了所有时间在做，亚历克斯，而它从未得到。它立刻落后，始终无法追赶上。因此我最终做了。
- en: internally is when the object reaches a reference count of zero。 I put it on
    another list and I pass， it back to the last thread that did the last deck ref
    and then he notices that later and he commits。 it。 So there is another delay built
    in before the object is destroyed。 So it's even worse。 than you probably thought。
    Next question， please。 Just another quick question about benchmarking。
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 当对象的引用计数达到零时，内部会发生。我把它放在另一个列表上，并将它传回最后一个做最后一次递减引用的线程，然后他会在稍后注意到，并提交它。因此，在对象被销毁之前还有另一个延迟。所以这比你想象的还要糟糕。下一个问题，请。关于基准测试再问一个快点的问题。
- en: I don't have a quick question。 There's probably a long answer。 I'm sorry。 I
    think this， uh， well。 um， assuming that speed step is just for reducing power
    consumption， is there not a， like。 bios setting， to turn it off？ Yeah。 So I went
    through my bios and tried to turn off everything and I guess。 I maybe I had a
    lame bios where I was looking in the wrong spot， but there was literally no big。
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 我没有一个快问题。可能是个长答案。抱歉。我认为，呃，假设速度步骤只是为了减少功耗，难道没有一个，像是，BIOS设置，来关闭它？是的。因此我查看了我的BIOS，试着关闭所有东西，结果我想。也许我在寻找错误的地方，实际上没有大。
- en: flashing set your CPU frequency here thing。 So I turned off AI， uh， turbo boost
    or something like。 that。 There were about three things that I turned off and it
    seemed to be a little bit more stable。 than。 And so these benchmarks are actually
    run with those settings turned on。 But apparently。 Victor said there's literally
    a Linux kernel setting where you can specify the max frequency。
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 刷新设置你的CPU频率的东西。因此我关闭了AI，呃，涡轮增压之类的。大约关闭了三件事，似乎稳定性稍微好了一些。所以这些基准测试实际上是在开启这些设置的情况下运行的。但显然。维克多说确实有一个Linux内核设置，你可以指定最大频率。
- en: And so if I said， oh yeah， your max frequency is a gigahertz。 And guess what？
    Everybody's gonna。 run at a gigahertz because that's like slower than it ever
    wants to be。 So I'm gonna do that。 once I figure out what that setting is and
    once I get home and I'm sitting in front of the computer。 again， but I'm in， I'm，
    like， I said， I'm in， I'm only in so much of a hurry about it because I'm。
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 所以如果我说，哦，是的，你的最大频率是一个千兆赫。你猜怎么着？每个人都会。运行在一个千兆赫，因为这比它想要的速度还要慢。因此，一旦我搞清楚那个设置是什么，并且回到家坐在电脑前时，我就会这样做。但如我所说，我对这件事并没有那么着急，因为我。
- en: like， I'm in the， I'm in the neighborhood。 Hey， last question。 Do you have an
    idea how much you can。 minimize the C API changes from this by the time you're
    on？ How much I've watched since the。 day of the day？ How much the C API changes
    will be to write？ Okay， so I kind of answered that。 question in my talk from last
    year。 The， the answer is that so far I have essentially preserved the。
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 我在附近。嘿，最后一个问题。你知道在你上车之前，如何最大程度地减少C API的变化吗？从那天开始，我观察了多少？写C API的变化会有多大？好的，所以我在去年的演讲中回答了这个问题。答案是，到目前为止我基本上保留了。
- en: existing source code level C API。 So a recompile would get you using all of
    the new technologies。 underneath。 You didn't have to touch the line of source
    code。 The problem is that there are。 semantic changes involved。 So there are guarantees
    that the guild gives you that I don't give you。 Like， and there are， and there
    are other things happening like the， what he alluded to， which is。
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 现有的源代码级C API。因此，重新编译将使你使用底层的所有新技术。你不需要碰源代码的任何一行。问题是，这涉及语义变化。因此，公会给你的保证是我无法给予你的。就像，还有其他事情在发生，正如他所暗示的。
- en: the instantaneous objects going away being able to rely on that， which you don't
    have any more。 because of buffer reference counting。 So another example of this，
    which。 there's literally an example， of this as a good approach to how to do things
    in the C Python extension documentation。 They say， here's example code。 They say，
    let's say you have an object you want to lazily instantiate。
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 瞬时对象的消失让你无法依赖这一点，而现在你无法再依赖它，因为缓冲区引用计数的问题。另一个例子就是，在C Python扩展文档中，有一个字面上的例子，展示如何做事的好方法。他们说，这是示例代码。他们说，假设你有一个对象，你想要懒惰地实例化。
- en: because it's expensive。 You only want to do it when it's needed。 So you say
    static， pi object star。 foo equals null。 And then you say inside of the middle
    of your function， you say， if foo is null。 create foo。 And now we can use foo。
    And everybody's happy。 The problem is。 if you call that three times， now you've
    got three races。 And probably you're going to。
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 因为这很昂贵。你只想在需要的时候做这件事。所以你说静态，pi对象星。foo等于null。然后你在函数中间说，如果foo是null。创建foo。现在我们可以使用foo。大家都很开心。问题是，如果你调用三次，现在你就有了三个竞争条件。可能你将会。
- en: allocate memory， uselessly， and you're going to stomp on that value。 And that's。
    that used to be safe， because it was protected by the guild。 Literally nobody
    could interrupt you and， and call into that， again。 But without the guild。 that's
    no longer safe。 And so I would say， don't do that。 But there's， lots of code that
    does that。
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 不必要地分配内存，而你将要覆盖那个值。这过去是安全的，因为它受到了公会的保护。实际上，没有人可以打断你，并再次调用它。但是没有公会后，这就不再安全了。所以我会说，不要这样做。但有很多代码是这么做的。
- en: And it's a guarantee that the guild gave you that I've taken away。 So。 that's
    a semantic change of the API that isn't encoded in the actual， like literal， pi，
    you know。 in graph， pi dec ref， those things haven't changed。 But the semantics
    around the stuff that you've。 been doing for years have changed。 And so that's
    why I say， I know I'm going to break C extensions。
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 这是公会给你的保证，我已经剥夺了这一点。所以。这是API的语义变化，并没有在实际中编码，比如字面意义上的pi，你知道。在图中，pi dec ref，这些东西没有改变。但你多年来一直在做的事情的语义发生了变化。这就是我说的，我知道我会破坏C扩展的原因。
- en: because the semantics have changed。 And I can't do anything about them。 I can't，
    the whole point。 was to break those sorts of things。 And I can't fix that。 As
    in terms of source code compatibility。 literally， my goal is that you can recompile
    and you will produce， like you won't get any errors。 and there are no new APIs
    that you were supposed to call that you didn't。 Your code will continue。
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 因为语义发生了变化。我对此无能为力。整个重点就是打破这些类型的东西。我无法修复这一点。在源代码兼容性方面，实际上，我的目标是你可以重新编译，并且不会出现任何错误，也没有你应该调用但没有调用的新API。你的代码将继续运行。
- en: to work。 And there are additional APIs that will make things go faster。 But
    unchanged。 C APIs should physically compile。 Larry Hastings， everyone。
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 要进行工作。还有一些额外的API可以加快速度。但没有变化。C API应该物理上编译。拉里·哈斯廷斯，大家好。
- en: '![](img/8819b309a1dc896d5dfc43ab416628b0_15.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8819b309a1dc896d5dfc43ab416628b0_15.png)'
- en: '![](img/8819b309a1dc896d5dfc43ab416628b0_16.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8819b309a1dc896d5dfc43ab416628b0_16.png)'
